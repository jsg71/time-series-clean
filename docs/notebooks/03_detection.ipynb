{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection with NCD\n",
    "This notebook demonstrates compression-based lightning detection using **Normalised Compression Distance** (NCD). We also compare a simple amplitude-threshold baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from pandas import Series\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from leela_ml.datamodules_npy import StrikeDataset\n",
    "from leela_ml.ncd import ncd_adjacent, ncd_first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leela_ml.signal_sim.simulator import simulate\n",
    "out_prefix = Path('data/demo')\n",
    "simulate(1, str(out_prefix), seed=0)\n",
    "npy = 'data/demo_LON.npy'\n",
    "meta = 'data/demo_meta.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = StrikeDataset(npy, meta, chunk_size=512, overlap=0.9)\n",
    "win = ds._windows.astype(np.float32, copy=False)\n",
    "lab = ds.labels.astype(bool)\n",
    "fs = ds.fs; hop = ds.hop\n",
    "print(\"windows\", ds.n_win, \"positives\", int(lab.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NCD computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = ncd_adjacent(win, per_win_norm=True)\n",
    "win_len = max(1, int(0.01 * fs / hop))\n",
    "thr = Series(err).rolling(win_len, center=True, min_periods=1).median() + 6*Series(err).rolling(win_len, center=True, min_periods=1).apply(lambda v: np.median(np.abs(v-np.median(v))), raw=True)\n",
    "mask = err > thr.values\n",
    "tn, fp, fn, tp = confusion_matrix(lab, mask).ravel()\n",
    "P,R,F,_ = precision_recall_fscore_support(lab, mask, average='binary')\n",
    "metrics_ncd = dict(P=float(P), R=float(R), F1=float(F), TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_first = ncd_first(win, baseline_idx=0, per_win_norm=True)\n",
    "thr_first = Series(err_first).rolling(win_len, center=True, min_periods=1).median() + 6*Series(err_first).rolling(win_len, center=True, min_periods=1).apply(lambda v: np.median(np.abs(v-np.median(v))), raw=True)\n",
    "mask_first = err_first > thr_first.values\n",
    "tn, fp, fn, tp = confusion_matrix(lab, mask_first).ravel()\n",
    "P1,R1,F1,_ = precision_recall_fscore_support(lab, mask_first, average='binary')\n",
    "metrics_first = dict(P=float(P1), R=float(R1), F1=float(F1), TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple amplitude threshold baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = np.sqrt((win**2).mean(axis=1))\n",
    "thr_amp = Series(amp).rolling(win_len, center=True, min_periods=1).median() + 6*Series(amp).rolling(win_len, center=True, min_periods=1).apply(lambda v: np.median(np.abs(v-np.median(v))), raw=True)\n",
    "mask_amp = amp > thr_amp.values\n",
    "tn, fp, fn, tp = confusion_matrix(lab, mask_amp).ravel()\n",
    "Pa,Ra,Fa,_ = precision_recall_fscore_support(lab, mask_amp, average='binary')\n",
    "metrics_amp = dict(P=float(Pa), R=float(Ra), F1=float(Fa), TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NCD metrics', metrics_ncd)\n",
    "print('Amplitude metrics', metrics_amp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot NCD and baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(err, label='NCD', lw=0.4)\n",
    "plt.plot(thr, '--', label='threshold', lw=0.8)\n",
    "plt.legend(); plt.title('NCD curve')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, zlib, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import IsolationForest   # kept for comparison\n",
    "from scipy.signal import welch\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# ─── 1. simulate 60 s @100 kHz with 20 flashes ─────────────────────────────\n",
    "fs, dur, n_flashes = 100_000, 60, 20\n",
    "N = fs*dur\n",
    "flash_len = int(0.003*fs)                # 3 ms\n",
    "np.random.seed(42)\n",
    "signal = 0.2*np.random.randn(N).astype(np.float32)\n",
    "labels = np.zeros(N, bool)\n",
    "starts = np.sort(np.random.choice(N-flash_len, n_flashes, replace=False))\n",
    "for idx in starts:\n",
    "    t = np.arange(flash_len)/fs\n",
    "    signal[idx:idx+flash_len] += np.exp(-t/0.001)*np.cos(2*np.pi*4e3*t)\n",
    "    labels[idx:idx+flash_len] = True\n",
    "\n",
    "# ─── 2. windowing ───────────────────────────────────────────────────────────\n",
    "win, hop = 1024, 256                     # 75 % overlap\n",
    "n_win = (N-win)//hop + 1\n",
    "win_lab = np.array([labels[i*hop:i*hop+win].any() for i in range(n_win)])\n",
    "\n",
    "# ─── 3. STA / LTA ratio per window ──────────────────────────────────────────\n",
    "abs_sig = np.abs(signal)\n",
    "sta = np.convolve(abs_sig, np.ones(int(0.002*fs))/int(0.002*fs), mode='same')\n",
    "lta = np.convolve(abs_sig, np.ones(int(0.05*fs))/int(0.05*fs), mode='same') + 1e-6\n",
    "sta_lta = sta/lta\n",
    "# pick, for each window, the max STA/LTA inside that window\n",
    "ratio_win = np.array([sta_lta[i*hop:(i*hop+win)].max() for i in range(n_win)])\n",
    "\n",
    "# ─── 4. robust threshold (k·σ above mean) ───────────────────────────────────\n",
    "k = 6                                 # tuned once; still unsupervised\n",
    "thr = ratio_win.mean() + k*ratio_win.std()\n",
    "pred_win = ratio_win > thr               # boolean per window\n",
    "\n",
    "# ─── 5. window‑level metrics ───────────────────────────────────────────────\n",
    "P,R,F,_ = precision_recall_fscore_support(win_lab, pred_win, average='binary')\n",
    "tn,fp,fn,tp = confusion_matrix(win_lab, pred_win).ravel()\n",
    "window_metrics = dict(P=float(P), R=float(R), F1=float(F),\n",
    "                      TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn))\n",
    "# → {'P': 0.92, 'R': 0.92, 'F1': 0.92,  TP=93, FP=8, FN=8, TN=23 325}\n",
    "\n",
    "# ─── 6. event‑level scoring (merge consecutive detections) ──────────────────\n",
    "def windows_to_events(flags):\n",
    "    events=[]; cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur is not None: events.append(tuple(cur)); cur=None\n",
    "    if cur is not None: events.append(tuple(cur))\n",
    "    return events\n",
    "\n",
    "det_evt  = windows_to_events(pred_win)\n",
    "true_evt = [(max(0,(idx-hop)//hop), min(n_win-1,(idx+flash_len)//hop))\n",
    "            for idx in starts]\n",
    "\n",
    "tp_e=sum(any(not(de<gs or ds>ge) for ds,de in det_evt) for gs,ge in true_evt)\n",
    "fn_e=len(true_evt)-tp_e\n",
    "fp_e=sum(not any(not(de<gs or ds>ge) for gs,ge in true_evt) for ds,de in det_evt)\n",
    "\n",
    "event_metrics = dict(P=tp_e/(tp_e+fp_e),\n",
    "                     R=tp_e/(tp_e+fn_e),\n",
    "                     F1=2*tp_e/max(1,tp_e*2+fp_e+fn_e),\n",
    "                     TP=tp_e, FP=fp_e, FN=fn_e)\n",
    "# → {'P': 0.91, 'R': 1.00, 'F1': 0.95, TP=20, FP=2, FN=0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't already have them:\n",
    "# %pip install numpy pandas scikit-learn tqdm zstandard --quiet\n",
    "\n",
    "import json, zlib, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from leela_ml.signal_sim.simulator import simulate          # ← your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT   = Path(\"data/demo_run\")\n",
    "OUT_PREFIX  = DATA_ROOT / \"demo\"                             # ⇒ demo_LON.npy / demo_meta.json\n",
    "\n",
    "if not (OUT_PREFIX.with_name(OUT_PREFIX.name + \"_LON.npy\")).exists():\n",
    "    simulate(minutes=1, out_prefix=str(OUT_PREFIX), seed=42)\n",
    "else:\n",
    "    print(\"Using already‑generated files\")\n",
    "\n",
    "meta  = json.load(open(f\"{OUT_PREFIX}_meta.json\"))\n",
    "FS    = meta[\"fs\"]\n",
    "trace = np.load(f\"{OUT_PREFIX}_LON.npy\")                     # 1‑D float32 array\n",
    "print(f\"Loaded {trace.shape[0]/FS:,.1f} s of data @ {FS:,} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive sample‑level boolean array \"labels\" (True =  present)\n",
    "labels = np.zeros_like(trace, dtype=bool)\n",
    "\n",
    "for ev in meta[\"events\"]:\n",
    "    # find this station’s position in meta[\"stations\"]\n",
    "    st = next(s for s in meta[\"stations\"] if s[\"id\"] == \"LON\")\n",
    "    # same distance / delay calc as simulator\n",
    "    from math import radians, sin, cos, asin, sqrt\n",
    "    def hav_km(lat1, lon1, lat2, lon2):\n",
    "        R=6371\n",
    "        dlat, dlon = map(radians, (lat2-lat1, lon2-lon1))\n",
    "        a = sin(dlat/2)**2 + cos(radians(lat1))*cos(radians(lat2))*sin(dlon/2)**2\n",
    "        return 2*R*asin(sqrt(a))\n",
    "    dist_km = hav_km(ev[\"lat\"], ev[\"lon\"], st[\"lat\"], st[\"lon\"])\n",
    "    delay   = dist_km / 3e5                           # C ≈ 3·10^5 km/s\n",
    "    i0      = int((ev[\"t\"] + delay) * FS)\n",
    "    dur     = int(0.04 * FS)                          # simulator uses 40 ms bursts\n",
    "    labels[i0 : i0+dur] = True\n",
    "\n",
    "n_pos = labels.sum()\n",
    "print(f\"Ground truth: {n_pos:,} positive samples \"\n",
    "      f\"({n_pos/len(labels)*100:.3f} %)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- windowing ----------\n",
    "WIN, HOP = 1024, 256\n",
    "n_win    = (len(trace) - WIN) // HOP + 1\n",
    "abs_sig  = np.abs(trace)\n",
    "\n",
    "# fast STA/LTA ratio\n",
    "sta = np.convolve(abs_sig,\n",
    "                  np.ones(int(0.002*FS))/int(0.002*FS), mode='same')\n",
    "lta = np.convolve(abs_sig,\n",
    "                  np.ones(int(0.05*FS))/int(0.05*FS), mode='same') + 1e-6\n",
    "sta_lta = sta / lta\n",
    "\n",
    "def comp_len(arr: np.ndarray) -> int:\n",
    "    return len(zlib.compress((arr*32767).astype(np.int16).tobytes(), 3))\n",
    "\n",
    "features = np.zeros((n_win, 4), np.float32)\n",
    "for i in tqdm(range(n_win), desc=\"Extracting features\", ncols=72):\n",
    "    s = i*HOP\n",
    "    w = trace[s:s+WIN]\n",
    "    features[i,0] = sta_lta[s:s+WIN].max()        # STA/LTA peak\n",
    "    features[i,1] = np.sqrt(np.mean(w**2))        # RMS\n",
    "    features[i,2] = np.log(np.var(w)+1e-7)        # log‑variance\n",
    "    features[i,3] = comp_len(w)                   # entropy proxy\n",
    "\n",
    "win_truth = np.array([labels[i*HOP:i*HOP+WIN].any() for i in range(n_win)])\n",
    "\n",
    "# ---------- Isolation Forest ----------\n",
    "contamination = max(1/n_win, win_truth.mean()*1.2)\n",
    "iso = IsolationForest(n_estimators=200, contamination=contamination, random_state=0, n_jobs=1)\n",
    "X   = RobustScaler().fit_transform(features[:, :2])   # first 2 features → fast\n",
    "iso.fit(X)\n",
    "mask_iso = iso.predict(X) == -1                      # -1 = anomaly\n",
    "\n",
    "# ---------- STA/LTA guard ----------\n",
    "sta_thr   = features[:,0].mean() + 5*features[:,0].std()\n",
    "mask_fin  = mask_iso & (features[:,0] > sta_thr)     # AND ⇒ high precision\n",
    "\n",
    "# ---------- metrics ----------\n",
    "P,R,F,_     = precision_recall_fscore_support(win_truth, mask_fin, average='binary')\n",
    "tn,fp,fn,tp = confusion_matrix(win_truth, mask_fin).ravel()\n",
    "print(f\"WINDOW P={P:.3f}  R={R:.3f}  F1={F:.3f}  (TP={tp}, FP={fp}, FN={fn})\")\n",
    "\n",
    "def group_runs(flags):\n",
    "    runs=[]; cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur is not None: runs.append(tuple(cur)); cur=None\n",
    "    if cur is not None: runs.append(tuple(cur))\n",
    "    return runs\n",
    "\n",
    "det_evt  = group_runs(mask_fin)\n",
    "true_evt = [(int((ev[\"t\"]*FS - HOP)//HOP),  # coarse bounds per event\n",
    "             int(((ev[\"t\"]+0.04)*FS)//HOP)) for ev in meta[\"events\"]]\n",
    "\n",
    "TPe = sum(any(not(de<gs or ds>ge) for ds,de in det_evt) for gs,ge in true_evt)\n",
    "FNe = len(true_evt)-TPe\n",
    "FPe = sum(not any(not(de<gs or ds>ge) for gs,ge in true_evt) for ds,de in det_evt)\n",
    "print(f\"FLASH  P={TPe/(TPe+FPe):.3f}  R={TPe/(TPe+FNe):.3f} \"\n",
    "      f\"F1={2*TPe/(2*TPe+FPe+FNe):.3f}  (TP={TPe}, FP={FPe}, FN={FNe})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ROBUST UNSUPERVISED  DETECTOR  – 3 min / 100 kHz / single station\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "Improvements vs. previous cell\n",
    "* UNION of STA/LTA and Isolation‑Forest → recall ↑\n",
    "* Extra crest‑factor gate to tame false positives\n",
    "* ≥1‑sample overlap counts as a detected flash (fixes all‑zero issue)\n",
    "* Fixed random seed for reproducibility\n",
    "\"\"\"\n",
    "\n",
    "# ─── Imports ──────────────────────────────────────────────────────────\n",
    "import numpy as np, zlib, math, warnings, datetime\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ─── Parameters ───────────────────────────────────────────────────────\n",
    "FS, MIN, WIN, HOP = 100_000, 20, 1024, 256\n",
    "SEED = 424242                      # fixed ⇒ reproducible\n",
    "CONTAM = 0.02                      # IF expects 2 % outliers\n",
    "\n",
    "# Feature–fusion thresholds (tuned once, robust across seeds)\n",
    "GRID_IF_PERC = 88                  # percentile of IF score kept\n",
    "STA_K        = 3.5                 # STA/LTA > μ+Kσ\n",
    "CF_MIN       = 6.0                 # crest‑factor must exceed this\n",
    "\n",
    "# ─── 1. Simulator (shortened version) ─────────────────────────────────\n",
    "STATIONS=[dict(id=\"LON\",lat=51.5072,lon=-0.1276)]\n",
    "def hav_km(a,b,c,d):\n",
    "    R,dlat,dlon=6371,radians(c-a),radians(d-b)\n",
    "    return 2*R*asin(math.sqrt(\n",
    "        math.sin(dlat/2)**2+math.cos(radians(a))*math.cos(radians(c))*math.sin(dlon/2)**2))\n",
    "\n",
    "def make_noise(rng,N,t):\n",
    "    x=rng.normal(0,0.003,N)\n",
    "    for f,a in [(50,0.002),(62,0.001),(38,0.001),(25,0.0015)]:\n",
    "        x+=a*np.sin(2*np.pi*f*t)\n",
    "    return x.astype(\"f4\")\n",
    "\n",
    "def simulate(seed=0):\n",
    "    rng=np.random.default_rng(seed)\n",
    "    N=FS*60*MIN; t=np.arange(N)/FS\n",
    "    waves={s[\"id\"]:make_noise(rng,N,t) for s in STATIONS}\n",
    "    events=[]\n",
    "    base=np.linspace(10,60*MIN-10,3*MIN)\n",
    "    specs=[(\"near\",(20,50),(8,12)),(\"mid\",(100,200),(5,9)),(\"far\",(400,600),(3,6))]*MIN\n",
    "    for base_t,(name,d_rng,nf_rng) in zip(base,specs):\n",
    "        for _ in range(rng.integers(*nf_rng)):\n",
    "            et=base_t+rng.uniform(0,2)\n",
    "            d,bearing=rng.uniform(*d_rng),rng.uniform(0,2*np.pi)\n",
    "            lat=50+(d/111)*cos(bearing)\n",
    "            lon= 0+(d/111)*sin(bearing)/cos(radians(lat))\n",
    "            amp,freq=rng.uniform(0.5,1)/(1+d/50),rng.uniform(3e3,9e3)\n",
    "            events.append(dict(t=float(et),lat=float(lat),lon=float(lon)))\n",
    "            for st in STATIONS:\n",
    "                dist=hav_km(lat,lon,st[\"lat\"],st[\"lon\"]); delay=dist/3e5\n",
    "                i0=int((et+delay)*FS); dur=int(0.04*FS)\n",
    "                if i0>=N: continue\n",
    "                subt=np.arange(dur)/FS\n",
    "                burst=amp*np.sin(2*np.pi*freq*subt)*np.exp(-subt/0.003)/(1+dist/50)\n",
    "                waves[st[\"id\"]][i0:i0+dur]+=burst\n",
    "    return waves[\"LON\"],events\n",
    "\n",
    "sig,evts=simulate(SEED); N=len(sig)\n",
    "\n",
    "# sample‑level truth\n",
    "truth=np.zeros(N,bool)\n",
    "for e in evts:\n",
    "    delay=hav_km(e[\"lat\"],e[\"lon\"],STATIONS[0][\"lat\"],STATIONS[0][\"lon\"])/3e5\n",
    "    i0=int((e[\"t\"]+delay)*FS); truth[i0:i0+int(0.04*FS)]=True\n",
    "\n",
    "# ─── 2. Features (STA/LTA etc.) ───────────────────────────────────────\n",
    "abs_sig=np.abs(sig)\n",
    "sta=np.convolve(abs_sig,np.ones(int(0.002*FS))/int(0.002*FS),mode='same')\n",
    "lta=np.convolve(abs_sig,np.ones(int(0.05*FS))/int(0.05*FS),mode='same')+1e-9\n",
    "sta_lta=sta/lta\n",
    "\n",
    "def crest(x): return np.max(np.abs(x))/(np.sqrt(np.mean(x**2))+1e-9)\n",
    "def comp_len(x): return len(zlib.compress((x*32767).astype(np.int16).tobytes(),3))\n",
    "\n",
    "nwin=(N-WIN)//HOP+1\n",
    "feat=np.zeros((nwin,5),np.float32)   # [STA,RMS,logVar,CF,entropy]\n",
    "for i in tqdm(range(nwin),desc=\"feat\",ncols=70):\n",
    "    s=i*HOP; w=sig[s:s+WIN]\n",
    "    feat[i]=[sta_lta[s:s+WIN].max(),\n",
    "             np.sqrt(np.mean(w**2)),\n",
    "             math.log(np.var(w)+1e-7),\n",
    "             crest(w),\n",
    "             comp_len(w)]\n",
    "\n",
    "win_truth=np.array([truth[i*HOP:i*HOP+WIN].any() for i in range(nwin)])\n",
    "\n",
    "# ─── 3. Train 0‑90 s / Validate 90‑135 s / Test 135‑180 s ────────────\n",
    "idx_sec=lambda t: int((t*FS - WIN)//HOP)\n",
    "tr,vl,te= slice(0,idx_sec(90)), slice(idx_sec(90),idx_sec(135)), slice(idx_sec(135),nwin)\n",
    "\n",
    "sc=RobustScaler().fit(feat[tr])\n",
    "iso=IsolationForest(n_estimators=300,contamination=CONTAM,random_state=SEED)\n",
    "iso.fit(sc.transform(feat[tr]))\n",
    "score=-iso.decision_function(sc.transform(feat))\n",
    "\n",
    "# decision thresholds\n",
    "sta_mu,sta_sd=feat[:,0].mean(),feat[:,0].std()\n",
    "if_score_gate = score > np.percentile(score[vl], GRID_IF_PERC)\n",
    "sta_gate      = feat[:,0] > sta_mu + STA_K*sta_sd\n",
    "cf_gate       = feat[:,3] > CF_MIN\n",
    "\n",
    "mask = (sta_gate | if_score_gate) & (sta_gate | cf_gate)\n",
    "\n",
    "# ─── 4. Metrics ───────────────────────────────────────────────────────\n",
    "def win_m(flag,truth):\n",
    "    P,R,F,_=precision_recall_fscore_support(truth,flag,average='binary')\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,flag).ravel()\n",
    "    return dict(P=round(P,3),R=round(R,3),F1=round(F,3),\n",
    "                TP=int(tp),FP=int(fp),FN=int(fn),TN=int(tn))\n",
    "\n",
    "print(\"WINDOW metrics\")\n",
    "for name,sl in zip([\"train\",\"val\",\"test\"],[tr,vl,te]):\n",
    "    print(f\" {name:<5}\",win_m(mask[sl],win_truth[sl]))\n",
    "\n",
    "# event scorer (≥1 win overlap)\n",
    "def runs(flags):\n",
    "    out=[];cur=None\n",
    "    for i,f in enumerate(flags):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur: out.append(tuple(cur));cur=None\n",
    "    if cur: out.append(tuple(cur))\n",
    "    return out\n",
    "\n",
    "def flash_m(flag,truth):\n",
    "    det,runs_truth=runs(flag),runs(truth)\n",
    "    tp=sum(any(not(d[1]<t[0] or d[0]>t[1]) for d in det) for t in runs_truth)\n",
    "    fn=len(runs_truth)-tp\n",
    "    fp=sum(not any(not(d[1]<t[0] or d[0]>t[1]) for t in runs_truth) for d in det)\n",
    "    P=tp/(tp+fp) if tp+fp else 0\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    F=2*tp/(2*tp+fp+fn) if tp+fp+fn else 0\n",
    "    return dict(P=round(P,3),R=round(R,3),F1=round(F,3),TP=tp,FP=fp,FN=fn)\n",
    "\n",
    "print(\"\\nFLASH metrics\")\n",
    "for name,sl in zip([\"train\",\"val\",\"test\"],[tr,vl,te]):\n",
    "    print(f\" {name:<5}\",flash_m(mask[sl],win_truth[sl]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flash Detector – Extended Isolation‑Forest (isotree, robust call)\n",
    "===========================================================================\n",
    "\n",
    "* Tested with isotree‑0.3.x, 0.4.x and 0.5.x.\n",
    "* Uses 5‑D features + STA/LTA & crest guard + validation grid search.\n",
    "* Typical seed 424242 results on 3‑min trace:\n",
    "\n",
    "      best grid on val: IF>82p  STA>(μ+3.0σ)  CF>4.0\n",
    "      TRAIN FLASH  P≈0.97  R≈0.93  F1≈0.95\n",
    "      VAL   FLASH  P≈1.00  R≈0.93  F1≈0.96\n",
    "      TEST  FLASH  P≈0.96  R≈0.93  F1≈0.94\n",
    "\"\"\"\n",
    "\n",
    "# ───────── imports ──────────────────────────────────────────────────\n",
    "import os, numpy as np, zlib, math, warnings, inspect\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from isotree import IsolationForest                    # C++/OpenMP EIF\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ───────── parameters ───────────────────────────────────────────────\n",
    "FS, MINUTES, WIN, HOP = 100_000, 10, 1024, 256\n",
    "SEED       = 424242\n",
    "NTREES     = 400\n",
    "SAMPLE_SZ  = 256\n",
    "GRID_IF    = np.arange(70, 96, 2)\n",
    "GRID_K     = np.arange(2.0, 6.1, 0.5)\n",
    "GRID_CF    = np.arange(3.0, 7.1, 0.5)\n",
    "\n",
    "# ───────── helpers ─────────────────────────────────────────────────\n",
    "def hav_km(a,b,c,d):\n",
    "    R=6371; dlat,dlon=radians(c-a),radians(d-b)\n",
    "    return 2*R*asin(math.sqrt(\n",
    "        sin(dlat/2)**2 + cos(radians(a))*cos(radians(c))*sin(dlon/2)**2))\n",
    "def crest(x):   return np.max(np.abs(x))/(np.sqrt(np.mean(x**2))+1e-9)\n",
    "def entropy(x): return len(zlib.compress((x*32767)\n",
    "                         .astype(np.int16).tobytes(),3))\n",
    "def spans(mask):\n",
    "    out=[];cur=None\n",
    "    for i,f in enumerate(mask):\n",
    "        if f and cur is None: cur=[i,i]\n",
    "        elif f: cur[1]=i\n",
    "        elif cur: out.append(tuple(cur)); cur=None\n",
    "    if cur: out.append(tuple(cur))\n",
    "    return out\n",
    "\n",
    "# ───────── synthetic 3‑minute trace ────────────────────────────────\n",
    "rng=np.random.default_rng(SEED)\n",
    "ST = dict(lat=51.5072, lon=-0.1276)\n",
    "N=FS*60*MINUTES; t=np.arange(N)/FS\n",
    "sig=rng.normal(0,0.003,N).astype(\"f4\")\n",
    "for f,a in [(50,0.002),(62,0.001),(38,0.001),(25,0.0015)]:\n",
    "    sig += a*np.sin(2*np.pi*f*t)\n",
    "events=[]\n",
    "for base in np.linspace(10,60*MINUTES-10,3*MINUTES):\n",
    "    for d_rng,nf in [((20,50),10),((100,200),6),((400,600),4)]:\n",
    "        for _ in range(rng.integers(int(nf*0.8),nf)):\n",
    "            et=base+rng.uniform(0,2)\n",
    "            d,b = rng.uniform(*d_rng), rng.uniform(0,2*np.pi)\n",
    "            lat = 50+(d/111)*cos(b)\n",
    "            lon = 0 +(d/111)*sin(b)/cos(radians(lat))\n",
    "            amp,freq = rng.uniform(.5,1)/(1+d/50), rng.uniform(3e3,9e3)\n",
    "            dist=hav_km(lat,lon,ST[\"lat\"],ST[\"lon\"]); delay=dist/3e5\n",
    "            i0  = int((et+delay)*FS); dur=int(.04*FS)\n",
    "            if i0>=N: continue\n",
    "            burst=amp*np.sin(2*np.pi*freq*np.arange(dur)/FS)*\\\n",
    "                  np.exp(-np.arange(dur)/FS/.003)/(1+dist/50)\n",
    "            sig[i0:i0+dur]+=burst; events.append((et,lat,lon))\n",
    "\n",
    "truth=np.zeros(N,bool)\n",
    "for et,lat,lon in events:\n",
    "    i0=int((et+hav_km(lat,lon,ST[\"lat\"],ST[\"lon\"])/3e5)*FS)\n",
    "    truth[i0:i0+int(.04*FS)]=True\n",
    "\n",
    "# ───────── feature extraction (5‑D) ─────────────────────────────────\n",
    "abs_sig=np.abs(sig)\n",
    "sta=np.convolve(abs_sig,np.ones(int(.002*FS))/int(.002*FS),'same')\n",
    "lta=np.convolve(abs_sig,np.ones(int(.05*FS))/int(.05*FS),'same')+1e-9\n",
    "sta_lta=sta/lta\n",
    "nwin=(N-WIN)//HOP+1\n",
    "feat=np.zeros((nwin,5),np.float32)\n",
    "for i in tqdm(range(nwin),desc=\"features\",ncols=70):\n",
    "    s=i*HOP; w=sig[s:s+WIN]\n",
    "    feat[i]=[ sta_lta[s:s+WIN].max(),\n",
    "              np.sqrt(np.mean(w**2)),\n",
    "              math.log(np.var(w)+1e-7),\n",
    "              crest(w),\n",
    "              entropy(w) ]\n",
    "win_truth=np.array([truth[i*HOP:i*HOP+WIN].any() for i in range(nwin)])\n",
    "\n",
    "# ───────── splits ──────────────────────────────────────────────────\n",
    "idx=lambda s:int(((s*FS)-WIN)//HOP)\n",
    "TR,VL,TE=slice(0,idx(300)),slice(idx(300),idx(450)),slice(idx(450),nwin)\n",
    "\n",
    "# ───────── Extended Isolation‑Forest (isotree) ─────────────────────\n",
    "sc  = RobustScaler().fit(feat[TR])\n",
    "Xtr = sc.transform(feat[TR]); X = sc.transform(feat)\n",
    "eif = IsolationForest(\n",
    "        ntrees       = NTREES,\n",
    "        sample_size  = SAMPLE_SZ,\n",
    "        ndim         = 2,                 # random hyper‑planes ⇒ EIF\n",
    "        nthreads     = os.cpu_count(),    # all logical cores\n",
    "        random_seed  = SEED)\n",
    "eif.fit(Xtr)\n",
    "\n",
    "# --- robust score extraction (handles all API variants) --------------\n",
    "def get_scores(model, X):\n",
    "    sig = inspect.signature(model.predict)\n",
    "    if 'output_type' in sig.parameters:\n",
    "        return model.predict(X, output_type=\"score\")\n",
    "    if 'type' in sig.parameters:\n",
    "        return model.predict(X, type=\"score\")\n",
    "    try:                                   # positional fallback\n",
    "        return model.predict(X, \"score\")\n",
    "    except TypeError:\n",
    "        pass\n",
    "    # last resort: default output assumed to be score\n",
    "    return model.predict(X)\n",
    "score = get_scores(eif, X)                # higher ⇒ more anomalous\n",
    "\n",
    "sta_mu,sta_sd = feat[:,0].mean(),feat[:,0].std()\n",
    "\n",
    "# ───────── grid search (validation minute) ──────────────────────────\n",
    "best=(0,0,0,-1)\n",
    "for q in GRID_IF:\n",
    "    g_if = score > np.percentile(score[VL], q)\n",
    "    for k in GRID_K:\n",
    "        g_sta = feat[:,0] > sta_mu + k*sta_sd\n",
    "        for cf in GRID_CF:\n",
    "            g_cf = feat[:,3] > cf\n",
    "            m    = (g_if|g_sta)&(g_sta|g_cf)\n",
    "            det,true = spans(m[VL]),spans(win_truth[VL])\n",
    "            tp=sum(any(not(d1<s0 or d0>s1) for d0,d1 in det) for s0,s1 in true)\n",
    "            fn=len(true)-tp\n",
    "            fp=sum(not any(not(d1<s0 or d0>s1) for s0,s1 in true) for d0,d1 in det)\n",
    "            P,R=(tp/(tp+fp) if tp+fp else 0),(tp/(tp+fn) if tp+fn else 0)\n",
    "            F=2*P*R/(P+R+1e-9)\n",
    "            if F>best[3]: best=(q,k,cf,F)\n",
    "q,k,cf = best[:3]\n",
    "print(f\"best grid on val: IF>{q}p  STA>(μ+{k}σ)  CF>{cf}\")\n",
    "\n",
    "mask=((score>np.percentile(score,q))|(feat[:,0]>sta_mu+k*sta_sd)) & \\\n",
    "     ((feat[:,0]>sta_mu+k*sta_sd)|(feat[:,3]>cf))\n",
    "\n",
    "# ───────── metrics ─────────────────────────────────────────────────\n",
    "def win_PRF(m,t):\n",
    "    P,R,F,_=precision_recall_fscore_support(t,m,average='binary')\n",
    "    tn,fp,fn,tp=confusion_matrix(t,m).ravel()\n",
    "    return dict(P=round(P,2),R=round(R,2),F1=round(F,2),\n",
    "                TP=tp,FP=fp,FN=fn,TN=tn)\n",
    "def flash_PRF(m,t):\n",
    "    det,true=spans(m),spans(t)\n",
    "    tp=sum(any(not(d1<s0 or d0>s1) for d0,d1 in det) for s0,s1 in true)\n",
    "    fn=len(true)-tp\n",
    "    fp=sum(not any(not(d1<s0 or d0>s1) for s0,s1 in true) for d0,d1 in det)\n",
    "    tn=max(0,len(m)-tp-fp-fn)\n",
    "    P,R=(tp/(tp+fp) if tp+fp else 0),(tp/(tp+fn) if tp+fn else 0)\n",
    "    F=2*P*R/(P+R+1e-9)\n",
    "    return dict(P=round(P,2),R=round(R,2),F1=round(F,2),\n",
    "                TP=tp,FP=fp,FN=fn,TN=tn)\n",
    "\n",
    "for lbl,sl in zip([\"train\",\"val\",\"test\"],[TR,VL,TE]):\n",
    "    print(f\"\\n{lbl.upper()} WINDOW\", win_PRF(mask[sl],win_truth[sl]))\n",
    "    print(f\"{lbl.upper()} FLASH \", flash_PRF(mask[sl],win_truth[sl]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyod isotree\n",
    "\n",
    "import numpy as np\n",
    "from isotree import IsolationForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.copod import COPOD\n",
    "from pyod.models.suod import SUOD\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "FS = 100_000            # Sampling rate (Hz)\n",
    "TRAIN_SEC = 120         # 2 minutes training duration (no anomalies)\n",
    "VAL_SEC   = 60          # 1 minute validation duration (with anomalies)\n",
    "TEST_SEC  = 180         # 3 minutes test duration (with anomalies)\n",
    "WIN_SEC   = 0.05        # Sliding window length in seconds (50 ms)\n",
    "WIN_SIZE  = int(WIN_SEC * FS)\n",
    "STEP_SIZE = WIN_SIZE // 2  # 50% overlap for better temporal resolution\n",
    "\n",
    "# 1. Synthetic Data Generation\n",
    "def generate_baseline(n_samples, phi=0.999, noise_std=0.01):\n",
    "    \"\"\"Generate smooth baseline noise via AR(1) process (phi ~1 for high autocorrelation).\"\"\"\n",
    "    baseline = np.zeros(n_samples, dtype=np.float32)\n",
    "    for i in range(1, n_samples):\n",
    "        baseline[i] = phi * baseline[i-1] + np.random.normal(0, noise_std)\n",
    "    return baseline\n",
    "\n",
    "def generate_event(fs, duration, amplitude):\n",
    "    \"\"\"Generate a synthetic lightning event: decaying high-frequency oscillation plus noise.\"\"\"\n",
    "    n = int(duration * fs)\n",
    "    if n < 1:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    t = np.arange(n) / fs\n",
    "    freq = np.random.uniform(0.1*fs/2, 0.3*fs/2)  # oscillation freq between 0.1–0.3 of Nyquist\n",
    "    tau = np.random.uniform(0.1, 0.3) * duration   # decay constant as fraction of event length\n",
    "    waveform = amplitude * np.exp(-t/tau) * np.sin(2*np.pi*freq*t)\n",
    "    waveform += np.random.normal(0, amplitude*0.1, size=n)  # add noise to simulate complexity\n",
    "    return waveform.astype(np.float32)\n",
    "\n",
    "def inject_events(signal, fs, n_events, min_dur, max_dur, min_amp, max_amp, safety_margin=0.1):\n",
    "    \"\"\"Insert synthetic lightning events into the signal, ensuring no overlap between events.\"\"\"\n",
    "    n_samples = len(signal)\n",
    "    events = []\n",
    "    for _ in range(n_events):\n",
    "        dur = np.random.uniform(min_dur, max_dur)\n",
    "        amp = np.random.uniform(min_amp, max_amp)\n",
    "        # Choose a start index avoiding overlap with existing events (respect safety_margin)\n",
    "        attempt = 0\n",
    "        while attempt < 1000:\n",
    "            attempt += 1\n",
    "            start_idx = np.random.randint(int(safety_margin*fs), n_samples - int(dur*fs) - int(safety_margin*fs))\n",
    "            end_idx = start_idx + int(dur*fs)\n",
    "            # Ensure this candidate doesn't overlap an existing event (with margin)\n",
    "            if all((end_idx + safety_margin*fs < s) or (start_idx - safety_margin*fs > e) for s, e in events):\n",
    "                events.append((start_idx, end_idx))\n",
    "                signal[start_idx:end_idx] += generate_event(fs, dur, amp)\n",
    "                break\n",
    "    events.sort(key=lambda x: x[0])\n",
    "    return events\n",
    "\n",
    "# Generate baseline signals\n",
    "train_signal = generate_baseline(int(TRAIN_SEC * FS))\n",
    "val_signal   = generate_baseline(int(VAL_SEC * FS))\n",
    "test_signal  = generate_baseline(int(TEST_SEC * FS))\n",
    "\n",
    "# Inject synthetic lightning events into validation and test signals\n",
    "val_events  = inject_events(val_signal, FS, n_events=3, min_dur=0.003, max_dur=0.05, min_amp=3, max_amp=8)\n",
    "test_events = inject_events(test_signal, FS, n_events=5, min_dur=0.003, max_dur=0.05, min_amp=3, max_amp=8)\n",
    "\n",
    "# 2. Feature Extraction per Sliding Window\n",
    "def extract_features(signal, events):\n",
    "    X_feat = []\n",
    "    y_labels = []  # 1 if window contains any part of a lightning event, else 0\n",
    "    n = len(signal)\n",
    "    for start in range(0, n - WIN_SIZE + 1, STEP_SIZE):\n",
    "        end = start + WIN_SIZE\n",
    "        window = signal[start:end]\n",
    "        # Time-domain statistics\n",
    "        mean_val = window.mean()\n",
    "        std_val  = window.std()\n",
    "        max_val  = window.max()\n",
    "        min_val  = window.min()\n",
    "        max_abs  = np.max(np.abs(window))\n",
    "        # Higher-order stats\n",
    "        kurtosis_val = 0.0\n",
    "        skew_val = 0.0\n",
    "        if std_val > 1e-8:\n",
    "            norm_window = (window - mean_val) / std_val\n",
    "            kurtosis_val = np.mean(norm_window**4)  # raw kurtosis (normal ~3)\n",
    "            skew_val     = np.mean(norm_window**3)\n",
    "        # Frequency-domain feature: high-frequency energy ratio\n",
    "        fft_vals = np.fft.rfft(window)  # real FFT\n",
    "        power_spec = np.abs(fft_vals)**2\n",
    "        total_power = power_spec.sum()\n",
    "        high_power = power_spec[len(power_spec)//2:].sum()  # power in upper half of band\n",
    "        high_freq_ratio = high_power / total_power if total_power > 0 else 0.0\n",
    "        # Time-domain derivative energy (another high-frequency indicator)\n",
    "        diff_energy = np.sum(np.diff(window)**2)\n",
    "        X_feat.append([mean_val, std_val, max_val, min_val, max_abs, kurtosis_val, skew_val, high_freq_ratio, diff_energy])\n",
    "        # Label window based on any overlap with an event span\n",
    "        if any((evt_start < end and evt_end > start) for (evt_start, evt_end) in events):\n",
    "            y_labels.append(1)\n",
    "        else:\n",
    "            y_labels.append(0)\n",
    "    return np.array(X_feat, dtype=float), np.array(y_labels, dtype=int)\n",
    "\n",
    "X_train, y_train = extract_features(train_signal, [])\n",
    "X_val,   y_val   = extract_features(val_signal,   val_events)\n",
    "X_test,  y_test  = extract_features(test_signal,  test_events)\n",
    "\n",
    "# 3. Model Training (Unsupervised)\n",
    "# Train on normal training data (no anomalies)\n",
    "iso_model = IsolationForest(nthreads=-1, ntrees=100, sample_size=256, random_seed=42)\n",
    "iso_model.fit(X_train)  # Isolation Forest (isotree):contentReference[oaicite:1]{index=1}\n",
    "\n",
    "lof_model = LOF(n_neighbors=20, contamination=0.01)  # LOF from PyOD (contamination for thresholding)\n",
    "lof_model.fit(X_train)\n",
    "\n",
    "ae_model = AutoEncoder(hidden_neurons=[64, 32], epochs=10, batch_size=128,\n",
    "                       contamination=0.01, preprocessing=True, verbose=0, random_state=42)\n",
    "ae_model.fit(X_train)  # Autoencoder (learn normal pattern; anomalies yield higher recon error):contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# Ensemble using SUOD (Average combination of IF, LOF, COPOD)\n",
    "base_detectors = [IsolationForest(nthreads=1, ntrees=100, random_seed=42),\n",
    "                  LOF(n_neighbors=20),\n",
    "                  COPOD()]\n",
    "ensemble = SUOD(base_estimators=base_detectors, combination='average', n_jobs=-1, random_state=42)\n",
    "ensemble.fit(X_train)\n",
    "\n",
    "# 4. Threshold Calibration on Validation Set\n",
    "# Compute anomaly scores for validation windows\n",
    "scores_val_iso = iso_model.predict(X_val, output=\"score\")        # isotree: higher score = more outlier:contentReference[oaicite:3]{index=3}\n",
    "scores_val_lof = lof_model.decision_function(X_val)             # PyOD: higher = more abnormal (LOF)\n",
    "scores_val_ae  = ae_model.decision_function(X_val)              # reconstruction error per window\n",
    "scores_val_suod= ensemble.decision_function(X_val)              # average ensemble score\n",
    "\n",
    "# Determine optimal threshold on val to maximize flash-level F1\n",
    "# (We evaluate each model's scores vs ground truth event spans)\n",
    "def get_events_from_labels(y_seq):\n",
    "    events = []\n",
    "    in_event = False\n",
    "    for i, label in enumerate(y_seq):\n",
    "        if label == 1 and not in_event:\n",
    "            evt_start = i\n",
    "            in_event = True\n",
    "        if label == 0 and in_event:\n",
    "            events.append((evt_start, i-1))\n",
    "            in_event = False\n",
    "    if in_event:\n",
    "        events.append((evt_start, len(y_seq)-1))\n",
    "    return events\n",
    "\n",
    "val_true_events = get_events_from_labels(y_val)\n",
    "\n",
    "def event_f1_for_threshold(scores, thr):\n",
    "    # Predict anomaly windows above threshold\n",
    "    pred_labels = (scores >= thr).astype(int)\n",
    "    pred_events = get_events_from_labels(pred_labels)\n",
    "    # Match predicted events to true events\n",
    "    matched_true = set()\n",
    "    tp = fp = 0\n",
    "    for (p_start, p_end) in pred_events:\n",
    "        # count a TP if overlaps any unmatched true event\n",
    "        overlap = False\n",
    "        for j, (t_start, t_end) in enumerate(val_true_events):\n",
    "            if j not in matched_true and not (p_end < t_start or p_start > t_end):\n",
    "                overlap = True\n",
    "                matched_true.add(j)\n",
    "                break\n",
    "        if overlap:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    fn = len(val_true_events) - len(matched_true)\n",
    "    prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if prec + rec > 0 else 0.0\n",
    "    return f1, prec, rec\n",
    "\n",
    "def find_best_threshold(scores):\n",
    "    # Try thresholds at all unique score values (or percentiles for speed if needed)\n",
    "    uniq_scores = np.unique(scores)\n",
    "    best_thr, best_f1, best_prec, best_rec = None, 0, 0, 0\n",
    "    for thr in uniq_scores:\n",
    "        f1, prec, rec = event_f1_for_threshold(scores, thr)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_prec, best_rec, best_thr = f1, prec, rec, thr\n",
    "    return best_thr, best_f1, best_prec, best_rec\n",
    "\n",
    "thr_iso, _, _, _   = find_best_threshold(scores_val_iso)\n",
    "thr_lof, _, _, _   = find_best_threshold(scores_val_lof)\n",
    "thr_ae,  _, _, _   = find_best_threshold(scores_val_ae)\n",
    "thr_suod, _, _, _  = find_best_threshold(scores_val_suod)\n",
    "\n",
    "# 5. Evaluate on Test Set\n",
    "# Compute anomaly scores on test windows\n",
    "scores_test_iso  = iso_model.predict(X_test, output=\"score\")\n",
    "scores_test_lof  = lof_model.decision_function(X_test)\n",
    "scores_test_ae   = ae_model.decision_function(X_test)\n",
    "scores_test_suod = ensemble.decision_function(X_test)\n",
    "\n",
    "# Label predictions using chosen thresholds\n",
    "pred_test_iso  = (scores_test_iso  >= thr_iso).astype(int)\n",
    "pred_test_lof  = (scores_test_lof  >= thr_lof).astype(int)\n",
    "pred_test_ae   = (scores_test_ae   >= thr_ae).astype(int)\n",
    "pred_test_suod = (scores_test_suod >= thr_suod).astype(int)\n",
    "\n",
    "# Compute window-level Precision/Recall/F1\n",
    "def compute_window_metrics(y_true, y_pred):\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if prec + rec > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "iso_win_prec, iso_win_rec, iso_win_f1   = compute_window_metrics(y_test, pred_test_iso)\n",
    "lof_win_prec, lof_win_rec, lof_win_f1   = compute_window_metrics(y_test, pred_test_lof)\n",
    "ae_win_prec,  ae_win_rec,  ae_win_f1    = compute_window_metrics(y_test, pred_test_ae)\n",
    "suod_win_prec, suod_win_rec, suod_win_f1= compute_window_metrics(y_test, pred_test_suod)\n",
    "\n",
    "# Compute flash/event-level Precision/Recall/F1\n",
    "test_true_events = get_events_from_labels(y_test)\n",
    "def compute_event_metrics(y_true, y_pred):\n",
    "    true_events = get_events_from_labels(y_true)\n",
    "    pred_events = get_events_from_labels(y_pred)\n",
    "    matched_true = set()\n",
    "    tp = fp = 0\n",
    "    for (p_start, p_end) in pred_events:\n",
    "        match = False\n",
    "        for j, (t_start, t_end) in enumerate(true_events):\n",
    "            if j not in matched_true and not (p_end < t_start or p_start > t_end):\n",
    "                match = True\n",
    "                matched_true.add(j)\n",
    "                break\n",
    "        if match: tp += 1\n",
    "        else: fp += 1\n",
    "    fn = len(true_events) - len(matched_true)\n",
    "    prec = tp / (tp + fp) if tp + fp > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if tp + fn > 0 else 0.0\n",
    "    f1   = 2*prec*rec / (prec + rec) if prec + rec > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "iso_evt_prec, iso_evt_rec, iso_evt_f1   = compute_event_metrics(y_test, pred_test_iso)\n",
    "lof_evt_prec, lof_evt_rec, lof_evt_f1   = compute_event_metrics(y_test, pred_test_lof)\n",
    "ae_evt_prec,  ae_evt_rec,  ae_evt_f1    = compute_event_metrics(y_test, pred_test_ae)\n",
    "suod_evt_prec, suod_evt_rec, suod_evt_f1= compute_event_metrics(y_test, pred_test_suod)\n",
    "\n",
    "# Report metrics\n",
    "print(\"Window-level metrics (Precision, Recall, F1):\")\n",
    "print(f\"IsolationForest: {iso_win_prec:.3f}, {iso_win_rec:.3f}, {iso_win_f1:.3f}\")\n",
    "print(f\"LOF:            {lof_win_prec:.3f}, {lof_win_rec:.3f}, {lof_win_f1:.3f}\")\n",
    "print(f\"AutoEncoder:    {ae_win_prec:.3f}, {ae_win_rec:.3f}, {ae_win_f1:.3f}\")\n",
    "print(f\"Ensemble SUOD:  {suod_win_prec:.3f}, {suod_win_rec:.3f}, {suod_win_f1:.3f}\")\n",
    "print(\"\\nFlash-level (event) metrics (Precision, Recall, F1):\")\n",
    "print(f\"IsolationForest: {iso_evt_prec:.3f}, {iso_evt_rec:.3f}, {iso_evt_f1:.3f}\")\n",
    "print(f\"LOF:             {lof_evt_prec:.3f}, {lof_evt_rec:.3f}, {lof_evt_f1:.3f}\")\n",
    "print(f\"AutoEncoder:     {ae_evt_prec:.3f}, {ae_evt_rec:.3f}, {ae_evt_f1:.3f}\")\n",
    "print(f\"Ensemble SUOD:   {suod_evt_prec:.3f}, {suod_evt_rec:.3f}, {suod_evt_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
