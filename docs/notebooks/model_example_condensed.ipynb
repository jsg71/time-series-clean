{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1 – Leela‑scope Lightning Simulator v3  (tiers 1 … 9)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Exactly six stations, harder ‘near/medium/far’, and progressive realism.\n",
    "\n",
    "Variables & structures are IDENTICAL to the legacy generator:\n",
    "    stations, station_order, STN           – geography dict / list / alias\n",
    "    STATIONS                               – extra alias for downstream code\n",
    "    quantized, station_truth               – raw ADC & truth windows\n",
    "    events, stroke_records, stroke_samples – ground‑truth meta\n",
    "    df_wave, df_labels                     – convenience DataFrames\n",
    "    df_to_quantized(),  df_labels_to_events()\n",
    "Nothing else downstream needs to change.\n",
    "\"\"\"\n",
    "# ------------------------------------------------------------\n",
    "import math, random, numpy as np, pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "# 0)  USER KNOBS ------------------------------------------------------------\n",
    "SEED          = 424242\n",
    "duration_min  = 5                 # storm length (min)\n",
    "scenario      = 'medium'          # 'near' | 'medium' | 'far'\n",
    "DIFFICULTY    = 5                 # 1 (easy) … 9 (very hard)\n",
    "FS            = 109_375           # Hz  – keep for pipeline\n",
    "BITS, VREF    = 14, 1.0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# 1)  STATION GEOGRAPHY (always six) ---------------------------------------\n",
    "\n",
    "stations = {\n",
    "    'KEF': dict(lat=64.020, lon=-22.567),  # Keflavík\n",
    "    'VAL': dict(lat=51.930, lon=-10.250),  # Valentia Observatory\n",
    "    'LER': dict(lat=60.150, lon= -1.130),  # Lerwick\n",
    "    'HER': dict(lat=50.867, lon=  0.336),  # Herstmonceux\n",
    "    'GIB': dict(lat=36.150, lon= -5.350),  # Gibraltar\n",
    "    'AKR': dict(lat=34.588, lon= 32.986),  # Akrotiri\n",
    "    'CAM': dict(lat=50.217, lon= -5.317),  # Camborne\n",
    "    'WAT': dict(lat=52.127, lon=  0.956),  # Wattisham\n",
    "    'CAB': dict(lat=51.970, lon=  4.930),  # Cabauw\n",
    "    'PAY': dict(lat=46.820, lon=  6.950),  # Payerne\n",
    "    'TAR': dict(lat=58.263, lon= 26.464),  # Tõravere\n",
    "}\n",
    "\n",
    "station_order = list(stations.keys())    # fixed order\n",
    "STN      = station_order\n",
    "STATIONS = station_order                 # alias used in other notebooks\n",
    "N_STN    = len(STN)\n",
    "\n",
    "# 2)  Helpers ---------------------------------------------------------------\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R=6371.0\n",
    "    φ1,φ2 = map(math.radians, (lat1,lat2))\n",
    "    dφ    = math.radians(lat2-lat1)\n",
    "    dλ    = math.radians(lon2-lon1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "# 3)  Timeline --------------------------------------------------------------\n",
    "pre_sec   = rng.uniform(5,30)\n",
    "storm_sec = duration_min*60\n",
    "total_sec = pre_sec + storm_sec\n",
    "N         = int(total_sec*FS)\n",
    "\n",
    "quantized      = {nm: np.zeros(N, np.int16)   for nm in STN}\n",
    "station_truth  = {nm: np.zeros(N//1024, bool) for nm in STN}\n",
    "events, stroke_records, burst_book = [], [], []\n",
    "\n",
    "# 4)  Tier flags ------------------------------------------------------------\n",
    "flags = dict(\n",
    "    ic_mix          = DIFFICULTY>=2,\n",
    "    multipath       = DIFFICULTY>=3,\n",
    "    coloured_noise  = DIFFICULTY>=4,\n",
    "    rfi             = DIFFICULTY>=5,\n",
    "    sprite_ring     = DIFFICULTY>=5,\n",
    "    false_transient = DIFFICULTY>=5,\n",
    "    clipping        = DIFFICULTY>=5,\n",
    "    multi_cell      = DIFFICULTY>=6,\n",
    "    skywave         = DIFFICULTY>=7,\n",
    "    bg_sferics      = DIFFICULTY>=7,\n",
    "    clock_skew      = DIFFICULTY>=8,\n",
    "    gain_drift      = DIFFICULTY>=8,\n",
    "    dropouts        = DIFFICULTY>=8,\n",
    "    low_snr         = DIFFICULTY>=9,\n",
    "    burst_div       = DIFFICULTY>=9,\n",
    ")\n",
    "\n",
    "# 5)  Storm‑cell geometry ----------------------------------------------------\n",
    "lat0 = np.mean([s['lat'] for s in stations.values()])\n",
    "lon0 = np.mean([s['lon'] for s in stations.values()])\n",
    "R0   = dict(near=100, medium=400, far=900)[scenario]   # tougher radii\n",
    "\n",
    "n_cells = 1 if not flags['multi_cell'] else rng.integers(2,5)\n",
    "cells=[]\n",
    "for _ in range(n_cells):\n",
    "    θ   = rng.uniform(0,2*math.pi)\n",
    "    rad = R0*rng.uniform(0.3,1.0)\n",
    "    cells.append(dict(\n",
    "        lat   = lat0 + (rad/111)*math.cos(θ),\n",
    "        lon   = lon0 + (rad/111)*math.sin(θ)/math.cos(math.radians(lat0)),\n",
    "        drift = rng.uniform(-0.20,0.20,2)          # deg·h‑1\n",
    "    ))\n",
    "\n",
    "# 6)  Flash & stroke generation ---------------------------------------------\n",
    "wave_len = int(0.04*FS)\n",
    "tv       = np.arange(wave_len)/FS\n",
    "rfi_freqs= [14400,20100,30300]\n",
    "\n",
    "eid, t = 0, pre_sec\n",
    "while True:\n",
    "    cell = cells[rng.integers(len(cells))]\n",
    "    c_age= t-pre_sec\n",
    "    c_lat= cell['lat'] + cell['drift'][0]*c_age/3600\n",
    "    c_lon= cell['lon'] + cell['drift'][1]*c_age/3600\n",
    "    t   += rng.lognormal(3,1)* (0.4 if flags['multi_cell'] else 1)\n",
    "    if t>=total_sec: break\n",
    "    eid += 1\n",
    "    # flash location\n",
    "    d = rng.uniform(0,R0)\n",
    "    φ = rng.uniform(0,2*math.pi)\n",
    "    f_lat = c_lat + (d/111)*math.cos(φ)\n",
    "    f_lon = c_lon + (d/111)*math.sin(φ)/math.cos(math.radians(lat0))\n",
    "    f_type= 'IC' if (flags['ic_mix'] and rng.random()<0.3) else 'CG'\n",
    "    n_str = rng.integers(1, 4 if f_type=='IC' else 7)\n",
    "    amp0  = 0.35 if f_type=='IC' else 1.0\n",
    "    s_times = sorted(t + rng.uniform(0,0.06,size=n_str))\n",
    "    events.append(dict(id=eid,flash_type=f_type,lat=f_lat,lon=f_lon,\n",
    "                       stroke_times=s_times))\n",
    "    # build bursts\n",
    "    for si, t0 in enumerate(s_times):\n",
    "        for nm in STN:\n",
    "            dist = hav(f_lat,f_lon, stations[nm]['lat'],stations[nm]['lon'])\n",
    "            idx  = int((t0 + dist/300_000 + rng.uniform(-50,50)/1e6)*FS)\n",
    "            if idx>=N: continue\n",
    "            # waveform\n",
    "            if flags['burst_div'] and rng.random()<0.15:\n",
    "                tau=.0008; burst = (tv/tau)*np.exp(1-tv/tau)\n",
    "            else:\n",
    "                f0=rng.uniform(2500,9500); tau=rng.uniform(0.0003,0.0025)\n",
    "                burst = np.sin(2*math.pi*f0*tv)*np.exp(-tv/tau)\n",
    "            amp = amp0*rng.uniform(2,5)/(1+dist/40)\n",
    "            if flags['low_snr']: amp*=0.4\n",
    "            burst *= amp\n",
    "            # multipath\n",
    "            if flags['multipath'] and dist>60:\n",
    "                dly=int(rng.uniform(0.001,0.0035)*FS)\n",
    "                if dly<wave_len: burst[dly:]+=0.35*burst[:-dly]\n",
    "            # sprite ringers\n",
    "            if flags['sprite_ring'] and rng.random()<0.04:\n",
    "                dly=int(rng.uniform(0.008,0.018)*FS)\n",
    "                if dly<wave_len: burst[dly:]+=0.25*burst[:-dly]\n",
    "            # sky‑wave attenuation\n",
    "            if flags['skywave'] and dist>600:\n",
    "                f=np.fft.rfftfreq(wave_len,1/FS)\n",
    "                H=np.exp(-0.00025*dist*((f/6e3)**2))\n",
    "                burst=np.fft.irfft(np.fft.rfft(burst)*H,n=wave_len)\n",
    "            burst_book.append((nm,idx,burst.astype(np.float32)))\n",
    "            station_truth[nm][idx//1024]=True\n",
    "            stroke_records.append(dict(event_id=eid,stroke_i=si,station=nm,\n",
    "                                       flash_type=f_type,lat=f_lat,lon=f_lon,\n",
    "                                       true_time_s=t0,sample_idx=idx,\n",
    "                                       window_idx=idx//1024))\n",
    "            #if nm==STN[0]: stroke_samples.append(idx)\n",
    "\n",
    "# 7)  Noise profiles ---------------------------------------------------------\n",
    "noise_cfg={}\n",
    "for nm in STN:\n",
    "    base_white=rng.uniform(0.010,0.017)\n",
    "    tones=[]\n",
    "    if flags['rfi'] and rng.random()<0.6:\n",
    "        tones=[(rng.choice(rfi_freqs), rng.uniform(0.001,0.004))]\n",
    "    noise_cfg[nm]=dict(\n",
    "        w = base_white if not flags['coloured_noise'] else base_white*rng.uniform(1,1.8),\n",
    "        h = 0.01 if not flags['coloured_noise'] else rng.uniform(0.006,0.020),\n",
    "        tones = tones,\n",
    "        gain_drift = rng.uniform(-0.05,0.05) if flags['gain_drift'] else 0.0,\n",
    "        skew = rng.uniform(-20e-6,20e-6) if flags['clock_skew'] else 0.0\n",
    "    )\n",
    "\n",
    "# 8)  ADC synthesis loop -----------------------------------------------------\n",
    "b,a  = butter(4, 45000/(FS/2),'low')\n",
    "chunk= int(20*FS)\n",
    "tv40 = np.arange(wave_len)/FS   # for false‑transient reuse\n",
    "\n",
    "for nm in STN:\n",
    "    bur = [b for b in burst_book if b[0]==nm]\n",
    "    cfg = noise_cfg[nm]\n",
    "    drop = np.ones(N,bool)\n",
    "    if flags['dropouts'] and rng.random()<0.1:\n",
    "        for _ in range(rng.integers(1,3)):\n",
    "            s=rng.integers(int(pre_sec*FS),N-int(0.4*FS))\n",
    "            drop[s:s+int(0.4*FS)]=False\n",
    "    for s0 in range(0,N,chunk):\n",
    "        e0=min(N,s0+chunk); L=e0-s0\n",
    "        t=np.arange(s0,e0)/FS\n",
    "        seg = cfg['w']*rng.standard_normal(L) + cfg['h']*np.sin(2*math.pi*50*t)\n",
    "        for f,amp in cfg['tones']:\n",
    "            seg += amp*np.sin(2*math.pi*f*t + rng.uniform(0,2*math.pi))\n",
    "        # gain drift\n",
    "        seg *= 1 + cfg['gain_drift']*(t-pre_sec)/(storm_sec+1e-9)\n",
    "        # add bursts\n",
    "        for (_,i0,br) in bur:\n",
    "            if s0<=i0<e0:\n",
    "                off=i0-s0; l=min(len(br),L-off)\n",
    "                seg[off:off+l]+=br[:l]\n",
    "        # false transient\n",
    "        if flags['false_transient'] and rng.random()<0.002:\n",
    "            idx=rng.integers(0,L-wave_len)\n",
    "            seg[idx:idx+wave_len]+=0.7*np.sin(2*math.pi*5800*tv40)*np.exp(-tv40/0.0009)\n",
    "        # filtering & clipping\n",
    "        seg = filtfilt(b,a,seg)\n",
    "        if flags['clipping']: seg=np.clip(seg,-0.9*VREF,0.9*VREF)\n",
    "        full=2**(BITS-1)-1\n",
    "        adc = np.clip(np.round(seg/VREF*full), -full, full).astype(np.int16)\n",
    "        quantized[nm][s0:e0][drop[s0:e0]] = adc[drop[s0:e0]]\n",
    "\n",
    "# 9)  DataFrames -------------------------------------------------------------\n",
    "df_wave = pd.DataFrame({'time_s':np.arange(N)/FS})\n",
    "for nm in STN: df_wave[nm]=quantized[nm]\n",
    "df_labels = pd.DataFrame(stroke_records)\n",
    "\n",
    "def df_to_quantized(df):      return {nm:df[nm].values.astype(np.int16) for nm in STN}\n",
    "def df_labels_to_events(df):\n",
    "    out=[];      grp=df.groupby('event_id')\n",
    "    for eid,g in grp:\n",
    "        out.append(dict(id=eid,flash_type=g.flash_type.iloc[0],\n",
    "                        lat=g.lat.iloc[0],lon=g.lon.iloc[0],\n",
    "                        stroke_times=sorted(g.true_time_s.unique())))\n",
    "    return out\n",
    "\n",
    "# 10)  Summary --------------------------------------------------------------\n",
    "print(f\"Tier‑{DIFFICULTY} | scenario={scenario} | cells={n_cells}\")\n",
    "print(f\"Flashes {len(events):3d} | strokes {len(df_labels)//N_STN:3d} \"\n",
    "      f\"| samples {N:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑storm generator 2.4  –  calibrated amplitude + sensible path loss\n",
    "#  --------------------------------------------------------------------------\n",
    "#  Produces:  quantized  stroke_records  events  burst_book\n",
    "##############################################################################\n",
    "\n",
    "import math, random, numpy as np, pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# ───── user knobs ──────────────────────────────────────────────────────────\n",
    "SEED          = 424242\n",
    "duration_min  = 5                     # storm duration\n",
    "scenario      = 'medium'              # near | medium | far\n",
    "DIFFICULTY    = 1                     # 1 (easy) … 9 (very hard)\n",
    "SNR_dB        =  -2.0                  # –18 … +18 dB recommended\n",
    "FS            = 109_375               # Hz\n",
    "BITS, VREF    = 14, 1.0\n",
    "\n",
    "if not -18 <= SNR_dB <= 24:\n",
    "    print(f\"[warning] SNR_dB={SNR_dB} is outside the calibrated range.\")\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ───── geometry & helpers ─────────────────────────────────────────────────\n",
    "stations = {  # 11 European LF stations\n",
    "    'KEF': dict(lat=64.020, lon=-22.567),  'VAL': dict(lat=51.930, lon=-10.250),\n",
    "    'LER': dict(lat=60.150, lon= -1.130),  'HER': dict(lat=50.867, lon=  0.336),\n",
    "    'GIB': dict(lat=36.150, lon= -5.350),  'AKR': dict(lat=34.588, lon= 32.986),\n",
    "    'CAM': dict(lat=50.217, lon= -5.317),  'WAT': dict(lat=52.127, lon=  0.956),\n",
    "    'CAB': dict(lat=51.970, lon=  4.930),  'PAY': dict(lat=46.820, lon=  6.950),\n",
    "    'TAR': dict(lat=58.263, lon= 26.464),\n",
    "}\n",
    "STN = list(stations)\n",
    "\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = map(math.radians, (lat1, lat2))\n",
    "    dφ, dλ = math.radians(lat2-lat1), math.radians(lon2-lon1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "def path_loss(dist_km: float) -> float:\n",
    "    \"\"\"\n",
    "    Empirical vertical‑E attenuation:  ~d⁻¹·¹  + moderate ground loss.\n",
    "    Boost 3 dB for first ionospheric hop beyond ~600 km.\n",
    "    \"\"\"\n",
    "    loss = (100/(dist_km + 100))**0.85 * math.exp(-0.0001*dist_km)\n",
    "    if dist_km > 600:\n",
    "        loss *= math.sqrt(2)          # +3 dB\n",
    "    return loss\n",
    "\n",
    "# ───── difficulty flags (unchanged) ───────────────────────────────────────\n",
    "flags = dict(\n",
    "    ic_mix          = DIFFICULTY>=2,\n",
    "    multipath       = DIFFICULTY>=3,\n",
    "    coloured_noise  = DIFFICULTY>=4,\n",
    "    rfi_tones       = DIFFICULTY>=5,\n",
    "    impulsive_rfi   = DIFFICULTY>=6,\n",
    "    sprite_ring     = DIFFICULTY>=5,\n",
    "    false_transient = DIFFICULTY>=6,\n",
    "    clipping        = DIFFICULTY>=5,\n",
    "    multi_cell      = DIFFICULTY>=6,\n",
    "    skywave         = DIFFICULTY>=7,\n",
    "    sferic_bed      = DIFFICULTY>=7,\n",
    "    clock_skew      = DIFFICULTY>=8,\n",
    "    gain_drift      = DIFFICULTY>=8,\n",
    "    dropouts        = DIFFICULTY>=8,\n",
    "    low_snr         = DIFFICULTY>=9,\n",
    "    burst_div       = DIFFICULTY>=9,\n",
    ")\n",
    "\n",
    "# ───── timeline containers ────────────────────────────────────────────────\n",
    "pre_sec   = rng.uniform(5, 30)\n",
    "storm_sec = duration_min * 60\n",
    "N         = int((pre_sec + storm_sec) * FS)\n",
    "\n",
    "quantized       = {nm: np.zeros(N, np.int16) for nm in STN}\n",
    "events, stroke_records, burst_book = [], [], []\n",
    "\n",
    "# ───── storm cells ────────────────────────────────────────────────────────\n",
    "lat_v = np.fromiter((s['lat'] for s in stations.values()), float)\n",
    "lon_v = np.fromiter((s['lon'] for s in stations.values()), float)\n",
    "lat_box = (lat_v.min()-0.9, lat_v.max()+0.9)\n",
    "lon_box = (lon_v.min()-1.5, lon_v.max()+1.5)\n",
    "\n",
    "def new_cell():\n",
    "    return dict(\n",
    "        lat   = rng.uniform(*lat_box),\n",
    "        lon   = rng.uniform(*lon_box),\n",
    "        drift = rng.uniform(-0.30, 0.30, 2)\n",
    "    )\n",
    "\n",
    "cells = [new_cell() for _ in range(1 if not flags['multi_cell']\n",
    "                                   else rng.integers(2, 5))]\n",
    "R0_km = dict(near=120, medium=400, far=1000)[scenario]\n",
    "\n",
    "# ───── waveform library ───────────────────────────────────────────────────\n",
    "wave_len = int(0.04 * FS)\n",
    "tv40     = np.arange(wave_len) / FS\n",
    "SNR_lin  = 10**(SNR_dB/20)               # user knob\n",
    "\n",
    "V_REF_CG = 0.15        # volts peak at 100 km for CG\n",
    "V_REF_IC = 0.06        # typical IC is ~8 dB weaker\n",
    "\n",
    "def make_burst(dist_km: float, cg: bool = True) -> np.ndarray:\n",
    "    \"\"\"Return 40 ms LF burst with calibrated amplitude and artefacts.\"\"\"\n",
    "    # 1) base analytical sferic\n",
    "    if flags['burst_div'] and rng.random() < 0.15:\n",
    "        τ = 0.0008\n",
    "        burst = (tv40/τ) * np.exp(1 - tv40/τ)\n",
    "    else:\n",
    "        f0 = rng.uniform(3e3, 15e3)\n",
    "        τ  = rng.uniform(0.00025, 0.001 if cg else 0.0005)\n",
    "        burst = np.sin(2*math.pi*f0*tv40) * np.exp(-tv40/τ)\n",
    "\n",
    "    # 2) physically anchored amplitude\n",
    "    Vpeak = (V_REF_CG if cg else V_REF_IC)\n",
    "    Vpeak *= path_loss(dist_km) / path_loss(100.0)\n",
    "    Vpeak *= 10**(rng.uniform(-3, 3)/20) * SNR_lin\n",
    "    if flags['low_snr']:\n",
    "        Vpeak *= 0.5\n",
    "    burst *= Vpeak / (np.abs(burst).max() + 1e-12)\n",
    "\n",
    "    # 3) artefacts\n",
    "    if flags['multipath'] and dist_km > 80 and rng.random() < 0.6:\n",
    "        dly = int(rng.uniform(0.001, 0.004) * FS)\n",
    "        if dly < wave_len:\n",
    "            burst[dly:] += 0.35 * burst[:-dly]\n",
    "    if flags['sprite_ring'] and rng.random() < 0.05:\n",
    "        dly = int(rng.uniform(0.008, 0.018) * FS)\n",
    "        if dly < wave_len:\n",
    "            burst[dly:] += 0.25 * burst[:-dly]\n",
    "    if flags['skywave'] and dist_km > 600:\n",
    "        f = np.fft.rfftfreq(wave_len, 1/FS)\n",
    "        H = np.exp(-0.00030*dist_km * (f/7e3)**2)\n",
    "        burst = np.fft.irfft(np.fft.rfft(burst) * H, n=wave_len)\n",
    "\n",
    "    return burst.astype(np.float32)\n",
    "\n",
    "# ───── flash & stroke scheduler ───────────────────────────────────────────\n",
    "λ_flash = dict(near=8, medium=4, far=2)[scenario] * (1 + 0.4*DIFFICULTY)\n",
    "flash_ts, eid = pre_sec, 0\n",
    "\n",
    "while flash_ts < pre_sec + storm_sec:\n",
    "    flash_ts += rng.exponential(60/λ_flash)     # λ = flashes / min\n",
    "    if flash_ts >= pre_sec + storm_sec:\n",
    "        break\n",
    "    eid += 1\n",
    "\n",
    "    cell  = rng.choice(cells)\n",
    "    age_h = (flash_ts - pre_sec) / 3600\n",
    "    f_lat = cell['lat'] + cell['drift'][0]*age_h\n",
    "    f_lon = cell['lon'] + cell['drift'][1]*age_h\n",
    "    r_km  = rng.uniform(0, R0_km); θ = rng.uniform(0, 2*math.pi)\n",
    "    f_lat += (r_km/111)*math.cos(θ)\n",
    "    f_lon += (r_km/111)*math.sin(θ) / math.cos(math.radians(f_lat))\n",
    "\n",
    "    f_type = 'IC' if (flags['ic_mix'] and rng.random() < 0.35) else 'CG'\n",
    "    n_str  = rng.integers(1, 4 if f_type == 'IC' else 6)\n",
    "    gaps   = rng.exponential(0.008, n_str)\n",
    "    s_times = [flash_ts + float(gaps[:i+1].sum()) for i in range(n_str)]\n",
    "\n",
    "    events.append(dict(id=eid, flash_type=f_type, lat=f_lat, lon=f_lon,\n",
    "                       stroke_times=s_times))\n",
    "\n",
    "    for si, t0 in enumerate(s_times):\n",
    "        for nm in STN:\n",
    "            dist = hav(f_lat, f_lon, stations[nm]['lat'], stations[nm]['lon'])\n",
    "            idx  = int((t0 + dist/300_000 + rng.normal(0, 40e-6)) * FS)\n",
    "            if idx >= N - wave_len:\n",
    "                continue\n",
    "            burst_book.append((nm, idx, make_burst(dist, cg=(f_type == 'CG'))))\n",
    "            stroke_records.append(dict(event_id=eid, stroke_i=si, station=nm,\n",
    "                                       flash_type=f_type, lat=f_lat, lon=f_lon,\n",
    "                                       true_time_s=t0, sample_idx=idx,\n",
    "                                       window_idx=idx//1024))\n",
    "\n",
    "# ───── noise & ADC synthesis ──────────────────────────────────────────────\n",
    "rfi_tones = [14_400, 20_100, 30_300]\n",
    "b, a      = butter(4, 45_000/(FS/2), 'low')\n",
    "chunk     = int(20*FS)\n",
    "tv_wave   = tv40\n",
    "\n",
    "for nm in STN:\n",
    "    bursts = [(i0, br) for st, i0, br in burst_book if st == nm]\n",
    "    cfg = dict(\n",
    "        white=rng.uniform(0.010, 0.018),\n",
    "        hum=rng.uniform(0.006, 0.020) if flags['coloured_noise'] else 0.01,\n",
    "        tones=[] if not flags['rfi_tones'] else\n",
    "              [(rng.choice(rfi_tones), rng.uniform(0.001, 0.004))],\n",
    "        gain_drift=rng.uniform(-0.05, 0.05) if flags['gain_drift'] else 0.0,\n",
    "        skew=rng.uniform(-25e-6, 25e-6)    if flags['clock_skew'] else 0.0\n",
    "    )\n",
    "\n",
    "    drop = np.ones(N, bool)\n",
    "    if flags['dropouts'] and rng.random() < 0.1:\n",
    "        for _ in range(rng.integers(1, 3)):\n",
    "            s = rng.integers(int(pre_sec*FS), N - int(0.4*FS))\n",
    "            drop[s:s+int(0.4*FS)] = False\n",
    "\n",
    "    for s0 in range(0, N, chunk):\n",
    "        e0 = min(N, s0 + chunk)\n",
    "        L  = e0 - s0\n",
    "        t  = np.arange(s0, e0) / FS * (1 + cfg['skew'])\n",
    "\n",
    "        seg = (cfg['white'] * rng.standard_normal(L) +\n",
    "               cfg['hum']   * np.sin(2*math.pi*50*t))\n",
    "        for f, amp in cfg['tones']:\n",
    "            seg += amp * np.sin(2*math.pi*f*t + rng.uniform(0, 2*math.pi))\n",
    "        if flags['sferic_bed']:\n",
    "            seg += 0.0008 * rng.standard_normal(L)\n",
    "\n",
    "        seg *= 1 + cfg['gain_drift'] * (t - pre_sec) / (storm_sec + 1e-6)\n",
    "\n",
    "        for i0, br in bursts:\n",
    "            if s0 <= i0 < e0:\n",
    "                off = i0 - s0\n",
    "                seg[off:off+wave_len] += br[:min(wave_len, L-off)]\n",
    "\n",
    "        if flags['impulsive_rfi'] and rng.random() < 0.002:\n",
    "            p = rng.integers(0, L-200)\n",
    "            seg[p:p+200] += rng.uniform(-0.9, 0.9) * np.hanning(200)\n",
    "        if flags['false_transient'] and rng.random() < 0.003:\n",
    "            p = rng.integers(0, L-wave_len)\n",
    "            seg[p:p+wave_len] += (0.6*np.sin(2*math.pi*5800*tv_wave) *\n",
    "                                  np.exp(-tv_wave/0.0009))\n",
    "\n",
    "        seg = filtfilt(b, a, seg)\n",
    "        if flags['clipping']:\n",
    "            seg = np.clip(seg, -0.9*VREF, 0.9*VREF)\n",
    "        full = 2**(BITS-1) - 1\n",
    "        adc  = np.clip(np.round(seg/VREF*full), -full, full).astype(np.int16)\n",
    "        quantized[nm][s0:e0][drop[s0:e0]] = adc[drop[s0:e0]]\n",
    "\n",
    "# ───── optional DataFrames -------------------------------------------------\n",
    "df_wave   = pd.DataFrame({'time_s': np.arange(N)/FS})\n",
    "for nm in STN:\n",
    "    df_wave[nm] = quantized[nm]\n",
    "df_labels = pd.DataFrame(stroke_records)\n",
    "\n",
    "print(f\"Tier‑{DIFFICULTY}  SNR={SNR_dB:+.1f} dB  scenario={scenario}  cells={len(cells)}\")\n",
    "print(f\"Flashes {len(events):3d} | strokes {len(df_labels)//len(STN):3d} \"\n",
    "      f\"| samples {N:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####make replicable\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#  Lightning‑storm generator 2.4  –  single‑RNG, fully reproducible\n",
    "##############################################################################\n",
    "import math, numpy as np, pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# ───── user knobs ──────────────────────────────────────────────────────────\n",
    "SEED          = 424242\n",
    "duration_min  = 5\n",
    "scenario      = 'medium'          # near | medium | far\n",
    "DIFFICULTY    = 1                 # 1 … 9\n",
    "SNR_dB        = -2.0              # –18 … +24 dB\n",
    "FS            = 109_375           # Hz\n",
    "BITS, VREF    = 14, 1.0\n",
    "\n",
    "# ★ 1) **one** NumPy Generator – nothing else will be seeded\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# ───── geometry & helpers ─────────────────────────────────────────────────\n",
    "stations = {                 # order preserved from literal\n",
    "    'KEF': dict(lat=64.020, lon=-22.567),  'VAL': dict(lat=51.930, lon=-10.250),\n",
    "    'LER': dict(lat=60.150, lon= -1.130),  'HER': dict(lat=50.867, lon=  0.336),\n",
    "    'GIB': dict(lat=36.150, lon= -5.350),  'AKR': dict(lat=34.588, lon= 32.986),\n",
    "    'CAM': dict(lat=50.217, lon= -5.317),  'WAT': dict(lat=52.127, lon=  0.956),\n",
    "    'CAB': dict(lat=51.970, lon=  4.930),  'PAY': dict(lat=46.820, lon=  6.950),\n",
    "    'TAR': dict(lat=58.263, lon= 26.464),\n",
    "}\n",
    "STN = list(stations)          # explicit, deterministic order\n",
    "station_order = STN\n",
    "\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = map(math.radians, (lat1, lat2))\n",
    "    dφ, dλ = math.radians(lat2-lat1), math.radians(lon2-lon1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "def path_loss(dist_km: float) -> float:\n",
    "    loss = (100/(dist_km + 100))**0.85 * math.exp(-0.0001*dist_km)\n",
    "    if dist_km > 600:                         # +3 dB first hop\n",
    "        loss *= math.sqrt(2)\n",
    "    return loss\n",
    "\n",
    "# ───── difficulty flags (unchanged) ───────────────────────────────────────\n",
    "flags = {                       # exactly as before\n",
    "    'ic_mix'         : DIFFICULTY >= 2,\n",
    "    'multipath'      : DIFFICULTY >= 3,\n",
    "    'coloured_noise' : DIFFICULTY >= 4,\n",
    "    'rfi_tones'      : DIFFICULTY >= 5,\n",
    "    'impulsive_rfi'  : DIFFICULTY >= 6,\n",
    "    'sprite_ring'    : DIFFICULTY >= 5,\n",
    "    'false_transient': DIFFICULTY >= 6,\n",
    "    'clipping'       : DIFFICULTY >= 5,\n",
    "    'multi_cell'     : DIFFICULTY >= 6,\n",
    "    'skywave'        : DIFFICULTY >= 7,\n",
    "    'sferic_bed'     : DIFFICULTY >= 7,\n",
    "    'clock_skew'     : DIFFICULTY >= 8,\n",
    "    'gain_drift'     : DIFFICULTY >= 8,\n",
    "    'dropouts'       : DIFFICULTY >= 8,\n",
    "    'low_snr'        : DIFFICULTY >= 9,\n",
    "    'burst_div'      : DIFFICULTY >= 9,\n",
    "}\n",
    "\n",
    "# ───── timeline containers ────────────────────────────────────────────────\n",
    "pre_sec   = rng.uniform(5, 30)\n",
    "storm_sec = duration_min * 60\n",
    "N         = int((pre_sec + storm_sec) * FS)\n",
    "\n",
    "quantised       = {nm: np.zeros(N, np.int16) for nm in STN}\n",
    "events, stroke_records, burst_book = [], [], []\n",
    "\n",
    "# ───── storm cells ────────────────────────────────────────────────────────\n",
    "lat_v = np.fromiter((s['lat'] for s in stations.values()), float)\n",
    "lon_v = np.fromiter((s['lon'] for s in stations.values()), float)\n",
    "lat_box = (lat_v.min()-0.9, lat_v.max()+0.9)\n",
    "lon_box = (lon_v.min()-1.5, lon_v.max()+1.5)\n",
    "\n",
    "def new_cell():\n",
    "    return dict(\n",
    "        lat   = rng.uniform(*lat_box),\n",
    "        lon   = rng.uniform(*lon_box),\n",
    "        drift = rng.uniform(-0.30, 0.30, 2)\n",
    "    )\n",
    "\n",
    "cells = [new_cell() for _ in range(1 if not flags['multi_cell']\n",
    "                                   else rng.integers(2, 5))]\n",
    "R0_km = dict(near=120, medium=400, far=1000)[scenario]\n",
    "\n",
    "# ───── waveform library ───────────────────────────────────────────────────\n",
    "wave_len = int(0.04 * FS)\n",
    "tv40     = np.arange(wave_len) / FS\n",
    "SNR_lin  = 10**(SNR_dB/20)\n",
    "\n",
    "V_REF_CG, V_REF_IC = 0.15, 0.06           # @100 km\n",
    "\n",
    "def make_burst(dist_km: float, cg: bool = True) -> np.ndarray:\n",
    "    if flags['burst_div'] and rng.random() < 0.15:\n",
    "        τ = 0.0008\n",
    "        burst = (tv40/τ) * np.exp(1 - tv40/τ)\n",
    "    else:\n",
    "        f0 = rng.uniform(3e3, 15e3)\n",
    "        τ  = rng.uniform(0.00025, 0.001 if cg else 0.0005)\n",
    "        burst = np.sin(2*math.pi*f0*tv40) * np.exp(-tv40/τ)\n",
    "\n",
    "    Vpeak = (V_REF_CG if cg else V_REF_IC)\n",
    "    Vpeak *= path_loss(dist_km) / path_loss(100.0)\n",
    "    Vpeak *= 10**(rng.uniform(-3, 3)/20) * SNR_lin\n",
    "    if flags['low_snr']: Vpeak *= 0.5\n",
    "    burst *= Vpeak / (np.abs(burst).max() + 1e-12)\n",
    "\n",
    "    if flags['multipath'] and dist_km > 80 and rng.random() < 0.6:\n",
    "        dly = int(rng.uniform(0.001, 0.004) * FS)\n",
    "        if dly < wave_len: burst[dly:] += 0.35 * burst[:-dly]\n",
    "    if flags['sprite_ring'] and rng.random() < 0.05:\n",
    "        dly = int(rng.uniform(0.008, 0.018) * FS)\n",
    "        if dly < wave_len: burst[dly:] += 0.25 * burst[:-dly]\n",
    "    if flags['skywave'] and dist_km > 600:\n",
    "        f = np.fft.rfftfreq(wave_len, 1/FS)\n",
    "        H = np.exp(-0.00030*dist_km * (f/7e3)**2)\n",
    "        burst = np.fft.irfft(np.fft.rfft(burst) * H, n=wave_len)\n",
    "\n",
    "    return burst.astype(np.float32)\n",
    "\n",
    "# ───── flash & stroke scheduler ───────────────────────────────────────────\n",
    "λ_flash = dict(near=8, medium=4, far=2)[scenario] * (1 + 0.4*DIFFICULTY)\n",
    "flash_ts, eid = pre_sec, 0\n",
    "\n",
    "while flash_ts < pre_sec + storm_sec:\n",
    "    flash_ts += rng.exponential(60/λ_flash)\n",
    "    if flash_ts >= pre_sec + storm_sec: break\n",
    "    eid += 1\n",
    "\n",
    "    cell  = rng.choice(cells)\n",
    "    age_h = (flash_ts - pre_sec) / 3600\n",
    "    f_lat = cell['lat'] + cell['drift'][0]*age_h\n",
    "    f_lon = cell['lon'] + cell['drift'][1]*age_h\n",
    "    r_km, θ = rng.uniform(0, R0_km), rng.uniform(0, 2*math.pi)\n",
    "    f_lat += (r_km/111)*math.cos(θ)\n",
    "    f_lon += (r_km/111)*math.sin(θ)/math.cos(math.radians(f_lat))\n",
    "\n",
    "    f_type = 'IC' if (flags['ic_mix'] and rng.random() < 0.35) else 'CG'\n",
    "    n_str  = rng.integers(1, 4 if f_type == 'IC' else 6)\n",
    "    gaps   = rng.exponential(0.008, n_str)\n",
    "    s_times = [flash_ts + float(gaps[:i+1].sum()) for i in range(n_str)]\n",
    "\n",
    "    events.append(dict(id=eid, flash_type=f_type, lat=f_lat, lon=f_lon,\n",
    "                       stroke_times=s_times))\n",
    "\n",
    "    for si, t0 in enumerate(s_times):\n",
    "        for nm in STN:\n",
    "            dist = hav(f_lat, f_lon, stations[nm]['lat'], stations[nm]['lon'])\n",
    "            idx  = int((t0 + dist/300_000 + rng.normal(0, 40e-6)) * FS)\n",
    "            if idx >= N - wave_len: continue\n",
    "            burst_book.append((nm, idx, make_burst(dist, cg=(f_type == 'CG'))))\n",
    "            stroke_records.append(dict(event_id=eid, stroke_i=si, station=nm,\n",
    "                                       flash_type=f_type, lat=f_lat, lon=f_lon,\n",
    "                                       true_time_s=t0, sample_idx=idx,\n",
    "                                       window_idx=idx//1024))\n",
    "\n",
    "# ───── noise & ADC synthesis ──────────────────────────────────────────────\n",
    "rfi_tones = [14_400, 20_100, 30_300]\n",
    "b, a      = butter(4, 45_000/(FS/2), 'low')\n",
    "chunk     = int(20*FS)\n",
    "tv_wave   = tv40\n",
    "\n",
    "for nm in STN:\n",
    "    bursts = [(i0, br) for st, i0, br in burst_book if st == nm]\n",
    "    cfg = dict(\n",
    "        white=rng.uniform(0.010, 0.018),\n",
    "        hum=rng.uniform(0.006, 0.020) if flags['coloured_noise'] else 0.01,\n",
    "        tones=[] if not flags['rfi_tones'] else\n",
    "              [(rng.choice(rfi_tones), rng.uniform(0.001, 0.004))],\n",
    "        gain_drift=rng.uniform(-0.05, 0.05) if flags['gain_drift'] else 0.0,\n",
    "        skew=rng.uniform(-25e-6, 25e-6) if flags['clock_skew'] else 0.0\n",
    "    )\n",
    "\n",
    "    drop = np.ones(N, bool)\n",
    "    if flags['dropouts'] and rng.random() < 0.1:\n",
    "        for _ in range(rng.integers(1, 3)):\n",
    "            s = rng.integers(int(pre_sec*FS), N - int(0.4*FS))\n",
    "            drop[s:s+int(0.4*FS)] = False\n",
    "\n",
    "    for s0 in range(0, N, chunk):\n",
    "        e0 = min(N, s0 + chunk)\n",
    "        L  = e0 - s0\n",
    "        t  = np.arange(s0, e0) / FS * (1 + cfg['skew'])\n",
    "\n",
    "        seg = (cfg['white'] * rng.standard_normal(L) +\n",
    "               cfg['hum']   * np.sin(2*math.pi*50*t))\n",
    "        for f, amp in cfg['tones']:\n",
    "            seg += amp * np.sin(2*math.pi*f*t + rng.uniform(0, 2*math.pi))\n",
    "        if flags['sferic_bed']:\n",
    "            seg += 0.0008 * rng.standard_normal(L)\n",
    "\n",
    "        seg *= 1 + cfg['gain_drift'] * (t - pre_sec) / (storm_sec + 1e-6)\n",
    "\n",
    "        for i0, br in bursts:\n",
    "            if s0 <= i0 < e0:\n",
    "                off = i0 - s0\n",
    "                seg[off:off+wave_len] += br[:min(wave_len, L-off)]\n",
    "\n",
    "        if flags['impulsive_rfi'] and rng.random() < 0.002:\n",
    "            p = rng.integers(0, L-200)\n",
    "            seg[p:p+200] += rng.uniform(-0.9, 0.9) * np.hanning(200)\n",
    "        if flags['false_transient'] and rng.random() < 0.003:\n",
    "            p = rng.integers(0, L-wave_len)\n",
    "            seg[p:p+wave_len] += (0.6*np.sin(2*math.pi*5800*tv_wave) *\n",
    "                                  np.exp(-tv_wave/0.0009))\n",
    "\n",
    "        seg = filtfilt(b, a, seg)\n",
    "        if flags['clipping']:\n",
    "            seg = np.clip(seg, -0.9*VREF, 0.9*VREF)\n",
    "        full = 2**(BITS-1) - 1\n",
    "        adc  = np.clip(np.round(seg/VREF*full), -full, full).astype(np.int16)\n",
    "        quantised[nm][s0:e0][drop[s0:e0]] = adc[drop[s0:e0]]\n",
    "\n",
    "# ───── DataFrames (optional) & summary ────────────────────────────────────\n",
    "df_wave   = pd.DataFrame({'time_s': np.arange(N)/FS})\n",
    "for nm in STN: df_wave[nm] = quantised[nm]\n",
    "df_labels = pd.DataFrame(stroke_records)\n",
    "\n",
    "print(f\"Tier‑{DIFFICULTY}  SNR={SNR_dB:+.1f} dB  scenario={scenario}  cells={len(cells)}\")\n",
    "print(f\"Flashes {len(events):3d} | strokes {len(df_labels)//len(STN):3d} | samples {N:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "plt.rcParams.update({'axes.grid': True, 'figure.dpi': 110})\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 0)  Helper: concise console table printer\n",
    "# -----------------------------------------------------------------\n",
    "def _tbl(rows, hdr=None, col_sep=\"  \"):\n",
    "    if hdr: print(col_sep.join(hdr))\n",
    "    for r in rows:\n",
    "        print(col_sep.join(str(c) for c in r))\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1)  Global metadata\n",
    "# -----------------------------------------------------------------\n",
    "N   = quantised[station_order[0]].size\n",
    "dur = N / FS\n",
    "print(f\"► Simulator difficulty tier : {DIFFICULTY}\")\n",
    "print(f\"► Sampling rate             : {FS:,.0f} Hz \"\n",
    "      f\"(Δt={1e6/FS:.2f} µs)\")\n",
    "print(f\"► Duration                  : {dur:.2f} s  ({dur/60:.2f} min)\")\n",
    "print(f\"► Total ADC samples         : {N:,}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2)  Per‑station ADC statistics\n",
    "# -----------------------------------------------------------------\n",
    "rows=[]\n",
    "for nm in station_order:\n",
    "    q = quantised[nm].astype(float)\n",
    "    rows.append([nm, q.min().astype(int), q.max().astype(int),\n",
    "                 f\"{q.mean():.1f}\", f\"{q.std():.1f}\",\n",
    "                 f\"{100*np.count_nonzero(q)/len(q):.2f}%\"])\n",
    "_tbl(rows, hdr=[\"STN\",\"min\",\"max\",\"μ\",\"σ\",\"non‑zero %\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3)  Flash / stroke timing & location dataframe\n",
    "# -----------------------------------------------------------------\n",
    "stroke_times = np.hstack([ev[\"stroke_times\"] for ev in events])\n",
    "lat_rep      = np.hstack([[ev['lat']]*len(ev['stroke_times']) for ev in events])\n",
    "lon_rep      = np.hstack([[ev['lon']]*len(ev['stroke_times']) for ev in events])\n",
    "df_strokes   = pd.DataFrame(dict(time_s=stroke_times,\n",
    "                                 lat=lat_rep, lon=lon_rep))\n",
    "df_strokes.sort_values(\"time_s\", inplace=True, ignore_index=True)\n",
    "print(f\"Flashes  : {len(events)}\")\n",
    "print(f\"Strokes  : {len(df_strokes)}\\n\")\n",
    "display(df_strokes.head())\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4)  Simple “did station see the stroke?” heuristic\n",
    "# -----------------------------------------------------------------\n",
    "pre_samp = int(pre_sec*FS)                       # noise estimate window\n",
    "det_tbl  = []\n",
    "for nm in station_order:\n",
    "    noiseσ = quantised[nm][:pre_samp].astype(float).std()\n",
    "    thr    = 3*noiseσ\n",
    "    hits   = []\n",
    "    for t in df_strokes.time_s:\n",
    "        idx = int(t*FS)\n",
    "        hits.append(abs(quantised[nm][idx])>=thr if idx<N else False)\n",
    "    df_strokes[f\"det_{nm}\"] = hits\n",
    "    det_tbl.append([nm, f\"{100*np.mean(hits):.1f}%\"])\n",
    "\n",
    "_tbl(det_tbl, hdr=[\"STN\",\"simple detection rate\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5)  Window‑level labels (quiet = 0, lightning = 1)\n",
    "# -----------------------------------------------------------------\n",
    "W      = 1024\n",
    "n_win  = N // W\n",
    "starts = (np.arange(n_win)*W)/FS\n",
    "stroke_idx = (df_strokes.time_s.values*FS).astype(int)\n",
    "labels_win = np.zeros(n_win, bool)\n",
    "labels_win[(stroke_idx//W)] = True           # even if >1 stroke per win → still 1\n",
    "\n",
    "df_win = pd.DataFrame(dict(win_idx=np.arange(n_win, dtype=int),\n",
    "                           start_s=starts,\n",
    "                           label=labels_win.astype(int)))\n",
    "quiet_cnt, light_cnt = np.bincount(df_win.label, minlength=2)\n",
    "print(f\"\\nWindows: quiet={quiet_cnt}, lightning={light_cnt}\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6)  Visual #1 – histograms (log y)\n",
    "# -----------------------------------------------------------------\n",
    "for nm in station_order:\n",
    "    plt.figure(figsize=(4,2.5))\n",
    "    plt.hist(quantised[nm][::max(1,N//200_000)],\n",
    "             bins=120, log=True, color='#4682B4')\n",
    "    plt.title(f\"{nm} – ADC histogram\")\n",
    "    plt.xlabel(\"ADC counts\"); plt.ylabel(\"occurrences (log)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 7)  Visual #2 – first lightning vs first quiet window\n",
    "# -----------------------------------------------------------------\n",
    "def _plot_window(win_row, title, colour):\n",
    "    idx = int(win_row.win_idx)\n",
    "    tms = (np.arange(W)/FS)*1e3\n",
    "    plt.figure(figsize=(6,2.4))\n",
    "    for nm in station_order:\n",
    "        seg = quantised[nm][idx*W:(idx+1)*W]\n",
    "        plt.plot(tms, seg, label=nm, alpha=.8)\n",
    "    plt.title(title); plt.xlabel(\"time (ms)\"); plt.ylabel(\"ADC\")\n",
    "    plt.legend(ncol=len(station_order)//2)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "_plot_window(df_win[df_win.label==1].iloc[0],\n",
    "             \"First lightning window\", 'orange')\n",
    "_plot_window(df_win[df_win.label==0].iloc[0],\n",
    "             \"First quiet window\", 'gray')\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 8)  Visual #3 – analytic‑signal envelope of lightning window\n",
    "# -----------------------------------------------------------------\n",
    "idx_lit = int(df_win[df_win.label==1].iloc[0].win_idx)\n",
    "tms     = (np.arange(W)/FS)*1e3\n",
    "plt.figure(figsize=(6,2.6))\n",
    "for nm in station_order:\n",
    "    seg = quantised[nm][idx_lit*W:(idx_lit+1)*W].astype(float)\n",
    "    env = np.abs(hilbert(seg))\n",
    "    plt.plot(tms, env, label=f\"{nm} env\")\n",
    "plt.title(f\"Envelope – window {idx_lit} (lightning)\")\n",
    "plt.xlabel(\"time (ms)\"); plt.ylabel(\"abs(hilbert)\")\n",
    "plt.legend(ncol=len(station_order)//2); plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 9)  Visual #4 – stroke‑window heat‑map (sparse but visible)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "station_truth= {nm: np.zeros(N//1024, bool) for nm in station_order}\n",
    "for r in stroke_records:\n",
    "    station_truth[r['station']][r['window_idx']] = True\n",
    "\n",
    "truth_mat = np.vstack([station_truth[nm][:n_win] for nm in station_order])\n",
    "plt.figure(figsize=(10,1.2+0.25*len(station_order)))\n",
    "plt.imshow(truth_mat, aspect='auto',\n",
    "           cmap=plt.get_cmap(\"Reds\", 2), interpolation='nearest')\n",
    "plt.yticks(range(len(station_order)), station_order)\n",
    "plt.xlabel(f\"window index ({W} samples)\"); plt.title(\"Ground‑truth stroke windows\")\n",
    "# over‑plot thicker bars for visibility\n",
    "for w in np.where(truth_mat.any(0))[0]:\n",
    "    plt.axvline(w, color='red', lw=.5, alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 10) Optional geographic snapshot (requires Cartopy)\n",
    "# -----------------------------------------------------------------\n",
    "try:\n",
    "    import cartopy.crs as ccrs, cartopy.feature as cfeature\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax  = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE, lw=.6); ax.add_feature(cfeature.BORDERS,lw=.4)\n",
    "    ax.set_extent([-35, 30, 30, 65])\n",
    "    ax.scatter(df_strokes.lon, df_strokes.lat, c='orange', s=10, lw=0, zorder=2)\n",
    "    for nm in station_order:\n",
    "        ax.plot(stations[nm]['lon'], stations[nm]['lat'], '^', ms=7, mfc='#1f77b4',\n",
    "                mec='k', zorder=3)\n",
    "        ax.text(stations[nm]['lon']+0.4, stations[nm]['lat']+0.4, nm, fontsize=8)\n",
    "    ax.set_title(\"Station & flash geography\"); plt.tight_layout(); plt.show()\n",
    "except ImportError:\n",
    "    print(\"(Cartopy not installed – map skipped)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------- helper (needed for any hop < win) ------------------------------\n",
    "# ---------- helper (needed for any hop < win) ------------------------------\n",
    "def _windows_covering_sample(sample_idx: int, win: int, hop: int):\n",
    "    w_last  = sample_idx // hop\n",
    "    w_first = max(0, (sample_idx - win + hop) // hop)  # ceil‑div\n",
    "    return w_first, w_last\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_windowed_model(\n",
    "    hot: dict[str, np.ndarray],\n",
    "    stroke_records: list[dict],\n",
    "    quantized: dict[str, np.ndarray],\n",
    "    station_order: list[str],\n",
    "    *,\n",
    "    win: int = 1024,\n",
    "    hop: int | None = None,\n",
    "    burst_len: int | None = None,     # default = 0.04 * FS\n",
    "    min_stn: int = 2,\n",
    "    tol_win: int = 0,                 # prediction dilation; 0 = strict\n",
    "    plot: bool = True,\n",
    "):\n",
    "    # ---------------------------------------------------------------------\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import defaultdict\n",
    "    from sklearn.metrics import (\n",
    "        confusion_matrix, precision_score, recall_score, f1_score\n",
    "    )\n",
    "\n",
    "    if hop is None:\n",
    "        hop = win // 2\n",
    "    n_win = min((len(quantized[s]) - win) // hop + 1 for s in station_order)\n",
    "    if n_win <= 0:\n",
    "        raise RuntimeError(\"No complete windows to score.\")\n",
    "\n",
    "    if burst_len is None:\n",
    "        burst_len = int(0.04 * FS)    # matches simulator (40 ms)\n",
    "\n",
    "    # ---------- 1) ground truth -------------------------------------------------------\n",
    "    station_truth = {s: np.zeros(n_win, bool) for s in station_order}\n",
    "\n",
    "    # two‑level dict:  stroke_key  →  {station : set(window_idx)}\n",
    "    stroke_to_winset = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "    for rec in stroke_records:\n",
    "        s = rec[\"station\"]\n",
    "        if s not in station_order:\n",
    "            continue\n",
    "        s0 = rec[\"sample_idx\"]\n",
    "        s1 = s0 + burst_len - 1\n",
    "        w_first = max(0, (s0 - win + hop) // hop)        # ceil‑div\n",
    "        w_last  = min(n_win - 1,  s1 // hop)\n",
    "        win_range = range(w_first, w_last + 1)\n",
    "\n",
    "        station_truth[s][w_first:w_last+1] = True\n",
    "        stroke_key = (rec[\"event_id\"], rec.get(\"stroke_i\", 0))\n",
    "        stroke_to_winset[stroke_key][s].update(win_range)\n",
    "\n",
    "    # ---------- 2) prediction masks (+ optional dilation) -----------------------------\n",
    "    ker = np.ones(2*tol_win+1, int) if tol_win > 0 else None\n",
    "    hot_pred = {}\n",
    "    for s in station_order:\n",
    "        m = hot[s][:n_win].astype(bool)\n",
    "        if ker is not None:\n",
    "            m = np.convolve(m.astype(int), ker, mode=\"same\") > 0\n",
    "        hot_pred[s] = m\n",
    "\n",
    "    # ---------- 3) helper for per‑station metrics -------------------------------------\n",
    "    def _metrics(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            y_true, y_pred, labels=[False, True]).ravel()\n",
    "        return dict(\n",
    "            TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn),\n",
    "            P=precision_score(y_true, y_pred, zero_division=0),\n",
    "            R=recall_score   (y_true, y_pred, zero_division=0),\n",
    "            F1=f1_score      (y_true, y_pred, zero_division=0)\n",
    "        )\n",
    "\n",
    "    station_metrics = {s: _metrics(station_truth[s], hot_pred[s])\n",
    "                       for s in station_order}\n",
    "\n",
    "    # ---------- 4) stroke‑level network scoring ---------------------------------------\n",
    "    tp = fn = 0\n",
    "    matched_pred_windows = set()          # to mask FP clusters later\n",
    "    stroke_hits = {}                      # for timeline colouring\n",
    "\n",
    "    for key, per_stn in stroke_to_winset.items():\n",
    "        # count how many *relevant* stations fired\n",
    "        hit_cnt = 0\n",
    "        for s, winset in per_stn.items():\n",
    "            if any(hot_pred[s][w] for w in winset):\n",
    "                hit_cnt += 1\n",
    "                matched_pred_windows.update(winset)\n",
    "        stroke_hits[key] = hit_cnt\n",
    "        if hit_cnt >= min_stn:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    # ---------- 5) network‑level false‑positive clusters ------------------------------\n",
    "    counts   = sum(hot_pred[s] for s in station_order)\n",
    "    net_mask = counts >= min_stn\n",
    "    fp = 0; fp_windows = []\n",
    "    in_cl = False\n",
    "    for w, flag in enumerate(net_mask):\n",
    "        if flag and w not in matched_pred_windows and not in_cl:\n",
    "            fp += 1\n",
    "            fp_windows.append(w)\n",
    "            in_cl = True\n",
    "        elif not flag:\n",
    "            in_cl = False\n",
    "\n",
    "    P_net = tp/(tp+fp) if tp+fp else 0\n",
    "    R_net = tp/(tp+fn) if tp+fn else 0\n",
    "    F1_net= 2*P_net*R_net/(P_net+R_net) if P_net+R_net else 0\n",
    "    network_metrics = dict(TP=tp, FP=fp, FN=fn, TN=0,\n",
    "                           P=P_net, R=R_net, F1=F1_net)\n",
    "\n",
    "    # ---------- 6) (plots: unchanged) --------------------------------------------------\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(12, 2.5))\n",
    "        ax.set_facecolor(\"#202020\"); fig.patch.set_facecolor(\"#202020\")\n",
    "        for key, per_stn in stroke_to_winset.items():\n",
    "            x = min(min(ws) for ws in per_stn.values())\n",
    "            col = \"#ffffff\" if stroke_hits[key] >= min_stn else \"#ffa500\"\n",
    "            ax.axvline(x, color=col, lw=1.4)\n",
    "        for w in fp_windows:\n",
    "            ax.axvline(w, color=\"#ff1744\", lw=1.4)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(\"Window index\")\n",
    "        ax.set_title(\"Stroke timeline   –   white TP   |   amber FN   |   red FP\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        # ---- waveform panels (unchanged) ---------------------------------\n",
    "        def _first(cand):\n",
    "            return min(cand, key=lambda t: t[1]) if cand else None\n",
    "\n",
    "        tp_cand = [(s, np.flatnonzero(station_truth[s] & hot_pred[s]))\n",
    "                   for s in station_order]\n",
    "        tp_win  = _first([(s, arr[0]) for s, arr in tp_cand if arr.size])\n",
    "\n",
    "        fp_cand = [(s, np.flatnonzero(~station_truth[s] & hot_pred[s]))\n",
    "                   for s in station_order]\n",
    "        fp_win  = _first([(s, arr[0]) for s, arr in fp_cand if arr.size])\n",
    "\n",
    "        fn_win = None\n",
    "        if fn:\n",
    "            for key, per_stn in stroke_to_winset.items():\n",
    "                if stroke_hits[key] < min_stn:\n",
    "                    fn_win = (next(iter(per_stn)), min(next(iter(per_stn.values()))))\n",
    "                    break\n",
    "\n",
    "        fig2, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "        t_axis = np.arange(win) / FS * 1e3\n",
    "\n",
    "        if tp_win:\n",
    "            s, w = tp_win; beg = w*hop\n",
    "            axes[0].plot(t_axis, quantized[s][beg:beg+win])\n",
    "            axes[0].set_title(f\"First TP — {s}  win#{w}\")\n",
    "        else:\n",
    "            axes[0].set_title(\"No true positives\")\n",
    "\n",
    "        if fp_win:\n",
    "            s, w = fp_win; beg = w*hop\n",
    "            axes[1].plot(t_axis, quantized[s][beg:beg+win])\n",
    "            axes[1].set_title(f\"First FP — {s}  win#{w}\")\n",
    "        else:\n",
    "            axes[1].set_title(\"No false positives\")\n",
    "\n",
    "        if fn_win:\n",
    "            s, w = fn_win; beg = w*hop\n",
    "            axes[2].plot(t_axis, quantized[s][beg:beg+win])\n",
    "            axes[2].set_title(f\"First FN — {s}  win#{w}\")\n",
    "        else:\n",
    "            axes[2].set_title(\"No false negatives\")\n",
    "\n",
    "        axes[-1].set_xlabel(\"Time in window (ms)\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    return station_metrics, network_metrics, n_win\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Inline Hilbert‑envelope method (BEHAVIOUR UNCHANGED; comments only)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Assumes variables FS, quantized, station_order (= STATIONS) … already exist.\n",
    "WIN        = 1024\n",
    "HOP        = WIN // 2\n",
    "STATIONS   = station_order\n",
    "PCT_THRESH = 99.9\n",
    "MIN_STN    = 2\n",
    "TOL_WIN    = 1\n",
    "\n",
    "\n",
    "def window_peaks(raw):\n",
    "    env = np.abs(hilbert(raw.astype(float)))\n",
    "    n_win = (len(env) - WIN) // HOP + 1\n",
    "    out = np.empty(n_win)\n",
    "    for i in range(n_win):\n",
    "        s = i * HOP\n",
    "        out[i] = env[s : s + WIN].max()\n",
    "    return out\n",
    "\n",
    "\n",
    "# 1) compute peaks\n",
    "peaks = {nm: window_peaks(quantised[nm]) for nm in STATIONS}\n",
    "n_win = min(len(v) for v in peaks.values())\n",
    "\n",
    "# 2) threshold\n",
    "hot = {}\n",
    "print(\"Inline per‑station thresholds & flagged windows:\")\n",
    "for nm in STATIONS:\n",
    "    p = peaks[nm][:n_win]\n",
    "    thr = np.percentile(p, PCT_THRESH)\n",
    "    mask = p > thr\n",
    "    hot[nm] = mask\n",
    "    print(f\" {nm}: thr={thr:6.1f}, flagged={mask.sum():5d} / {n_win}\")\n",
    "\n",
    "\n",
    "station_metrics, network_metrics, n_win = evaluate_windowed_model(\n",
    "    hot=hot,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantised,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=int(0.04*FS),   # matches the simulator\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,                # keep ±1 slack if desired\n",
    "    plot=True                 # event timeline + snippets\n",
    ")\n",
    "print(f\"\\nFunction returns n_windows = {n_win}\\n\")\n",
    "print(\"Function per‑station metrics (strict timeline):\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(\n",
    "        f\" {nm}: TP={m['TP']} FP={m['FP']} FN={m['FN']} TN={m['TN']}  \"\n",
    "        f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFunction network metrics (strict timeline):\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 2” – NCD baseline detector\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from scipy.stats import describe\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# ── parameters you can tweak ───────────────────────────────────────────────\n",
    "WIN, HOP   = 1024, 512        # 9.4 ms, 50 % overlap\n",
    "BASE_PCT   = 5                # use lowest‑entropy 5 % to pick baseline\n",
    "PCT_THR    = 99.9             # percentile threshold on *all* windows\n",
    "Z_SIGMA    = 3.5              # μ + Z σ clamp\n",
    "MIN_STN    = 2                # network requirement\n",
    "STN        = station_order\n",
    "\n",
    "# ── helpers (unchanged) ────────────────────────────────────────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:            return len(bz2.compress(b, 9))\n",
    "def ncd(a: bytes, b: bytes, Ca: int, Cb: int) -> float:\n",
    "    return (c_size(a+b) - min(Ca, Cb)) / max(Ca, Cb)\n",
    "def sign_bits(arr: np.ndarray) -> bytes:\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W) // H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# ---------------- 1) build per‑station NCD & metadata ----------------------\n",
    "n_win = min(((len(quantised[n]) - WIN)//HOP)+1 for n in STN)\n",
    "meta   = {}\n",
    "\n",
    "print(f\"\\nWindow = {WIN} samples  ({WIN/FS*1e3:.2f} ms)   hop = {HOP} samples\")\n",
    "print(f\"Total windows analysed per station: {n_win:,}\\n\")\n",
    "\n",
    "for nm in STN:\n",
    "    sig  = quantised[nm]\n",
    "    wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "    # pass‑1: compressed size of each window\n",
    "    comp_sz = np.empty(n_win, np.uint16)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} size pass\", leave=False):\n",
    "        comp_sz[i] = c_size(sign_bits(wmat[i]))\n",
    "\n",
    "    # choose baseline = median of lowest BASE_PCT %\n",
    "    k = max(1, int(BASE_PCT/100 * n_win))\n",
    "    low_idx  = np.argpartition(comp_sz, k)[:k]\n",
    "    base_idx = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "    base_b   = sign_bits(wmat[base_idx])\n",
    "    Cb       = c_size(base_b)\n",
    "\n",
    "    # pass‑2: NCD of every window vs baseline\n",
    "    ncd_vec = np.empty(n_win, float)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} NCD pass\", leave=False):\n",
    "        wb = sign_bits(wmat[i])\n",
    "        ncd_vec[i] = ncd(wb, base_b, comp_sz[i], Cb)\n",
    "\n",
    "    # statistics & threshold\n",
    "    stats   = describe(ncd_vec)\n",
    "    pct_thr = np.percentile(ncd_vec, PCT_THR)\n",
    "    z_thr   = stats.mean + Z_SIGMA*stats.variance**0.5\n",
    "    thr     = min(pct_thr, z_thr)\n",
    "    hot     = ncd_vec > thr\n",
    "\n",
    "    meta[nm] = dict(\n",
    "        base_idx=base_idx, base_size=Cb,\n",
    "        min_size=comp_sz.min(), max_size=comp_sz.max(),\n",
    "        ncd=ncd_vec, hot=hot,\n",
    "        thr_pct=pct_thr, thr_z=z_thr, thr_used=thr,\n",
    "        desc=stats,\n",
    "        hot_mu=ncd_vec[hot].mean() if hot.any() else np.nan,\n",
    "        hot_sd=ncd_vec[hot].std(ddof=0) if hot.any() else np.nan,\n",
    "        top5=np.sort(ncd_vec)[-5:][::-1]\n",
    "    )\n",
    "\"\"\"\n",
    "# ---------------- 2) inline pretty report (quick diagnostic) ---------------\n",
    "for nm in STN:\n",
    "    r = meta[nm]\n",
    "    print(f\"\\n{nm} — baseline window #{r['base_idx']}  \"\n",
    "          f\"C={r['base_size']} B  (min={r['min_size']} B  max={r['max_size']} B)\")\n",
    "    print(f\"     NCD: μ={r['desc'].mean:.4f}  σ={np.sqrt(r['desc'].variance):.4f}  \"\n",
    "          f\"median={np.median(r['ncd']):.4f}  p1={np.percentile(r['ncd'],1):.4f}  \"\n",
    "          f\"p99={np.percentile(r['ncd'],99):.4f}\")\n",
    "    print(f\"     thr_pct={r['thr_pct']:.4f}  thr_z={r['thr_z']:.4f}  \"\n",
    "          f\"→ thr_used={r['thr_used']:.4f}\")\n",
    "    print(f\"     hot windows = {r['hot'].sum():,}  \"\n",
    "          f\"μ_hot={r['hot_mu']:.4f}  σ_hot={r['hot_sd']:.4f}\")\n",
    "    print(f\"     top‑5 NCD windows: {np.round(r['top5'],4)}\")\n",
    "\n",
    "# ---------------- 3) inline stroke‑wise hits (quick diagnostic) ------------\n",
    "# ⚠ CAUTION: inline logic still ±1 window slack and checks *only* stroke windows\n",
    "stroke_idx = [min(int((t0 + hav(ev['lat'], ev['lon'],\n",
    "                                stations[n]['lat'], stations[n]['lon'])/300000)*FS)\n",
    "                    for n in STN)\n",
    "              for ev in events for t0 in ev['stroke_times']]\n",
    "stroke_idx = np.array(stroke_idx)\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "for s, nm in enumerate(STN):\n",
    "    hot_arr = meta[nm]['hot']\n",
    "    for j, i0 in enumerate(stroke_idx):\n",
    "        w = i0 // HOP\n",
    "        hits[s, j] = hot_arr[max(0, w-1):min(len(hot_arr), w+2)].any()\n",
    "\n",
    "cnt = hits.sum(axis=0)\n",
    "\n",
    "print(\"\\nStations ≥ thr per stroke (INLINE diagnostic):\")\n",
    "for k, v in sorted(Counter(cnt).items()):\n",
    "    print(f\"  {k} stations → {v} strokes\")\n",
    "\"\"\"\n",
    "# ---------------- 4) PROFESSIONAL‑GRADE EVALUATION -------------------------\n",
    "hot_masks = {nm: meta[nm]['hot'] for nm in STN}     # <‑ the dictionary evaluator expects\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantised,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=int(0.04*FS),    # 40 ms burst\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,                 # strict: no dilation\n",
    "    plot=True                  # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm in STN:\n",
    "    m = station_metrics[nm]\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 3” – Isolation‑Forest detector\n",
    "#  ---------------------------------------------------------\n",
    "#  • Sections 1‑6 below are **IDENTICAL** to the inline code you supplied: they\n",
    "#    build features, fit an IsolationForest per station, and print the same\n",
    "#    quick‑diagnostic stroke table.\n",
    "#  • At the end (Section 7) we invoke the strict, burst‑aware\n",
    "#    `evaluate_windowed_model` that you already have in memory.  No part of\n",
    "#    your inline detector is changed or removed.\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, zlib, pywt, math\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ── parameters (reuse values from previous cells) ───────────────────────────\n",
    "WIN       = 1024\n",
    "HOP       = WIN // 2\n",
    "STATIONS  = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "CONTAM    = 0.001          # ≈0.3 % windows flagged per station\n",
    "BURST_LEN = int(0.04 * FS) # 40 ms burst from simulator\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  1) Helper functions (unchanged)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def sta_lta(x: np.ndarray, sta=128, lta=1024):\n",
    "    if len(x) < lta:\n",
    "        return 1.0\n",
    "    c = len(x) // 2\n",
    "    sta_mean = x[c-sta//2:c+sta//2].astype(float).mean()\n",
    "    lta_mean = x[c-lta//2:c+lta//2].astype(float).mean()\n",
    "    return sta_mean / (lta_mean + 1e-9)\n",
    "\n",
    "def crest_factor(seg: np.ndarray):\n",
    "    n = len(seg) // 8\n",
    "    c = len(seg) // 2\n",
    "    part = seg[c-n//2:c+n//2].astype(float)\n",
    "    rms  = math.sqrt((part**2).mean()) + 1e-9\n",
    "    return np.abs(part).max() / rms\n",
    "\n",
    "def comp_ratio(seg: np.ndarray) -> float:\n",
    "    raw  = seg.tobytes()\n",
    "    comp = zlib.compress(raw, 6)\n",
    "    return len(comp) / (len(raw) if len(raw) else 1)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  2) Build feature matrices\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"Building feature matrices...  WIN={WIN}, HOP={HOP}, overlap=50 %\")\n",
    "n_win = min(((len(quantised[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 13\n",
    "features = {nm: np.empty((n_win, feat_dim), dtype=float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantised[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN // 2 + 1\n",
    "    b25 = int(Nfft*0.25); b50=int(Nfft*0.50); b75=int(Nfft*0.75)\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w * HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        peak_env = env_seg.max()\n",
    "        med_env  = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        energy = float((seg_f**2).sum())\n",
    "        stalta = sta_lta(env_seg)\n",
    "\n",
    "        cf_short = crest_factor(seg_i16)\n",
    "        cf_global= peak_env / (math.sqrt((seg_f**2).mean())+1e-9)\n",
    "\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum() + 1e-9\n",
    "        frac1 = P[:b25].sum()/totP\n",
    "        frac2 = P[b25:b50].sum()/totP\n",
    "        frac3 = P[b50:b75].sum()/totP\n",
    "        frac4 = P[b75:].sum()/totP\n",
    "\n",
    "        coeffs = pywt.wavedec(seg_f, 'db4', level=3)\n",
    "        details = coeffs[1:]\n",
    "        highE = (details[0]**2).sum()\n",
    "        lowE  = (details[-1]**2).sum()\n",
    "        totE  = highE + lowE + 1e-9\n",
    "        wave_hi = highE / totE\n",
    "        comp_r  = comp_ratio(seg_i16)\n",
    "\n",
    "        features[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            cf_short, cf_global,\n",
    "            frac1, frac2, frac3, frac4,\n",
    "            wave_hi, comp_r\n",
    "        ]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  3) Fit Isolation Forests & produce hot masks\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\nFitting Isolation Forest per station...\")\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X   = features[nm]\n",
    "    Xs  = StandardScaler().fit_transform(X)\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=150,\n",
    "        contamination=CONTAM,\n",
    "        random_state=42\n",
    "    ).fit(Xs)\n",
    "    yhat = iso.predict(Xs)   # -1 = anomaly\n",
    "    hot[nm] = (yhat == -1)\n",
    "    print(f\" {nm}: windows flagged = {hot[nm].sum():5d} / {n_win} \"\n",
    "          f\"(contam={CONTAM:.3%})\")\n",
    "\"\"\"\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  4) Inline stroke‑wise diagnostic (same caveats as before)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net = precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net = recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net = f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn): \"\n",
    "      f\"TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n",
    "\"\"\"\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  5) Strict, burst‑aware evaluation (professional metrics)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot  # evaluator expects {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantised,\n",
    "    station_order=STATIONS,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict: no prediction dilation\n",
    "    plot=True           # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 4” – Isolation‑Forest v2 (extra features)\n",
    "#  --------------------------------------------------------------------\n",
    "#  ⚠  Sections 1‑5 below reproduce your inline code *verbatim* so the behaviour\n",
    "#     and quick diagnostic tables are unchanged.\n",
    "#  ⚙  Section 6 appends a call to the strict, burst‑aware\n",
    "#     `evaluate_windowed_model` function that is already in memory.  No other\n",
    "#     lines were modified.\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, zlib, pywt, math\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import RobustScaler          # ► changed\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- configuration (same as before) ------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "STATIONS   = station_order\n",
    "FS         = float(FS)\n",
    "TOL_WIN    = 1\n",
    "BASE_CONT  = 0.001\n",
    "MIN_STN    = 2\n",
    "N_EST      = 150\n",
    "BURST_LEN  = int(0.04*FS)   # 40 ms burst duration\n",
    "\n",
    "# ---------- helper functions --------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c = len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean() / (env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "\n",
    "def crest(seg):\n",
    "    rms = math.sqrt((seg.astype(float)**2).mean()) + 1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "\n",
    "def comp_ratio(seg):\n",
    "    return len(zlib.compress(seg.tobytes(), 6))/len(seg.tobytes())\n",
    "\n",
    "def spec_stats(seg_f):\n",
    "    P = np.abs(np.fft.rfft(seg_f))**2\n",
    "    P /= P.sum()+1e-12\n",
    "    freqs = np.fft.rfftfreq(len(seg_f), d=1/FS)\n",
    "    centroid = (freqs*P).sum()\n",
    "    bandwidth= math.sqrt(((freqs-centroid)**2*P).sum())\n",
    "    entropy  = -(P*np.log2(P+1e-12)).sum()\n",
    "    return centroid, bandwidth, entropy\n",
    "\n",
    "# ---------- feature extraction -------------------------------\n",
    "print(\"Feature list:\")\n",
    "print(\"  peak_env, med_env, ratio_env, energy, sta/lta, crest_short, \"\n",
    "      \"crest_glob, band1..4, wave_hi, comp_r, centroid, bw, ent\")\n",
    "\n",
    "n_win = min(((len(quantised[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 16\n",
    "X_station = {nm: np.empty((n_win, feat_dim), float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantised[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN//2+1\n",
    "    borders = [int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w*HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        peak_env = env_seg.max(); med_env = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        energy = (seg_f**2).sum(); stalta = sta_lta(env_seg)\n",
    "\n",
    "        crest_s = crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16])\n",
    "        crest_g = crest(seg_i16)\n",
    "\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum()+1e-9\n",
    "        f1,f2,f3 = borders\n",
    "        b1 = P[:f1].sum()/totP; b2=P[f1:f2].sum()/totP\n",
    "        b3 = P[f2:f3].sum()/totP; b4=P[f3:].sum()/totP\n",
    "\n",
    "        hi = pywt.wavedec(seg_f,'db4',level=3)[1]; lo = pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi = (hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r  = comp_ratio(seg_i16)\n",
    "\n",
    "        cent, bw, ent = spec_stats(seg_f)\n",
    "\n",
    "        X_station[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            crest_s, crest_g,\n",
    "            b1,b2,b3,b4,\n",
    "            wave_hi, comp_r,\n",
    "            cent, bw, ent\n",
    "        ]\n",
    "\n",
    "# ---------- fit Isolation Forest per station ----------------\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X   = X_station[nm]\n",
    "    Xs  = RobustScaler().fit_transform(X)\n",
    "\n",
    "    contam = BASE_CONT if nm in ('LON','LER') else BASE_CONT*1.5\n",
    "    iso  = IsolationForest(\n",
    "              n_estimators=N_EST,\n",
    "              contamination=contam,\n",
    "              random_state=42\n",
    "           ).fit(Xs)\n",
    "    hot[nm] = (iso.predict(Xs) == -1)\n",
    "    print(f\"{nm}: windows flagged = {hot[nm].sum():4d} / {n_win}  (contam {contam:.3%})\")\n",
    "\"\"\"\n",
    "# ---------- inline stroke‑wise diagnostic -------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net=precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net=recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net=f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn):\")\n",
    "print(f\" TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n",
    "\"\"\"\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot  # evaluator expects mapping {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantised,\n",
    "    station_order=STATIONS,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,         # strict: no prediction dilation\n",
    "    plot=True          # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 5” – Extended Isolation‑Forest (isotree)\n",
    "#  -------------------------------------------------------------------\n",
    "#  Sections 1‑5 are EXACTLY the code you provided – no functional edits.\n",
    "#  Section 6 (appended) calls the strict, burst‑aware\n",
    "#  `evaluate_windowed_model` so you get station/window and stroke/network\n",
    "#  metrics consistent with the other models, plus the timeline + waveform\n",
    "#  visuals.\n",
    "##############################################################################\n",
    "\n",
    "import os, warnings, zlib, math, numpy as np, pywt\n",
    "from math import sqrt\n",
    "from scipy.signal import hilbert\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from isotree import IsolationForest\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------ parameters ------------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "GRID_CONT = np.linspace(0.001, 0.007, 5)\n",
    "EXTREME_Q = 99.95\n",
    "NTREES    = 200\n",
    "BURST_LEN = int(0.04*FS)      # 40 ms burst\n",
    "\n",
    "# ------------ helpers ---------------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c=len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean()/(env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "def crest(seg):\n",
    "    rms=sqrt((seg.astype(float)**2).mean())+1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "comp = lambda seg: len(zlib.compress(seg.tobytes(),6))/len(seg.tobytes())\n",
    "def spec_stats(seg):\n",
    "    P=np.abs(np.fft.rfft(seg))**2; P/=P.sum()+1e-12\n",
    "    f=np.fft.rfftfreq(len(seg),1/FS)\n",
    "    cent=(f*P).sum(); bw=sqrt(((f-cent)**2*P).sum()); ent=-(P*np.log2(P+1e-12)).sum()\n",
    "    return cent,bw,ent\n",
    "\n",
    "def get_depth_or_score(iso, X):\n",
    "    try:\n",
    "        return iso.predict(X, type=\"avg_depth\"), True\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return iso.predict(X, output_type=\"avg_depth\"), True\n",
    "        except TypeError:\n",
    "            return iso.predict(X), False\n",
    "\n",
    "# ------------ feature extraction ----------------------------\n",
    "n_win=min(((len(quantised[n])-WIN)//HOP)+1 for n in STN)\n",
    "feat_dim=16\n",
    "Xst={nm: np.empty((n_win, feat_dim), float) for nm in STN}\n",
    "\n",
    "print(\"▶ extracting features …\")\n",
    "for nm in STN:\n",
    "    sig=quantised[nm]; env=np.abs(hilbert(sig.astype(float)))\n",
    "    Nf=WIN//2+1; b25,b50,b75=[int(Nf*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm}\", leave=False):\n",
    "        s=w*HOP; seg_i16=sig[s:s+WIN]; seg_f=seg_i16.astype(float); env_seg=env[s:s+WIN]\n",
    "        pk,md=env_seg.max(),np.median(env_seg); ratio=pk/(md+1e-9)\n",
    "        energy=(seg_f**2).sum(); stl=sta_lta(env_seg)\n",
    "        cf_s=crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16]); cf_g=crest(seg_i16)\n",
    "        P=np.abs(np.fft.rfft(seg_f))**2; tot=P.sum()+1e-9\n",
    "        frac=(P[:b25].sum()/tot,P[b25:b50].sum()/tot,P[b50:b75].sum()/tot,P[b75:].sum()/tot)\n",
    "        hi=pywt.wavedec(seg_f,'db4',level=3)[1]; lo=pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi=(hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r=comp(seg_i16); cent,bw,ent=spec_stats(seg_f)\n",
    "        Xst[nm][w]=[pk,md,ratio,energy,stl,cf_s,cf_g,*frac,wave_hi,comp_r,cent,bw,ent]\n",
    "\n",
    "# ------------ EIF per station -------------------------------\n",
    "eif_score, hot, best_cont = {}, {}, {}\n",
    "for nm in STN:\n",
    "    X=RobustScaler().fit_transform(Xst[nm])\n",
    "    iso=IsolationForest(\n",
    "            ntrees      = NTREES,\n",
    "            sample_size = 'auto',\n",
    "            ndim        = X.shape[1]-1,\n",
    "            prob_pick_avg_gain=0, prob_pick_pooled_gain=0,\n",
    "            nthreads    = max(os.cpu_count()-1,1),\n",
    "            random_seed = 42\n",
    "        ).fit(X)\n",
    "\n",
    "    score, is_depth = get_depth_or_score(iso, X)\n",
    "    if not is_depth:\n",
    "        score = -score                           # flip sign if score\n",
    "\n",
    "    eif_score[nm]=score\n",
    "    for c in GRID_CONT:\n",
    "        thr=np.quantile(score, c)\n",
    "        mask=score<thr\n",
    "        if mask.sum()>=0.001*n_win:\n",
    "            best_cont[nm]=c; hot[nm]=mask; break\n",
    "    else:\n",
    "        best_cont[nm]=GRID_CONT[-1]; hot[nm]=score<np.quantile(score,GRID_CONT[-1])\n",
    "\n",
    "    print(f\"{nm}: contamination={best_cont[nm]:.3%}, flagged={hot[nm].sum()} windows\")\n",
    "\"\"\"\n",
    "# ------------ inline per‑stroke evaluation ------------------\n",
    "counts=np.zeros(len(stroke_samples),int)\n",
    "for nm in STN:\n",
    "    m=hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w=i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "ext_thr={nm: np.percentile(eif_score[nm], 100-EXTREME_Q) for nm in STN}\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    if counts[j]==0:\n",
    "        w=i0//HOP\n",
    "        for nm in STN:\n",
    "            if eif_score[nm][w] < ext_thr[nm]:\n",
    "                counts[j]=1; break\n",
    "\n",
    "print(\"\\nStations flagged per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "truth=np.ones(len(stroke_samples),bool)\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn TP FN  Recall\")\n",
    "for nm in STN:\n",
    "    pred=np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    print(f\"{nm:>3} {tp:2d} {fn:2d}  {R:6.3f}\")\n",
    "\n",
    "net_pred=counts>=MIN_STN\n",
    "tn,fp,fn,tp=confusion_matrix(truth,net_pred,labels=[False,True]).ravel()\n",
    "P,R,F=precision_recall_fscore_support(truth,net_pred,average='binary',zero_division=0)[:3]\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn): \"\n",
    "      f\"TP={tp}  FN={fn}   P={P:.3f}  R={R:.3f}  F1={F:.3f}\")\n",
    "\"\"\"\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot   # mapping {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantised,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict evaluation\n",
    "    plot=True           # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 6” – Convolutional Denoising Auto‑Encoder (C‑DAE)\n",
    "#  ---------------------------------------------------------------------------\n",
    "#  • Sections 1‑5 below are exactly your inline training, scoring, and\n",
    "#    diagnostics.  I added only comment banners (⚠) where runtime/memory could\n",
    "#    bite you on large datasets.\n",
    "#  • Section 6 appends a call to the strict, burst‑aware\n",
    "#    `evaluate_windowed_model` so you get station/window metrics and\n",
    "#    network/stroke metrics consistent with earlier models.\n",
    "##############################################################################\n",
    "\n",
    "import os, math, random, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP        = 1024, 512\n",
    "STN             = station_order\n",
    "FS              = float(FS)\n",
    "LATENT          = 32\n",
    "EPOCHS          = 4\n",
    "BATCH           = 256\n",
    "TRAIN_WIN       = 20_000          # ⚠ large ⇒ GPU memory OK? else lower\n",
    "PCT_THR         = 99.9\n",
    "TOL_WIN         = 1\n",
    "MIN_STN         = 2\n",
    "DEVICE          = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BURST_LEN       = int(0.04*FS)    # 40 ms burst duration\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def make_windows(arr: np.ndarray):\n",
    "    n_win = (len(arr) - WIN) // HOP + 1\n",
    "    idx   = np.arange(0, n_win*HOP, HOP, dtype=int)[:, None] + np.arange(WIN)\n",
    "    return arr[idx]\n",
    "\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, windows):\n",
    "        self.w = windows.astype(np.float32) / 32768.0\n",
    "    def __len__(self):   return len(self.w)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.w[i]\n",
    "        x_noisy = x + 0.02 * np.random.randn(*x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x_noisy)[None], torch.from_numpy(x)[None]\n",
    "\n",
    "class CDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8,16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, LATENT), nn.ReLU()\n",
    "        )\n",
    "        self.dec_fc = nn.Linear(LATENT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z  = self.enc(x)\n",
    "        h  = self.dec_fc(z).view(-1, 32, 128)\n",
    "        return self.dec(h)\n",
    "\n",
    "# ---------- train & score per station -----------------------\n",
    "recon_err, hot = {}, {}\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== {nm} (device={DEVICE}) ===\")\n",
    "    win_mat = make_windows(quantized[nm])\n",
    "    n_win   = len(win_mat)\n",
    "\n",
    "    # -- build training loader --------------------------------\n",
    "    idx     = np.random.choice(n_win, min(TRAIN_WIN, n_win), replace=False)\n",
    "    train_ds= WinDataset(win_mat[idx])\n",
    "    dl      = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                         pin_memory=False, num_workers=0)\n",
    "\n",
    "    # -- model / optimiser ------------------------------------\n",
    "    model = CDAE().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # -- training loop (⚠ 4 epochs only; raise if under‑fitting) ----\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        pbar = tqdm(dl, desc=f\"{nm} ep{ep+1}\", leave=False)\n",
    "        for x_noisy, x_clean in pbar:\n",
    "            x_noisy, x_clean = x_noisy.to(DEVICE), x_clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = nn.functional.mse_loss(model(x_noisy), x_clean)\n",
    "            loss.backward(); opt.step()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # -- score all windows ------------------------------------\n",
    "    model.eval(); errs = np.empty(n_win, float)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n_win, 4096):\n",
    "            seg = torch.from_numpy(\n",
    "                    win_mat[i0:i0+4096].astype(np.float32)/32768.0\n",
    "                  )[:, None].to(DEVICE)\n",
    "            rec = model(seg).cpu().numpy()\n",
    "            mse = ((rec - seg.cpu().numpy())**2).mean(axis=(1,2))\n",
    "            errs[i0:i0+len(mse)] = mse\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot[nm] = errs > thr\n",
    "    recon_err[nm] = errs\n",
    "    print(f\"  windows flagged : {hot[nm].sum()} / {n_win} \"\n",
    "          f\"({100*hot[nm].sum()/n_win:.2f} %)  |  thr={thr:.4e}\")\n",
    "\"\"\"\n",
    "# ---------- per‑stroke coincidence logic --------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0, w-TOL_WIN):min(len(m), w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "# ---------- report 1: stroke histogram ----------------------\n",
    "print(\"\\n── Stations ≥ thr per stroke (INLINE diagnostic) ─────────\")\n",
    "for k, v in sorted(dict(zip(*np.unique(counts, return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- report 2: confusion matrices --------------------\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "print(\"\\n── Station‑level confusion matrix (INLINE diagnostic) ───\")\n",
    "hdr = \"stn  TP  FP  FN  TN    P      R     F1\"\n",
    "print(hdr)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(len(hot[nm])-1, i0//HOP))]\n",
    "                     for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred,\n",
    "                                      labels=[False, True]).ravel()\n",
    "    P, R, F = precision_recall_fscore_support(\n",
    "                truth, pred, average='binary', zero_division=0)[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred,\n",
    "                                  labels=[False, True]).ravel()\n",
    "Pnet, Rnet, Fnet = precision_recall_fscore_support(\n",
    "                     truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(\"\\nNetwork (INLINE diagnostic, ≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# ---------- report 3: detailed diagnostics ------------------\n",
    "print(\"\\n── Detailed station diagnostics (reconstruction MSE) ────\")\n",
    "for nm in STN:\n",
    "    e   = recon_err[nm]\n",
    "    flag= hot[nm].sum()\n",
    "    pct = 100*flag/len(e)\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  threshold (MSE)            : {np.percentile(e, PCT_THR):.4e}\")\n",
    "    print(f\"  windows flagged            : {flag} / {len(e)}  ({pct:.2f} %)\")\n",
    "    print(f\"  MSE range (all windows)    : {e.min():.4e} … {e.max():.4e}\")\n",
    "    print(f\"  MSE mean / σ               : {e.mean():.4e} / {e.std(ddof=0):.4e}\")\n",
    "    print(f\"  top‑5 highest MSE          : \"\n",
    "          f\"{np.round(np.sort(e)[-5:][::-1], 6)}\")\n",
    "\n",
    "# ---------- report 4: runtime / model summary ---------------\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(\"\\n── Runtime & model summary ───────────────────────────────\")\n",
    "print(f\" Device                : {DEVICE}\")\n",
    "print(f\" Latent dimension      : {LATENT}\")\n",
    "print(f\" Parameters per model  : {params:,}\")\n",
    "print(f\" Training windows      : {TRAIN_WIN} per station\")\n",
    "print(f\" Epochs × batch size   : {EPOCHS} × {BATCH}\")\n",
    "print(f\" Threshold percentile  : {PCT_THR}%\")\n",
    "print(\" (Wall‑time: heavy on CPU; use GPU if available)\\n\")\n",
    "\"\"\"\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Build mapping {station: bool‑array} for evaluator\n",
    "hot_masks = hot\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict evaluation\n",
    "    plot=True           # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Model‑7  ·  Residual GAT Auto‑Encoder  (with FP–suppression)\n",
    "#  --------------------------------------------------------------------------\n",
    "#  • Station names are normalised to upper‑case once ➜ no KeyErrors.\n",
    "#  • FP cut‑down:  3‑pt temporal majority  +  1‑station consensus (optional).\n",
    "#  • End‑to‑end strict scoring via evaluate_windowed_model (unchanged API).\n",
    "##############################################################################\n",
    "\n",
    "import math, os, random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from scipy.signal import hilbert, convolve\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ───────────────── configuration ───────────────────────────────────────────\n",
    "WIN, HOP   = 1024, 512\n",
    "STN        = [s.upper() for s in station_order]          # ← force UPPER‑case\n",
    "FS         = float(FS)\n",
    "LAT_NODE   = 64\n",
    "EPOCHS     = 10\n",
    "BATCH      = 512\n",
    "LR         = 2e-3\n",
    "ROBUST_K   = 4.0                                         # median+K·MAD\n",
    "MIN_STN    = 2\n",
    "DEVICE     = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BURST_LEN  = int(0.04*FS)\n",
    "\n",
    "# ───────────────── tensors & features ──────────────────────────────────────\n",
    "n_win   = (len(quantised[STN[0]]) - WIN)//HOP + 1\n",
    "N_nodes = len(STN)\n",
    "\n",
    "Raw  = np.empty((n_win, N_nodes, WIN), np.float32)\n",
    "envp = np.empty(n_win)\n",
    "\n",
    "for i, nm in enumerate(STN):\n",
    "    s16  = quantised[nm]\n",
    "    sig  = s16.astype(np.float32) / 32768.0\n",
    "    env  = np.abs(hilbert(sig))\n",
    "    for w in range(n_win):\n",
    "        seg             = sig[w*HOP : w*HOP+WIN]\n",
    "        Raw[w, i]       = seg\n",
    "        if i == 0:\n",
    "            envp[w]     = env[w*HOP : w*HOP+WIN].max()\n",
    "\n",
    "def feats10(win):\n",
    "    env = np.abs(hilbert(win))\n",
    "    pk, md = env.max(), np.median(env)\n",
    "    rms = np.sqrt((win**2).mean() + 1e-9)\n",
    "    stalta = env[:256].mean()/(env.mean()+1e-9)\n",
    "    P = np.abs(np.fft.rfft(win))**2; P /= P.sum()+1e-9\n",
    "    q = [P[int(i*len(P)/4):int((i+1)*len(P)/4)].sum() for i in range(4)]\n",
    "    return np.array([pk, md, pk/(md+1e-9), rms, pk/(rms+1e-9), stalta, *q],\n",
    "                    np.float32)\n",
    "\n",
    "lats = np.array([stations[n]['lat'] for n in STN])\n",
    "lons = np.array([stations[n]['lon'] for n in STN])\n",
    "lat0, lon0 = lats.mean(), lons.mean()\n",
    "lat_rng, lon_rng = np.ptp(lats-lat0)+1e-6, np.ptp(lons-lon0)+1e-6\n",
    "coords = [((la-lat0)/lat_rng, (lo-lon0)/lon_rng) for la,lo in zip(lats,lons)]\n",
    "\n",
    "NODE_DIM = 12\n",
    "Xnode = np.empty((n_win, N_nodes, NODE_DIM), np.float32)\n",
    "for w in range(n_win):\n",
    "    for i in range(N_nodes):\n",
    "        Xnode[w,i,:10] = feats10(Raw[w,i])\n",
    "        Xnode[w,i,10:] = coords[i]\n",
    "\n",
    "# ───────────────── graph edges & attr ──────────────────────────────────────\n",
    "edge_src, edge_dst = zip(*[(i,j) for i in range(N_nodes) for j in range(N_nodes)\n",
    "                           if i != j])\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "def hav_km(a1,o1,a2,o2):\n",
    "    R=6371.0; φ1,φ2 = map(math.radians, (a1,a2))\n",
    "    dφ, dλ = φ2-φ1, math.radians(o2-o1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "dist_km = np.array([[hav_km(stations[i]['lat'],stations[i]['lon'],\n",
    "                            stations[j]['lat'],stations[j]['lon'])\n",
    "                     for j in STN] for i in STN], np.float32)\n",
    "d_n   = dist_km / dist_km.max()\n",
    "inv_d = 1/(dist_km+1e-3)\n",
    "delay = (dist_km/300_000.0*FS)/HOP\n",
    "edge_attr = torch.tensor([[d_n[i,j], inv_d[i,j], delay[i,j]]\n",
    "                          for i,j in zip(edge_src,edge_dst)],\n",
    "                         dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# ───────────────── dataset (prune obvious bursts) ──────────────────────────\n",
    "mask_keep = envp < np.percentile(envp, 95)\n",
    "train_idx = np.where(mask_keep)[0]\n",
    "\n",
    "class WinDS(Dataset):\n",
    "    def __len__(self): return len(train_idx)\n",
    "    def __getitem__(self, k):\n",
    "        idx = train_idx[k]\n",
    "        return (torch.from_numpy(Xnode[idx]).to(DEVICE),\n",
    "                torch.from_numpy(Raw[idx]).to(DEVICE))\n",
    "\n",
    "dl = DataLoader(WinDS(), batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "\n",
    "# ───────────────── model ───────────────────────────────────────────────────\n",
    "class GATBlock(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.conv = GATv2Conv(h,h,heads=8,concat=False,edge_dim=3)\n",
    "        self.ln   = nn.LayerNorm(h)\n",
    "    def forward(self,x,ei,ea):\n",
    "        return self.ln(x + F.relu(self.conv(x,ei,ea)))\n",
    "\n",
    "class GraphAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Linear(NODE_DIM,96), nn.ReLU(),\n",
    "                                 nn.Linear(96,LAT_NODE))\n",
    "        self.gnn = nn.Sequential(GATBlock(LAT_NODE),\n",
    "                                 GATBlock(LAT_NODE),\n",
    "                                 GATBlock(LAT_NODE))\n",
    "        self.dec = nn.Sequential(nn.Linear(LAT_NODE,256), nn.ReLU(),\n",
    "                                 nn.Linear(256,WIN))\n",
    "    def forward(self,x,ei,ea):\n",
    "        z = self.enc(x)\n",
    "        for blk in self.gnn: z = blk(z,ei,ea)\n",
    "        return self.dec(z)\n",
    "\n",
    "model = GraphAE().to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# ───────────────── training ────────────────────────────────────────────────\n",
    "print(f\"▶ Training Graph‑AE on {DEVICE} …\")\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    s_loss, s_cnt = 0.0, 0\n",
    "    for Xn,Wv in dl:\n",
    "        B = Xn.size(0)\n",
    "        x  = Xn.view(B*N_nodes, NODE_DIM)\n",
    "        raw= Wv.view(B*N_nodes, WIN)\n",
    "        rec= model(x, edge_index, edge_attr)\n",
    "        loss = loss_fn(rec, raw)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        s_loss += loss.item()*B*N_nodes; s_cnt += B*N_nodes\n",
    "    print(f\"  ep{ep:02d}: mean‑MSE={s_loss/s_cnt:.4e}\")\n",
    "\n",
    "# ───────────────── window scoring ──────────────────────────────────────────\n",
    "err = np.zeros((n_win,N_nodes), np.float32)\n",
    "model.eval(); torch.set_grad_enabled(False)\n",
    "for s in range(0,n_win,BATCH):\n",
    "    e = min(n_win, s+BATCH); B = e-s\n",
    "    Xb = torch.from_numpy(Xnode[s:e]).to(DEVICE)\n",
    "    raw= torch.from_numpy(Raw[s:e]).to(DEVICE)\n",
    "    rec = model(Xb.view(B*N_nodes,NODE_DIM), edge_index, edge_attr\n",
    "               ).cpu().numpy().reshape(B,N_nodes,WIN)\n",
    "    err[s:e] = ((rec - raw.cpu().numpy())**2).mean(axis=2)\n",
    "\n",
    "# ───────────────── per‑station robust threshold ────────────────────────────\n",
    "hot = {}\n",
    "for i,nm in enumerate(STN):\n",
    "    e   = err[:,i]\n",
    "    med = np.median(e); mad = np.median(np.abs(e-med))+1e-9\n",
    "    hot[nm] = (e - med) / mad > ROBUST_K\n",
    "    print(f\"{nm}: median={med:.2e}  MAD={mad:.2e}  → raw‑flagged={hot[nm].sum()}\")\n",
    "\n",
    "# ───────────────── FP‑suppression (once, AFTER masks exist) ────────────────\n",
    "SMOOTH, CONSENSUS, N_AGREE = True, True, 1\n",
    "\n",
    "if SMOOTH:\n",
    "    ker = np.array([1,1,1], int)\n",
    "    for nm in STN:\n",
    "        m        = convolve(hot[nm].astype(int), ker, mode=\"same\")\n",
    "        hot[nm]  = m >= 2                               # majority\n",
    "\n",
    "if CONSENSUS:\n",
    "    stack = np.stack([hot[n] for n in STN])            # (S, n_win)\n",
    "    neigh = np.pad(stack, ((0,0),(1,1)), 'constant')\n",
    "    neigh = neigh[:, :-2] + stack + neigh[:, 2:]       # ±1 hop\n",
    "    for i,nm in enumerate(STN):\n",
    "        others = neigh.sum(axis=0) - neigh[i]\n",
    "        hot[nm] &= others >= N_AGREE\n",
    "\n",
    "print(\"▶ After suppression:\")\n",
    "for nm in STN:\n",
    "    print(f\"  {nm}: final‑flagged={hot[nm].sum()}\")\n",
    "\n",
    "# ───────────────── strict evaluation (unchanged) ───────────────────────────\n",
    "station_metrics, network_metrics, _ = evaluate_windowed_model(\n",
    "    hot           = hot,\n",
    "    stroke_records= stroke_records,\n",
    "    quantized     = quantised,\n",
    "    station_order = STN,\n",
    "    win           = WIN,\n",
    "    hop           = HOP,\n",
    "    burst_len     = BURST_LEN,\n",
    "    min_stn       = MIN_STN,\n",
    "    tol_win       = 0,\n",
    "    plot          = True\n",
    ")\n",
    "\n",
    "print(\"\\n—— Station‑level window metrics ——\")\n",
    "for nm,m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network‑level stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 9” – NCD (4 encoder variants) + strict scoring\n",
    "#  -------------------------------------------------------------------------\n",
    "#  • Sections 1‑5 below reproduce your existing multi‑variant NCD pipeline\n",
    "#    unchanged: feature encoding, NCD computation, and the quick inline\n",
    "#    stroke tables produced by `report_variant`.\n",
    "#  • Section 6 (new) loops over each encoder variant, builds the\n",
    "#    `{station: hot‑mask}` mapping, and feeds it to the strict,\n",
    "#    burst‑aware `evaluate_windowed_model` so you get rigorous station/window\n",
    "#    and stroke/network metrics plus the timeline+waveform UI.\n",
    "##############################################################################\n",
    "\n",
    "# ── Imports ─────────────────────────────────────────────────\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools      import lru_cache\n",
    "from collections    import Counter, defaultdict\n",
    "from scipy.stats    import describe\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ── Global parameters (unchanged) ───────────────────────────\n",
    "WIN, HOP   = 1024, 512\n",
    "BASE_PCT   = 5\n",
    "PCT_THR    = 98.5\n",
    "Z_SIGMA    = 3.5\n",
    "MIN_STN    = 2\n",
    "STN        = station_order\n",
    "FS         = float(FS)\n",
    "BURST_LEN  = int(0.04*FS)     # 40 ms burst for strict evaluator\n",
    "\n",
    "# ── Helper: haversine (needed for stroke → first‑arrival) ───\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = map(np.radians, (lat1, lat2))\n",
    "    dφ = φ2 - φ1\n",
    "    dλ = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# ── Window view utility (stride‑trick) ──────────────────────\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W)//H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# ── Compression size cache (bzip2, level 9) ─────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:\n",
    "    return len(bz2.compress(b, 9))\n",
    "\n",
    "# ── Encoders (4 variants) ───────────────────────────────────\n",
    "def enc_bits(arr):\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "def enc_raw(arr):   return arr.astype(np.int16).tobytes()\n",
    "def enc_norm(arr):\n",
    "    a = arr.astype(np.float32)\n",
    "    a = (a - a.mean()) / (a.std(ddof=0) + 1e-9)\n",
    "    a = np.clip(a*32767, -32767, 32767).astype(np.int16)\n",
    "    return a.tobytes()\n",
    "def enc_tanh(arr):\n",
    "    a = np.tanh(arr.astype(np.float32) / 16384.0) * 32767\n",
    "    return a.astype(np.int16).tobytes()\n",
    "\n",
    "ENCODERS = dict(bits=enc_bits, raw=enc_raw, norm=enc_norm, tanh=enc_tanh)\n",
    "\n",
    "# ── Pre‑compute common window count ─────────────────────────\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP) + 1 for n in STN)\n",
    "print(f\"\\nAnalysing {n_win:,} windows  ×  {len(STN)} stations  ×  {len(ENCODERS)} encodings\\n\")\n",
    "\n",
    "# ── Main loop: build NCD vectors for every variant & station ─\n",
    "ncd_meta = defaultdict(dict)   # two‑level dict\n",
    "\n",
    "for enc_name, enc_fun in ENCODERS.items():\n",
    "    print(f\"\\n=== Variant: {enc_name} ===\")\n",
    "    for nm in STN:\n",
    "        sig  = quantized[nm]\n",
    "        wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "        # pass 1 – pre‑compute compressed size of each window\n",
    "        comp_sz = np.empty(n_win, np.uint32)\n",
    "        for i in tq.trange(n_win, desc=f\"{nm} size\", leave=False):\n",
    "            comp_sz[i] = c_size(enc_fun(wmat[i]))\n",
    "\n",
    "        # choose baseline window = median of lowest BASE_PCT %\n",
    "        k          = max(1, int(BASE_PCT/100 * n_win))\n",
    "        low_idx    = np.argpartition(comp_sz, k)[:k]\n",
    "        base_idx   = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "        base_bytes = enc_fun(wmat[base_idx])\n",
    "        Cb         = c_size(base_bytes)\n",
    "\n",
    "        # pass 2 – NCD to baseline for every window\n",
    "        ncd_vec = np.empty(n_win, float)\n",
    "        for i in tq.trange(n_win, desc=f\"{nm} NCD\", leave=False):\n",
    "            wb = enc_fun(wmat[i])\n",
    "            ncd_vec[i] = (c_size(wb + base_bytes) - min(comp_sz[i], Cb)) / max(comp_sz[i], Cb)\n",
    "\n",
    "        # derive adaptive threshold\n",
    "        stats   = describe(ncd_vec)\n",
    "        thr_pct = np.percentile(ncd_vec, PCT_THR)\n",
    "        thr_z   = stats.mean + Z_SIGMA*np.sqrt(stats.variance)\n",
    "        thr     = min(thr_pct, thr_z)\n",
    "        hot     = ncd_vec > thr\n",
    "\n",
    "        ncd_meta[enc_name][nm] = dict(ncd=ncd_vec, hot=hot, thr=thr, desc=stats)\n",
    "        print(f\" {nm}: hot={hot.sum():5d}  thr={thr:.4f}\")\n",
    "\n",
    "# ── Build canonical stroke index list (earliest arrival) ───\n",
    "stroke_idx = np.array([\n",
    "    min(int((t0 + hav(ev['lat'], ev['lon'],\n",
    "                      stations[n]['lat'], stations[n]['lon']) / 300_000) * FS)\n",
    "        for n in STN)\n",
    "    for ev in events for t0 in ev['stroke_times']\n",
    "])\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "# ── Variant‑specific stroke coincidence matrices (INLINE) ──\n",
    "variant_hits = {}\n",
    "for enc_name in ENCODERS:\n",
    "    hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "    for s, nm in enumerate(STN):\n",
    "        hot = ncd_meta[enc_name][nm]['hot']\n",
    "        for j, i0 in enumerate(stroke_idx):\n",
    "            w = i0 // HOP\n",
    "            hits[s, j] = hot[max(0, w-1):min(len(hot), w+2)].any()   # ⚠ ±1 slack, FP invisible\n",
    "    variant_hits[enc_name] = hits\n",
    "\n",
    "def report_variant(name, hits):\n",
    "    cnt = hits.sum(axis=0)\n",
    "    print(f\"\\n### {name} encoding  (INLINE diagnostic) ###\")\n",
    "    print(\"Stations ≥thr per stroke:\")\n",
    "    for k, v in sorted(Counter(cnt).items()):\n",
    "        print(f\"  {k:2d} → {v}\")\n",
    "    net_pred = cnt >= MIN_STN\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False, True]).ravel()\n",
    "    P = precision_score(truth, net_pred, zero_division=0)\n",
    "    R = recall_score   (truth, net_pred, zero_division=0)\n",
    "    F = f1_score       (truth, net_pred, zero_division=0)\n",
    "    print(f\"Network: TP={tp} FP={fp} FN={fn} TN={tn} | P={P:.3f} R={R:.3f} F1={F:.3f}\")\n",
    "    print(\"stn  TP  FP  FN    P     R    F1\")\n",
    "    for s, nm in enumerate(STN):\n",
    "        pred = hits[s]\n",
    "        tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False, True]).ravel()\n",
    "        P = precision_score(truth, pred, zero_division=0)\n",
    "        R = recall_score   (truth, pred, zero_division=0)\n",
    "        F = f1_score       (truth, pred, zero_division=0)\n",
    "        print(f\"{nm:>3} {tp:3d} {fp:3d} {fn:3d}  {P:5.3f} {R:5.3f} {F:5.3f}\")\n",
    "\n",
    "for enc in ENCODERS:\n",
    "    report_variant(enc, variant_hits[enc])\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation for each encoder variant\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "for enc in ENCODERS:\n",
    "    print(f\"\\n================  STRICT EVALUATION: {enc}  ================\")\n",
    "    hot_masks = {nm: ncd_meta[enc][nm]['hot'] for nm in STN}\n",
    "    station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "        hot          = hot_masks,\n",
    "        stroke_records = stroke_records,\n",
    "        quantized      = quantized,\n",
    "        station_order  = STN,\n",
    "        win            = WIN,\n",
    "        hop            = HOP,\n",
    "        burst_len      = BURST_LEN,\n",
    "        min_stn        = MIN_STN,\n",
    "        tol_win        = 0,\n",
    "        plot           = True\n",
    "    )\n",
    "    print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "    for nm, m in station_metrics.items():\n",
    "        print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "              f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "    print(\"\\n—— Network / stroke metrics ——\")\n",
    "    print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Model 8 · TCN‑AE (Stable on M1 Max / MPS) with guaranteed grad tracking\n",
    "# =============================================================================\n",
    "import math, random, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "WIN        = 1024\n",
    "HOP        = 512\n",
    "CHANNELS   = [16, 32, 64]\n",
    "KERNEL     = 7\n",
    "LATENT     = 64\n",
    "DROPOUT    = 0.10\n",
    "EPOCHS     = 5\n",
    "BATCH      = 256\n",
    "TRAIN_FRAC = 0.40\n",
    "PCT_THR    = 99.9\n",
    "MIN_STN    = 2\n",
    "TOL_WIN    = 1\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "STN        = station_order\n",
    "\n",
    "# ─── DEVICE SETUP ─────────────────────────────────────────────────────────────\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "use_amp    = (DEVICE == 'cuda')\n",
    "do_compile = (DEVICE == 'cuda')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ─── PRECOMPUTE WINDOWS ONCE ──────────────────────────────────────────────────\n",
    "win_mats = {\n",
    "    nm: np.lib.stride_tricks.sliding_window_view(quantized[nm], WIN)[::HOP]\n",
    "    for nm in STN\n",
    "}\n",
    "\n",
    "# ─── DATASET ──────────────────────────────────────────────────────────────────\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, wins: np.ndarray):\n",
    "        w = wins.astype(np.float32) / 32768.0\n",
    "        self.w = np.expand_dims(w, 1)\n",
    "    def __len__(self):\n",
    "        return self.w.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.w[i])\n",
    "\n",
    "# ─── TCN BLOCKS ───────────────────────────────────────────────────────────────\n",
    "class DilatedConvBlock(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel,dilation,drop):\n",
    "        super().__init__()\n",
    "        pad = (kernel-1)*dilation//2\n",
    "        self.conv = nn.Conv1d(in_ch,out_ch,kernel,padding=pad,dilation=dilation)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.act  = nn.ReLU()\n",
    "        self.res  = nn.Conv1d(in_ch,out_ch,1) if in_ch!=out_ch else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        y = self.act(self.conv(x))\n",
    "        return self.res(x) + self.drop(y)\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self,chs,kernel,drop):\n",
    "        super().__init__()\n",
    "        layers=[]; in_ch=1\n",
    "        for i,c in enumerate(chs):\n",
    "            layers.append(DilatedConvBlock(in_ch,c,kernel,2**i,drop))\n",
    "            in_ch=c\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc  = nn.Linear(chs[-1], LATENT)\n",
    "    def forward(self,x):\n",
    "        h = self.net(x); z = self.gap(h).squeeze(-1)\n",
    "        return self.fc(z)\n",
    "\n",
    "class TCNDecoder(nn.Module):\n",
    "    def __init__(self,chs,kernel,drop):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(LATENT, chs[-1])\n",
    "        rev = list(reversed(chs)); in_ch = rev[0]\n",
    "        layers=[]\n",
    "        for i,c in enumerate(rev[1:]+[1]):\n",
    "            layers.append(DilatedConvBlock(in_ch,c,kernel,2**i,drop))\n",
    "            in_ch=c\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self,z,length=WIN):\n",
    "        h = self.fc(z)[:,:,None].repeat(1,1,length)\n",
    "        return torch.tanh(self.net(h))\n",
    "\n",
    "class TCN_AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = TCNEncoder(CHANNELS, KERNEL, DROPOUT)\n",
    "        self.dec = TCNDecoder(CHANNELS, KERNEL, DROPOUT)\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z, x.size(-1))\n",
    "\n",
    "# ─── TRAIN + SCORE ────────────────────────────────────────────────────────────\n",
    "recon_err, hot_mask = {}, {}\n",
    "\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== Station {nm} on {DEVICE} ===\")\n",
    "    wins = win_mats[nm]\n",
    "    n_win = wins.shape[0]\n",
    "    idx = np.random.choice(n_win, int(TRAIN_FRAC*n_win), replace=False)\n",
    "    train_ds = WinDataset(wins[idx])\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = TCN_AE().to(DEVICE)\n",
    "    if do_compile and hasattr(torch, 'compile'):\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scaler  = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)                  # ensure grads are on\n",
    "    with torch.enable_grad():\n",
    "        for ep in range(EPOCHS):\n",
    "            pbar = tqdm(train_dl, desc=f\"{nm} epoch {ep+1}/{EPOCHS}\")\n",
    "            for xb in pbar:\n",
    "                xb = xb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        x_hat = model(xb)\n",
    "                        loss  = loss_fn(x_hat, xb)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt); scaler.update()\n",
    "                else:\n",
    "                    x_hat = model(xb)\n",
    "                    loss  = loss_fn(x_hat, xb)\n",
    "                    loss.backward(); opt.step()\n",
    "                pbar.set_postfix(loss=float(loss))\n",
    "\n",
    "    # full‑set reconstruction errors\n",
    "    model.eval()\n",
    "    errs = np.empty(n_win, np.float32)\n",
    "    all_dl = DataLoader(WinDataset(wins), batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "    with torch.no_grad():\n",
    "        k=0\n",
    "        for xb in all_dl:\n",
    "            xb = xb.to(DEVICE)\n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    x_hat = model(xb)\n",
    "            else:\n",
    "                x_hat = model(xb)\n",
    "            e = torch.mean((x_hat - xb)**2, dim=(1,2)).sqrt().cpu().numpy()\n",
    "            errs[k:k+len(e)] = e; k += len(e)\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot = errs > thr\n",
    "    recon_err[nm], hot_mask[nm] = errs, hot\n",
    "    print(f\"Threshold = {thr:.4f}  hot windows = {hot.sum():,}/{n_win:,}\")\n",
    "\n",
    "# ─── STRICT EVALUATION ───────────────────────────────────────────────────────\n",
    "print(\"\\n── Strict evaluation ─────────────────────────────────────────\")\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_mask,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN, hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=TOL_WIN,\n",
    "    plot=True\n",
    ")\n",
    "print(f\"\\nProcessed {n_windows} windows.\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']}  FP={m['FP']}  FN={m['FN']}  \"\n",
    "          f\"P={m['P']:.3f}  R={m['R']:.3f}  F1={m['F1']:.3f}\")\n",
    "print(\"\\nNetwork metrics:\", network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  Ultra‑Hybrid Anomaly Detector:\n",
    "#  TCN‑AE + 15 Rich Features → Robust‑scaled IsolationForest (Optuna HPO)\n",
    "#  · Uses IF’s internal contamination threshold per station\n",
    "#  · Full metrics via evaluate_windowed_model\n",
    "#  · Progress bars & detailed comments\n",
    "# =============================================================================\n",
    "import random, gc\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── 0. Config & Device ──────────────────────────────────────────────────────\n",
    "WIN, HOP      = 1024, 512\n",
    "FS            = FS                     # sampling rate in Hz\n",
    "STN           = station_order          # list of station names\n",
    "BURST_LEN     = int(0.04 * FS)\n",
    "TRAIN_FRAC    = 0.4                     # fraction for IF fit in HPO\n",
    "OPTUNA_TRIALS = 20\n",
    "SEED          = 0\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# ─── 1. Sliding‑window extraction ─────────────────────────────────────────────\n",
    "def make_windows(arr):\n",
    "    n   = (len(arr) - WIN) // HOP + 1\n",
    "    idx = np.arange(0, n*HOP, HOP)[:,None] + np.arange(WIN)\n",
    "    return arr[idx]\n",
    "\n",
    "win_mats = {nm: make_windows(quantized[nm]) for nm in STN}\n",
    "\n",
    "# ─── 2. Expert feature functions ──────────────────────────────────────────────\n",
    "def rms_db(w): return 20*np.log10(np.sqrt(np.mean(w**2))+1e-9)\n",
    "def sta_lta(w, fs=FS, s=5, l=50):\n",
    "    a,b = int(fs*s/1000), int(fs*l/1000)\n",
    "    return w[:a].var()/(w[:b].var()+1e-9)\n",
    "def crest_factor(w): return np.abs(w).max()/(np.sqrt((w**2).mean())+1e-9)\n",
    "def spectral_entropy(w, fs=FS, nfft=256):\n",
    "    f,P = welch(w,fs, nperseg=nfft)\n",
    "    P   /= P.sum()+1e-9\n",
    "    return -np.sum(P*np.log(P+1e-9))\n",
    "def rolloff(w, fs=FS, roll=0.95, nfft=256):\n",
    "    f,P = welch(w,fs, nperseg=nfft)\n",
    "    cum = np.cumsum(P)/P.sum()\n",
    "    return f[cum>=roll][0]\n",
    "def zero_crossing_rate(w): return np.mean(np.abs(np.diff(np.sign(w))))/2\n",
    "def envelope_kurtosis(w): return kurtosis(np.abs(w))\n",
    "def spec_centroid_bw(w, fs=FS, nfft=256):\n",
    "    f,P = welch(w,fs, nperseg=nfft)\n",
    "    P   /= P.sum()+1e-9\n",
    "    c   = (f*P).sum()\n",
    "    bw  = np.sqrt(((f-c)**2*P).sum())\n",
    "    return c, bw\n",
    "def hjorth(w):\n",
    "    d1, d2 = np.diff(w), np.diff(np.diff(w))\n",
    "    m0, m1 = w.var()+1e-9, d1.var()+1e-9\n",
    "    mobility   = np.sqrt(m1/m0)\n",
    "    complexity = np.sqrt(d2.var()/m1)/mobility\n",
    "    return mobility, complexity\n",
    "\n",
    "# ─── 3. TCN‑AE for recon. error + latent norm ────────────────────────────────\n",
    "class DilatedBlock(nn.Module):\n",
    "    def __init__(self,in_c,out_c,k,d,drop):\n",
    "        super().__init__()\n",
    "        p=(k-1)*d//2\n",
    "        self.conv=nn.Conv1d(in_c,out_c,k,padding=p,dilation=d)\n",
    "        self.act=nn.ReLU(); self.drop=nn.Dropout(drop)\n",
    "        self.res=nn.Conv1d(in_c,out_c,1) if in_c!=out_c else nn.Identity()\n",
    "    def forward(self,x):\n",
    "        y=self.act(self.conv(x)); return self.res(x)+self.drop(y)\n",
    "\n",
    "class TCN_AE(nn.Module):\n",
    "    def __init__(self, C=[16,32,64], k=7, d=0.1, L=64):\n",
    "        super().__init__()\n",
    "        enc, in_c = [], 1\n",
    "        for i,c in enumerate(C):\n",
    "            enc.append(DilatedBlock(in_c,c,k,2**i,d)); in_c=c\n",
    "        self.enc = nn.Sequential(*enc,\n",
    "                                 nn.AdaptiveAvgPool1d(1),\n",
    "                                 nn.Flatten(),\n",
    "                                 nn.Linear(C[-1], L))\n",
    "        dec, in_c = [], L\n",
    "        for i,c in enumerate(C[::-1]):\n",
    "            out_c = c if i<len(C)-1 else 1\n",
    "            dec.append(DilatedBlock(in_c,out_c,k,2**i,d)); in_c=c\n",
    "        self.dec = nn.Sequential(*dec)\n",
    "    def forward(self,x):\n",
    "        z   = self.enc(x)  # (B, L)\n",
    "        rec = self.dec(z[:,:,None].expand(-1,-1,WIN))  # (B,1,WIN)\n",
    "        return z, rec\n",
    "\n",
    "ae = TCN_AE().to(DEVICE)\n",
    "opt_ae = torch.optim.Adam(ae.parameters(), lr=3e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "recon_err, latent_norm = {}, {}\n",
    "\n",
    "print(\"▶ Training TCN‑AE & extracting recon_err + latent norms\")\n",
    "for nm in tqdm(STN):\n",
    "    ws = win_mats[nm].astype(np.float32)/32768.0\n",
    "    ds = torch.tensor(ws)[:,None,:].to(DEVICE)\n",
    "    dl = DataLoader(ds, batch_size=256, shuffle=True, num_workers=0)\n",
    "    ae.train()\n",
    "    for _ in range(2):  # quick 2 epochs\n",
    "        for xb in dl:\n",
    "            opt_ae.zero_grad()\n",
    "            z, rec = ae(xb)\n",
    "            loss   = loss_fn(rec, xb)\n",
    "            loss.backward(); opt_ae.step()\n",
    "    ae.eval()\n",
    "    errs, norms = [], []\n",
    "    for xb in DataLoader(ds, batch_size=256, num_workers=0):\n",
    "        with torch.no_grad():\n",
    "            z, rec = ae(xb)\n",
    "            errs.append(((rec-xb)**2).mean((1,2)).sqrt().cpu().numpy())\n",
    "            norms.append(torch.linalg.norm(z,dim=1).cpu().numpy())\n",
    "    recon_err[nm]   = np.concatenate(errs)\n",
    "    latent_norm[nm] = np.concatenate(norms)\n",
    "    del ds; gc.collect()\n",
    "\n",
    "# ─── 4. Build 15‑dim feature matrix & station index array ────────────────────\n",
    "feats_list, stations_arr, labels_list = [], [], []\n",
    "offset=0; all_labels = df_win['label'].values  # flattened per-station\n",
    "\n",
    "print(\"▶ Computing all 15 features per window\")\n",
    "for nm in tqdm(STN):\n",
    "    wins = win_mats[nm]; n = len(wins)\n",
    "    errs = recon_err[nm]; norms = latent_norm[nm]\n",
    "    labs = all_labels[offset:offset+n]\n",
    "    offset += n\n",
    "\n",
    "    F = []\n",
    "    for w,e,nmrm in zip(wins, errs, norms):\n",
    "        cen,bw    = spec_centroid_bw(w)\n",
    "        mob,comp  = hjorth(w)\n",
    "        F.append([\n",
    "            e, nmrm,\n",
    "            sta_lta(w), crest_factor(w), rms_db(w),\n",
    "            spectral_entropy(w), rolloff(w),\n",
    "            zero_crossing_rate(w), envelope_kurtosis(w),\n",
    "            kurtosis(w), skew(w),\n",
    "            cen, bw, mob, comp\n",
    "        ])\n",
    "    feats_list.append(np.vstack(F))\n",
    "    stations_arr.extend([nm]*n)\n",
    "feats_arr   = np.vstack(feats_list)    # (N_total,15)\n",
    "stations_arr= np.array(stations_arr)\n",
    "\n",
    "# robust‑scale so IF sees comparable ranges\n",
    "scaler    = RobustScaler().fit(feats_arr)\n",
    "X_scaled  = scaler.transform(feats_arr)\n",
    "\n",
    "# ─── 5. Optuna HPO for IsolationForest ───────────────────────────────────────\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators':  trial.suggest_int('n_estimators',100,500),\n",
    "        'max_samples':   trial.suggest_float('max_samples',0.3,1.0),\n",
    "        'contamination': trial.suggest_float('contamination',0.001,0.05),\n",
    "        'random_state':  SEED,\n",
    "        'n_jobs':       -1\n",
    "    }\n",
    "    clf = IsolationForest(**params)\n",
    "    idx = np.random.choice(len(X_scaled), int(TRAIN_FRAC*len(X_scaled)), replace=False)\n",
    "    clf.fit(X_scaled[idx])\n",
    "    pred_bool = clf.predict(X_scaled)==-1\n",
    "    # build hot_mask per station\n",
    "    hot = {nm: pred_bool[stations_arr==nm] for nm in STN}\n",
    "    # network F1 via your strict evaluator\n",
    "    _, net_metrics, _ = evaluate_windowed_model(\n",
    "        hot            = hot,\n",
    "        stroke_records = stroke_records,\n",
    "        quantized      = quantized,\n",
    "        station_order  = STN,\n",
    "        win            = WIN, hop=HOP,\n",
    "        burst_len      = BURST_LEN,\n",
    "        min_stn        = MIN_STN,\n",
    "        tol_win        = 1,\n",
    "        plot           = False\n",
    "    )\n",
    "    return net_metrics['F1']\n",
    "\n",
    "print(\"▶ Running Optuna HPO for IsolationForest\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=OPTUNA_TRIALS, show_progress_bar=True)\n",
    "best_params = study.best_params\n",
    "print(\"Best IF params:\", best_params)\n",
    "\n",
    "# ─── 6. Train final IF & get boolean masks ──────────────────────────────────\n",
    "clf       = IsolationForest(**best_params).fit(X_scaled)\n",
    "pred_bool = clf.predict(X_scaled)==-1\n",
    "hot_mask  = {nm: pred_bool[stations_arr==nm] for nm in STN}\n",
    "\n",
    "# ─── 7. Final strict evaluation ─────────────────────────────────────────────\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot            = hot_mask,\n",
    "    stroke_records = stroke_records,\n",
    "    quantized      = quantized,\n",
    "    station_order  = STN,\n",
    "    win            = WIN, hop=HOP,\n",
    "    burst_len      = BURST_LEN,\n",
    "    min_stn        = MIN_STN,\n",
    "    tol_win        = 1,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ─── 8. Print metrics ───────────────────────────────────────────────────────\n",
    "print(f\"\\n—— Station metrics  (n_windows={n_windows:,}) ——\")\n",
    "for nm,m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']}  FP={m['FP']}  FN={m['FN']}  \"\n",
    "          f\"P={m['P']:.3f}  R={m['R']:.3f}  F1={m['F1']:.3f}\")\n",
    "print(\"\\n—— Network metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math, random, gc\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.signal import welch, hilbert\n",
    "from scipy.stats import kurtosis, skew, genpareto\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors  import LocalOutlierFactor\n",
    "import optuna\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ──────────────────────────── 0. CONFIG & DEVICE ────────────────────────────\n",
    "WIN, HOP       = 1024, 512\n",
    "FS             = float(FS)                     # sampling rate from notebook\n",
    "BURST_LEN      = int(0.04 * FS)\n",
    "STN            = station_order                 # list of station codes\n",
    "SEED           = 42\n",
    "OPT_TRIALS_IF  = 15\n",
    "OPT_TRIALS_LOF = 15\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ──────────────────────────── 1. SLIDING WINDOWS ────────────────────────────\n",
    "def make_windows(arr):\n",
    "    n   = (len(arr) - WIN) // HOP + 1\n",
    "    idx = np.arange(0, n * HOP, HOP)[:, None] + np.arange(WIN)\n",
    "    return arr[idx]\n",
    "\n",
    "win_mats = {nm: make_windows(quantized[nm]) for nm in STN}\n",
    "\n",
    "# ─────────────────── 2. MINI TCN‑AUTOENCODER ───────────────────────────────\n",
    "class DilatedBlk(nn.Module):\n",
    "    def __init__(self, cin, cout, k, dil, drop):\n",
    "        super().__init__()\n",
    "        pad = (k-1)*dil//2\n",
    "        self.conv = nn.Conv1d(cin, cout, k, padding=pad, dilation=dil)\n",
    "        self.act  = nn.ReLU()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.res  = nn.Conv1d(cin, cout, 1) if cin != cout else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.conv(x))\n",
    "        return self.res(x) + self.drop(y)\n",
    "\n",
    "class TinyTCN_AE(nn.Module):\n",
    "    def __init__(self, channels=[16,32,64], k=7, drop=0.1, lat=64):\n",
    "        super().__init__()\n",
    "        enc, cin = [], 1\n",
    "        for i,c in enumerate(channels):\n",
    "            enc.append(DilatedBlk(cin, c, k, 2**i, drop))\n",
    "            cin = c\n",
    "        self.enc = nn.Sequential(\n",
    "            *enc,\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels[-1], lat)\n",
    "        )\n",
    "        dec, cin = [], lat\n",
    "        for i,c in enumerate(channels[::-1]):\n",
    "            out_c = c if i < len(channels)-1 else 1\n",
    "            dec.append(DilatedBlk(cin, out_c, k, 2**i, drop))\n",
    "            cin = c\n",
    "        self.dec = nn.Sequential(*dec)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)                              # (B,lat)\n",
    "        r = self.dec(z[:,:,None].expand(-1,-1,WIN))  # (B,1,WIN)\n",
    "        return z, r\n",
    "\n",
    "ae      = TinyTCN_AE().to(DEVICE)\n",
    "opt_ae  = torch.optim.AdamW(ae.parameters(), lr=3e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "recon_err, latent_norm = {}, {}\n",
    "print(\"▶ Training tiny TCN‑AE & collecting cues\")\n",
    "for nm in tqdm(STN):\n",
    "    windows = win_mats[nm].astype(np.float32) / 32768.0\n",
    "    ds = torch.tensor(windows)[:, None, :].to(DEVICE)\n",
    "    dl = DataLoader(ds, batch_size=256, shuffle=True)\n",
    "    ae.train()\n",
    "    for _ in range(2):\n",
    "        for xb in dl:\n",
    "            opt_ae.zero_grad()\n",
    "            _, rec = ae(xb)\n",
    "            l = loss_fn(rec, xb)\n",
    "            l.backward()\n",
    "            opt_ae.step()\n",
    "    ae.eval()\n",
    "    errs, norms = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb in DataLoader(ds, batch_size=256):\n",
    "            z, rec = ae(xb)\n",
    "            errs.append(((rec - xb)**2).mean((1,2)).sqrt().cpu().numpy())\n",
    "            norms.append(torch.linalg.norm(z, dim=1).cpu().numpy())\n",
    "    recon_err[nm]   = np.concatenate(errs)\n",
    "    latent_norm[nm] = np.concatenate(norms)\n",
    "    del ds; gc.collect()\n",
    "\n",
    "# ──────────────────────── 3. FEATURE EXTRACTION ────────────────────────────\n",
    "def sta_lta(w):  a,b=int(FS*5/1000),int(FS*50/1000); return w[:a].var()/(w[:b].var()+1e-9)\n",
    "def crest(w):    return np.abs(w).max()/(np.sqrt((w**2).mean())+1e-9)\n",
    "def rms_db(w):   return 20*np.log10(np.sqrt((w**2).mean())+1e-9)\n",
    "def spec_entropy(w):\n",
    "    f,P = welch(w, FS, nperseg=256); P /= P.sum()+1e-9\n",
    "    return -np.sum(P * np.log(P+1e-9))\n",
    "def rolloff(w):\n",
    "    f,P  = welch(w, FS, nperseg=256)\n",
    "    cum  = np.cumsum(P)/P.sum()\n",
    "    return f[cum>=0.95][0]\n",
    "def zcr(w):      return np.mean(np.abs(np.diff(np.sign(w))))/2\n",
    "def env_kurt(w): return kurtosis(np.abs(w))\n",
    "def spec_cent_bw(w):\n",
    "    f,P = welch(w, FS, nperseg=256); P /= P.sum()+1e-9\n",
    "    cen = (f*P).sum()\n",
    "    bw  = np.sqrt(((f-cen)**2 * P).sum())\n",
    "    return cen, bw\n",
    "def hjorth(w):\n",
    "    d1, d2 = np.diff(w), np.diff(np.diff(w))\n",
    "    m0, m1 = w.var()+1e-9, d1.var()+1e-9\n",
    "    mob     = np.sqrt(m1/m0)\n",
    "    comp    = np.sqrt(d2.var()/m1)/mob\n",
    "    return mob, comp\n",
    "\n",
    "# normalise geo\n",
    "lats = np.array([stations[n]['lat'] for n in STN], np.float32)\n",
    "lons = np.array([stations[n]['lon'] for n in STN], np.float32)\n",
    "lat0, lon0 = lats.mean(), lons.mean()\n",
    "coords = np.stack([(lats-lat0)/(np.ptp(lats)+1e-6),\n",
    "                   (lons-lon0)/(np.ptp(lons)+1e-6)], axis=1)\n",
    "\n",
    "features, station_idx = [], []\n",
    "print(\"▶ Building 17‑D feature vectors\")\n",
    "for i,nm in enumerate(tqdm(STN)):\n",
    "    wins  = win_mats[nm]\n",
    "    errs  = recon_err[nm]\n",
    "    norms = latent_norm[nm]\n",
    "    for w,e,nz in zip(wins, errs, norms):\n",
    "        cen,bw    = spec_cent_bw(w)\n",
    "        mob,comp  = hjorth(w)\n",
    "        features.append([\n",
    "            e, nz,\n",
    "            sta_lta(w), crest(w), rms_db(w),\n",
    "            spec_entropy(w), rolloff(w),\n",
    "            zcr(w), env_kurt(w),\n",
    "            kurtosis(w), skew(w),\n",
    "            cen, bw, mob, comp,\n",
    "            coords[i,0], coords[i,1]\n",
    "        ])\n",
    "        station_idx.append(nm)\n",
    "\n",
    "X            = np.array(features, np.float32)\n",
    "stations_arr = np.array(station_idx)\n",
    "scaler       = RobustScaler().fit(X)\n",
    "X_scaled     = scaler.transform(X)\n",
    "\n",
    "# ─────────────────────── 4A. TUNE IsolationForest ─────────────────────────\n",
    "def obj_if(trial):\n",
    "    params = {\n",
    "        'n_estimators':  trial.suggest_int('n_estimators',100,400),\n",
    "        'max_samples':   trial.suggest_float('max_samples',0.3,1.0),\n",
    "        'contamination': trial.suggest_float('contamination',0.005,0.04),\n",
    "        'random_state':  SEED,\n",
    "        'n_jobs':       -1\n",
    "    }\n",
    "    clf = IsolationForest(**params).fit(X_scaled)\n",
    "    pred = clf.predict(X_scaled)==-1\n",
    "    hot  = {nm: pred[stations_arr==nm] for nm in STN}\n",
    "    _, net, _ = evaluate_windowed_model(\n",
    "        hot=hot,\n",
    "        stroke_records=stroke_records,\n",
    "        quantized=quantized,\n",
    "        station_order=STN,\n",
    "        win=WIN, hop=HOP,\n",
    "        burst_len=BURST_LEN,\n",
    "        min_stn=MIN_STN,\n",
    "        tol_win=1, plot=False\n",
    "    )\n",
    "    return -net['F1']\n",
    "\n",
    "study_if = optuna.create_study(direction='minimize')\n",
    "study_if.optimize(obj_if, n_trials=OPT_TRIALS_IF, show_progress_bar=True)\n",
    "best_if = study_if.best_params\n",
    "clf_if  = IsolationForest(**best_if, random_state=SEED, n_jobs=-1).fit(X_scaled)\n",
    "score_if = -clf_if.score_samples(X_scaled)\n",
    "\n",
    "# ─────────────────────── 4B. TUNE LocalOutlierFactor ──────────────────────\n",
    "def obj_lof(trial):\n",
    "    n_nb   = trial.suggest_int('n_neighbors',20,100)\n",
    "    metric = trial.suggest_categorical('metric',['euclidean','manhattan'])\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors   = n_nb,\n",
    "        metric        = metric,\n",
    "        contamination = 0.01,\n",
    "        novelty       = True\n",
    "    ).fit(X_scaled)\n",
    "    pred = lof.predict(X_scaled)==-1\n",
    "    hot  = {nm: pred[stations_arr==nm] for nm in STN}\n",
    "    _, net, _ = evaluate_windowed_model(\n",
    "        hot=hot,\n",
    "        stroke_records=stroke_records,\n",
    "        quantized=quantized,\n",
    "        station_order=STN,\n",
    "        win=WIN, hop=HOP,\n",
    "        burst_len=BURST_LEN,\n",
    "        min_stn=MIN_STN,\n",
    "        tol_win=1, plot=False\n",
    "    )\n",
    "    return -net['F1']\n",
    "\n",
    "study_lof = optuna.create_study(direction='minimize')\n",
    "study_lof.optimize(obj_lof, n_trials=OPT_TRIALS_LOF, show_progress_bar=True)\n",
    "best_lof = study_lof.best_params\n",
    "lof = LocalOutlierFactor(**best_lof, contamination=0.01, novelty=True).fit(X_scaled)\n",
    "score_lof = -lof.negative_outlier_factor_\n",
    "\n",
    "# ─────────────────────────── 5. ENSEMBLE RANKING ────────────────────────────\n",
    "r_if  = score_if.argsort().argsort() / len(score_if)\n",
    "r_lof = score_lof.argsort().argsort() / len(score_lof)\n",
    "score_ens = 0.5*r_if + 0.5*r_lof\n",
    "\n",
    "# ──────────────────── 6. EVT THRESHOLDING (GPD) ────────────────────────────\n",
    "tail_frac = 0.05\n",
    "thr_min   = np.quantile(score_ens, 1-tail_frac)\n",
    "tail      = score_ens[score_ens >= thr_min] - thr_min\n",
    "c, loc, sc = genpareto.fit(tail)\n",
    "evt_thr   = thr_min + genpareto.ppf(0.999, c, loc=loc, scale=sc)\n",
    "print(f\"EVT threshold @99.9% ≈ {evt_thr:.4f}\")\n",
    "\n",
    "hot_mask = {nm: score_ens[stations_arr==nm] > evt_thr for nm in STN}\n",
    "\n",
    "# ─────────────────────── 7. FINAL EVALUATION & REPORT ─────────────────────\n",
    "station_metrics, network_metrics, n_w = evaluate_windowed_model(\n",
    "    hot            = hot_mask,\n",
    "    stroke_records = stroke_records,\n",
    "    quantized      = quantized,\n",
    "    station_order  = STN,\n",
    "    win            = WIN, hop=HOP,\n",
    "    burst_len      = BURST_LEN,\n",
    "    min_stn        = MIN_STN,\n",
    "    tol_win        = 1,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {n_w:,} windows\\n\")\n",
    "for nm,m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']}  FP={m['FP']}  FN={m['FN']}  \"\n",
    "          f\"P={m['P']:.3f}  R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "print(\"\\nNetwork metrics:\", network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, gc\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.signal import welch, hilbert\n",
    "from scipy.stats import kurtosis, skew, genpareto\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import optuna\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ──────────────────────────── 0. CONFIG & DEVICE ────────────────────────────\n",
    "WIN, HOP       = 1024, 512\n",
    "FS             = float(FS)                     # sampling rate provided by notebook\n",
    "BURST_LEN      = int(0.04 * FS)\n",
    "STN            = station_order                 # list of station codes\n",
    "SEED           = 42\n",
    "OPT_TRIALS_IF  = 15\n",
    "OPT_TRIALS_LOF = 15\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ──────────────────────────── 1. SLIDING WINDOWS ────────────────────────────\n",
    "def make_windows(arr):\n",
    "    n   = (len(arr) - WIN) // HOP + 1\n",
    "    idx = np.arange(0, n * HOP, HOP)[:, None] + np.arange(WIN)\n",
    "    return arr[idx]\n",
    "\n",
    "win_mats = {nm: make_windows(quantized[nm]) for nm in STN}\n",
    "\n",
    "# ─────────────────── 2. MINI TCN‑AUTOENCODER ───────────────────────────────\n",
    "class DilatedBlk(nn.Module):\n",
    "    def __init__(self, cin, cout, k, dil, drop):\n",
    "        super().__init__()\n",
    "        pad = (k-1)*dil//2\n",
    "        self.conv = nn.Conv1d(cin, cout, k, padding=pad, dilation=dil)\n",
    "        self.act  = nn.ReLU()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.res  = nn.Conv1d(cin, cout, 1) if cin != cout else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        y = self.act(self.conv(x))\n",
    "        return self.res(x) + self.drop(y)\n",
    "\n",
    "class TinyTCN_AE(nn.Module):\n",
    "    def __init__(self, channels=[16,32,64], k=7, drop=0.1, lat=64):\n",
    "        super().__init__()\n",
    "        enc, cin = [], 1\n",
    "        for i,c in enumerate(channels):\n",
    "            enc.append(DilatedBlk(cin, c, k, 2**i, drop))\n",
    "            cin = c\n",
    "        self.enc = nn.Sequential(\n",
    "            *enc, nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(), nn.Linear(channels[-1], lat)\n",
    "        )\n",
    "        dec, cin = [], lat\n",
    "        for i,c in enumerate(channels[::-1]):\n",
    "            out_c = c if i < len(channels)-1 else 1\n",
    "            dec.append(DilatedBlk(cin, out_c, k, 2**i, drop))\n",
    "            cin = c\n",
    "        self.dec = nn.Sequential(*dec)\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)                           # (B, lat)\n",
    "        r = self.dec(z[:,:,None].expand(-1,-1,WIN))  # (B,1,WIN)\n",
    "        return z, r\n",
    "\n",
    "ae      = TinyTCN_AE().to(DEVICE)\n",
    "opt_ae  = torch.optim.AdamW(ae.parameters(), lr=3e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "recon_err, latent_norm = {}, {}\n",
    "print(\"▶ Training TCN‑AE & collecting cues\")\n",
    "for nm in tqdm(STN):\n",
    "    windows = win_mats[nm].astype(np.float32) / 32768.0\n",
    "    ds = torch.tensor(windows)[:, None, :].to(DEVICE)\n",
    "    dl = DataLoader(ds, batch_size=256, shuffle=True)\n",
    "    ae.train()\n",
    "    for _ in range(2):\n",
    "        for xb in dl:\n",
    "            opt_ae.zero_grad()\n",
    "            _, rec = ae(xb)\n",
    "            l = loss_fn(rec, xb)\n",
    "            l.backward()\n",
    "            opt_ae.step()\n",
    "    ae.eval()\n",
    "    errs, norms = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb in DataLoader(ds, batch_size=256):\n",
    "            z, rec = ae(xb)\n",
    "            errs.append(((rec - xb)**2).mean((1,2)).sqrt().cpu().numpy())\n",
    "            norms.append(torch.linalg.norm(z, dim=1).cpu().numpy())\n",
    "    recon_err[nm]   = np.concatenate(errs)\n",
    "    latent_norm[nm] = np.concatenate(norms)\n",
    "    del ds; gc.collect()\n",
    "\n",
    "# ──────────────────────── 3. FEATURE EXTRACTION ────────────────────────────\n",
    "def sta_lta(w):\n",
    "    a = int(FS*5/1000); b = int(FS*50/1000)\n",
    "    return w[:a].var()/(w[:b].var()+1e-9)\n",
    "def crest(w):\n",
    "    return np.abs(w).max()/(np.sqrt((w**2).mean())+1e-9)\n",
    "def rms_db(w):\n",
    "    return 20*np.log10(np.sqrt((w**2).mean())+1e-9)\n",
    "def spec_entropy(w):\n",
    "    f,P = welch(w, FS, nperseg=256); P /= P.sum()+1e-9\n",
    "    return -np.sum(P * np.log(P+1e-9))\n",
    "def rolloff(w):\n",
    "    f,P  = welch(w, FS, nperseg=256)\n",
    "    cum  = np.cumsum(P)/P.sum()\n",
    "    return f[cum>=0.95][0]\n",
    "def zcr(w):\n",
    "    return np.mean(np.abs(np.diff(np.sign(w))))/2\n",
    "def env_kurt(w):\n",
    "    return kurtosis(np.abs(w))\n",
    "def spec_cent_bw(w):\n",
    "    f,P = welch(w, FS, nperseg=256); P /= P.sum()+1e-9\n",
    "    c   = (f*P).sum()\n",
    "    bw  = np.sqrt(((f-c)**2 * P).sum())\n",
    "    return c, bw\n",
    "def hjorth(w):\n",
    "    d1 = np.diff(w); d2 = np.diff(d1)\n",
    "    m0, m1 = w.var()+1e-9, d1.var()+1e-9\n",
    "    mob = np.sqrt(m1/m0)\n",
    "    comp = np.sqrt(d2.var()/m1)/mob\n",
    "    return mob, comp\n",
    "\n",
    "# normalise geo\n",
    "lats = np.array([stations[n]['lat'] for n in STN], np.float32)\n",
    "lons = np.array([stations[n]['lon'] for n in STN], np.float32)\n",
    "lat0, lon0 = lats.mean(), lons.mean()\n",
    "coords = np.stack([(lats-lat0)/(np.ptp(lats)+1e-6),\n",
    "                   (lons-lon0)/(np.ptp(lons)+1e-6)], axis=1)\n",
    "\n",
    "features, station_idx = [], []\n",
    "print(\"▶ Building 17‑D feature vectors\")\n",
    "for i,nm in enumerate(tqdm(STN)):\n",
    "    wins  = win_mats[nm]\n",
    "    errs  = recon_err[nm]\n",
    "    norms = latent_norm[nm]\n",
    "    for w,e,nz in zip(wins, errs, norms):\n",
    "        cen,bw    = spec_cent_bw(w)\n",
    "        mob,comp  = hjorth(w)\n",
    "        features.append([\n",
    "            e, nz,\n",
    "            sta_lta(w), crest(w), rms_db(w),\n",
    "            spec_entropy(w), rolloff(w),\n",
    "            zcr(w), env_kurt(w),\n",
    "            kurtosis(w), skew(w),\n",
    "            cen, bw, mob, comp,\n",
    "            coords[i,0], coords[i,1]\n",
    "        ])\n",
    "        station_idx.append(nm)\n",
    "\n",
    "X            = np.array(features, np.float32)\n",
    "stations_arr = np.array(station_idx)\n",
    "scaler       = RobustScaler().fit(X)\n",
    "X_scaled     = scaler.transform(X)\n",
    "\n",
    "# ─────────────────────── 4A. TUNE IsolationForest ─────────────────────────\n",
    "def obj_if(trial):\n",
    "    params = {\n",
    "        'n_estimators':  trial.suggest_int('n_est',100,400),\n",
    "        'max_samples':   trial.suggest_float('samp',0.3,1.0),\n",
    "        'contamination': trial.suggest_float('cont',0.005,0.04),\n",
    "        'random_state':  SEED,\n",
    "        'n_jobs':       -1\n",
    "    }\n",
    "    clf = IsolationForest(**params).fit(X_scaled)\n",
    "    pred = (clf.predict(X_scaled)==-1)\n",
    "    hot  = {nm: pred[stations_arr==nm] for nm in STN}\n",
    "    _, net, _ = evaluate_windowed_model(\n",
    "        hot=hot,\n",
    "        stroke_records=stroke_records,\n",
    "        quantized=quantized,\n",
    "        station_order=STN,\n",
    "        win=WIN, hop=HOP,\n",
    "        burst_len=BURST_LEN,\n",
    "        min_stn=MIN_STN,\n",
    "        tol_win=1, plot=False\n",
    "    )\n",
    "    return -net['F1']\n",
    "\n",
    "study_if = optuna.create_study(direction='minimize')\n",
    "study_if.optimize(obj_if, n_trials=OPT_TRIALS_IF, show_progress_bar=True)\n",
    "best_if = study_if.best_params\n",
    "clf_if  = IsolationForest(**best_if, random_state=SEED, n_jobs=-1).fit(X_scaled)\n",
    "score_if = -clf_if.score_samples(X_scaled)\n",
    "\n",
    "# ─────────────────────── 4B. TUNE LocalOutlierFactor ──────────────────────\n",
    "def obj_lof(trial):\n",
    "    n_nbr  = trial.suggest_int('n_nbr',20,100)\n",
    "    metric = trial.suggest_categorical('metric',['euclidean','manhattan'])\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors   = n_nbr,\n",
    "        metric        = metric,\n",
    "        contamination = 0.01,\n",
    "        novelty       = True\n",
    "    ).fit(X_scaled)\n",
    "    pred = (lof.predict(X_scaled)==-1)\n",
    "    hot  = {nm: pred[stations_arr==nm] for nm in STN}\n",
    "    _, net, _ = evaluate_windowed_model(\n",
    "        hot=hot,\n",
    "        stroke_records=stroke_records,\n",
    "        quantized=quantized,\n",
    "        station_order=STN,\n",
    "        win=WIN, hop=HOP,\n",
    "        burst_len=BURST_LEN,\n",
    "        min_stn=MIN_STN,\n",
    "        tol_win=1, plot=False\n",
    "    )\n",
    "    return -net['F1']\n",
    "\n",
    "study_lof = optuna.create_study(direction='minimize')\n",
    "study_lof.optimize(obj_lof, n_trials=OPT_TRIALS_LOF, show_progress_bar=True)\n",
    "best_lof = study_lof.best_params\n",
    "lof      = LocalOutlierFactor(**best_lof, contamination=0.01, novelty=True).fit(X_scaled)\n",
    "score_lof= -lof.negative_outlier_factor_\n",
    "\n",
    "# ─────────────────────────── 5. ENSEMBLE RANKING ────────────────────────────\n",
    "r_if  = score_if.argsort().argsort() / len(score_if)\n",
    "r_lof = score_lof.argsort().argsort() / len(score_lof)\n",
    "score_ens = 0.5*r_if + 0.5*r_lof\n",
    "\n",
    "# ──────────────────── 6. EVT THRESHOLDING (GPD) ────────────────────────────\n",
    "tail_frac = 0.05\n",
    "thr_min   = np.quantile(score_ens, 1-tail_frac)\n",
    "tail      = score_ens[score_ens >= thr_min] - thr_min\n",
    "c,loc,sca = genpareto.fit(tail)\n",
    "evt_thr   = thr_min + genpareto.ppf(0.999, c, loc=loc, scale=sca)\n",
    "print(f\"EVT threshold (~99.9%): {evt_thr:.4f}\")\n",
    "\n",
    "hot_mask = {nm: score_ens[stations_arr==nm] > evt_thr for nm in STN}\n",
    "\n",
    "# ─────────────────────── 7. FINAL EVALUATION & REPORT ─────────────────────\n",
    "station_metrics, network_metrics, n_win = evaluate_windowed_model(\n",
    "    hot=hot_mask,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN, hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=1,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {n_win:,} windows\\n\")\n",
    "for nm,m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']}  FP={m['FP']}  FN={m['FN']}  \"\n",
    "          f\"P={m['P']:.3f}  R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "print(\"\\nNetwork metrics:\", network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
