{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1 – Leela‑scope Lightning Simulator v3  (tiers 1 … 9)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Exactly six stations, harder ‘near/medium/far’, and progressive realism.\n",
    "\n",
    "Variables & structures are IDENTICAL to the legacy generator:\n",
    "    stations, station_order, STN           – geography dict / list / alias\n",
    "    STATIONS                               – extra alias for downstream code\n",
    "    quantized, station_truth               – raw ADC & truth windows\n",
    "    events, stroke_records, stroke_samples – ground‑truth meta\n",
    "    df_wave, df_labels                     – convenience DataFrames\n",
    "    df_to_quantized(),  df_labels_to_events()\n",
    "Nothing else downstream needs to change.\n",
    "\"\"\"\n",
    "# ------------------------------------------------------------\n",
    "import math, random, numpy as np, pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "# 0)  USER KNOBS ------------------------------------------------------------\n",
    "SEED          = 424242\n",
    "duration_min  = 5                 # storm length (min)\n",
    "scenario      = 'medium'          # 'near' | 'medium' | 'far'\n",
    "DIFFICULTY    = 5                 # 1 (easy) … 9 (very hard)\n",
    "FS            = 109_375           # Hz  – keep for pipeline\n",
    "BITS, VREF    = 14, 1.0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# 1)  STATION GEOGRAPHY (always six) ---------------------------------------\n",
    "stations = {\n",
    "    'LER': dict(lat=60.15, lon=-1.13),   # Lerwick\n",
    "    'INV': dict(lat=57.48, lon=-4.23),   # Inverness\n",
    "    'DUB': dict(lat=53.35, lon=-6.26),   # Dublin\n",
    "    'LON': dict(lat=51.50, lon=-0.12),   # London\n",
    "    'AMS': dict(lat=52.37, lon= 4.90),   # Amsterdam\n",
    "    'BER': dict(lat=52.52, lon=13.40),   # Berlin\n",
    "}\n",
    "\n",
    "stations = {\n",
    "    'KEF': dict(lat=64.020, lon=-22.567),  # Keflavík\n",
    "    'VAL': dict(lat=51.930, lon=-10.250),  # Valentia Observatory\n",
    "    'LER': dict(lat=60.150, lon= -1.130),  # Lerwick\n",
    "    'HER': dict(lat=50.867, lon=  0.336),  # Herstmonceux\n",
    "    'GIB': dict(lat=36.150, lon= -5.350),  # Gibraltar\n",
    "    'AKR': dict(lat=34.588, lon= 32.986),  # Akrotiri\n",
    "    'CAM': dict(lat=50.217, lon= -5.317),  # Camborne\n",
    "    'WAT': dict(lat=52.127, lon=  0.956),  # Wattisham\n",
    "    'CAB': dict(lat=51.970, lon=  4.930),  # Cabauw\n",
    "    'PAY': dict(lat=46.820, lon=  6.950),  # Payerne\n",
    "    'TAR': dict(lat=58.263, lon= 26.464),  # Tõravere\n",
    "}\n",
    "\n",
    "station_order = list(stations.keys())    # fixed order\n",
    "STN      = station_order\n",
    "STATIONS = station_order                 # alias used in other notebooks\n",
    "N_STN    = len(STN)\n",
    "\n",
    "# 2)  Helpers ---------------------------------------------------------------\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R=6371.0\n",
    "    φ1,φ2 = map(math.radians, (lat1,lat2))\n",
    "    dφ    = math.radians(lat2-lat1)\n",
    "    dλ    = math.radians(lon2-lon1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "# 3)  Timeline --------------------------------------------------------------\n",
    "pre_sec   = rng.uniform(5,30)\n",
    "storm_sec = duration_min*60\n",
    "total_sec = pre_sec + storm_sec\n",
    "N         = int(total_sec*FS)\n",
    "\n",
    "quantized      = {nm: np.zeros(N, np.int16)   for nm in STN}\n",
    "station_truth  = {nm: np.zeros(N//1024, bool) for nm in STN}\n",
    "events, stroke_records, burst_book = [], [], []\n",
    "\n",
    "# 4)  Tier flags ------------------------------------------------------------\n",
    "flags = dict(\n",
    "    ic_mix          = DIFFICULTY>=2,\n",
    "    multipath       = DIFFICULTY>=3,\n",
    "    coloured_noise  = DIFFICULTY>=4,\n",
    "    rfi             = DIFFICULTY>=5,\n",
    "    sprite_ring     = DIFFICULTY>=5,\n",
    "    false_transient = DIFFICULTY>=5,\n",
    "    clipping        = DIFFICULTY>=5,\n",
    "    multi_cell      = DIFFICULTY>=6,\n",
    "    skywave         = DIFFICULTY>=7,\n",
    "    bg_sferics      = DIFFICULTY>=7,\n",
    "    clock_skew      = DIFFICULTY>=8,\n",
    "    gain_drift      = DIFFICULTY>=8,\n",
    "    dropouts        = DIFFICULTY>=8,\n",
    "    low_snr         = DIFFICULTY>=9,\n",
    "    burst_div       = DIFFICULTY>=9,\n",
    ")\n",
    "\n",
    "# 5)  Storm‑cell geometry ----------------------------------------------------\n",
    "lat0 = np.mean([s['lat'] for s in stations.values()])\n",
    "lon0 = np.mean([s['lon'] for s in stations.values()])\n",
    "R0   = dict(near=100, medium=400, far=900)[scenario]   # tougher radii\n",
    "\n",
    "n_cells = 1 if not flags['multi_cell'] else rng.integers(2,5)\n",
    "cells=[]\n",
    "for _ in range(n_cells):\n",
    "    θ   = rng.uniform(0,2*math.pi)\n",
    "    rad = R0*rng.uniform(0.3,1.0)\n",
    "    cells.append(dict(\n",
    "        lat   = lat0 + (rad/111)*math.cos(θ),\n",
    "        lon   = lon0 + (rad/111)*math.sin(θ)/math.cos(math.radians(lat0)),\n",
    "        drift = rng.uniform(-0.20,0.20,2)          # deg·h‑1\n",
    "    ))\n",
    "\n",
    "# 6)  Flash & stroke generation ---------------------------------------------\n",
    "wave_len = int(0.04*FS)\n",
    "tv       = np.arange(wave_len)/FS\n",
    "rfi_freqs= [14400,20100,30300]\n",
    "\n",
    "eid, t = 0, pre_sec\n",
    "while True:\n",
    "    cell = cells[rng.integers(len(cells))]\n",
    "    c_age= t-pre_sec\n",
    "    c_lat= cell['lat'] + cell['drift'][0]*c_age/3600\n",
    "    c_lon= cell['lon'] + cell['drift'][1]*c_age/3600\n",
    "    t   += rng.lognormal(3,1)* (0.4 if flags['multi_cell'] else 1)\n",
    "    if t>=total_sec: break\n",
    "    eid += 1\n",
    "    # flash location\n",
    "    d = rng.uniform(0,R0)\n",
    "    φ = rng.uniform(0,2*math.pi)\n",
    "    f_lat = c_lat + (d/111)*math.cos(φ)\n",
    "    f_lon = c_lon + (d/111)*math.sin(φ)/math.cos(math.radians(lat0))\n",
    "    f_type= 'IC' if (flags['ic_mix'] and rng.random()<0.3) else 'CG'\n",
    "    n_str = rng.integers(1, 4 if f_type=='IC' else 7)\n",
    "    amp0  = 0.35 if f_type=='IC' else 1.0\n",
    "    s_times = sorted(t + rng.uniform(0,0.06,size=n_str))\n",
    "    events.append(dict(id=eid,flash_type=f_type,lat=f_lat,lon=f_lon,\n",
    "                       stroke_times=s_times))\n",
    "    # build bursts\n",
    "    for si, t0 in enumerate(s_times):\n",
    "        for nm in STN:\n",
    "            dist = hav(f_lat,f_lon, stations[nm]['lat'],stations[nm]['lon'])\n",
    "            idx  = int((t0 + dist/300_000 + rng.uniform(-50,50)/1e6)*FS)\n",
    "            if idx>=N: continue\n",
    "            # waveform\n",
    "            if flags['burst_div'] and rng.random()<0.15:\n",
    "                tau=.0008; burst = (tv/tau)*np.exp(1-tv/tau)\n",
    "            else:\n",
    "                f0=rng.uniform(2500,9500); tau=rng.uniform(0.0003,0.0025)\n",
    "                burst = np.sin(2*math.pi*f0*tv)*np.exp(-tv/tau)\n",
    "            amp = amp0*rng.uniform(2,5)/(1+dist/40)\n",
    "            if flags['low_snr']: amp*=0.4\n",
    "            burst *= amp\n",
    "            # multipath\n",
    "            if flags['multipath'] and dist>60:\n",
    "                dly=int(rng.uniform(0.001,0.0035)*FS)\n",
    "                if dly<wave_len: burst[dly:]+=0.35*burst[:-dly]\n",
    "            # sprite ringers\n",
    "            if flags['sprite_ring'] and rng.random()<0.04:\n",
    "                dly=int(rng.uniform(0.008,0.018)*FS)\n",
    "                if dly<wave_len: burst[dly:]+=0.25*burst[:-dly]\n",
    "            # sky‑wave attenuation\n",
    "            if flags['skywave'] and dist>600:\n",
    "                f=np.fft.rfftfreq(wave_len,1/FS)\n",
    "                H=np.exp(-0.00025*dist*((f/6e3)**2))\n",
    "                burst=np.fft.irfft(np.fft.rfft(burst)*H,n=wave_len)\n",
    "            burst_book.append((nm,idx,burst.astype(np.float32)))\n",
    "            station_truth[nm][idx//1024]=True\n",
    "            stroke_records.append(dict(event_id=eid,stroke_i=si,station=nm,\n",
    "                                       flash_type=f_type,lat=f_lat,lon=f_lon,\n",
    "                                       true_time_s=t0,sample_idx=idx,\n",
    "                                       window_idx=idx//1024))\n",
    "            #if nm==STN[0]: stroke_samples.append(idx)\n",
    "\n",
    "# 7)  Noise profiles ---------------------------------------------------------\n",
    "noise_cfg={}\n",
    "for nm in STN:\n",
    "    base_white=rng.uniform(0.010,0.017)\n",
    "    tones=[]\n",
    "    if flags['rfi'] and rng.random()<0.6:\n",
    "        tones=[(rng.choice(rfi_freqs), rng.uniform(0.001,0.004))]\n",
    "    noise_cfg[nm]=dict(\n",
    "        w = base_white if not flags['coloured_noise'] else base_white*rng.uniform(1,1.8),\n",
    "        h = 0.01 if not flags['coloured_noise'] else rng.uniform(0.006,0.020),\n",
    "        tones = tones,\n",
    "        gain_drift = rng.uniform(-0.05,0.05) if flags['gain_drift'] else 0.0,\n",
    "        skew = rng.uniform(-20e-6,20e-6) if flags['clock_skew'] else 0.0\n",
    "    )\n",
    "\n",
    "# 8)  ADC synthesis loop -----------------------------------------------------\n",
    "b,a  = butter(4, 45000/(FS/2),'low')\n",
    "chunk= int(20*FS)\n",
    "tv40 = np.arange(wave_len)/FS   # for false‑transient reuse\n",
    "\n",
    "for nm in STN:\n",
    "    bur = [b for b in burst_book if b[0]==nm]\n",
    "    cfg = noise_cfg[nm]\n",
    "    drop = np.ones(N,bool)\n",
    "    if flags['dropouts'] and rng.random()<0.1:\n",
    "        for _ in range(rng.integers(1,3)):\n",
    "            s=rng.integers(int(pre_sec*FS),N-int(0.4*FS))\n",
    "            drop[s:s+int(0.4*FS)]=False\n",
    "    for s0 in range(0,N,chunk):\n",
    "        e0=min(N,s0+chunk); L=e0-s0\n",
    "        t=np.arange(s0,e0)/FS\n",
    "        seg = cfg['w']*rng.standard_normal(L) + cfg['h']*np.sin(2*math.pi*50*t)\n",
    "        for f,amp in cfg['tones']:\n",
    "            seg += amp*np.sin(2*math.pi*f*t + rng.uniform(0,2*math.pi))\n",
    "        # gain drift\n",
    "        seg *= 1 + cfg['gain_drift']*(t-pre_sec)/(storm_sec+1e-9)\n",
    "        # add bursts\n",
    "        for (_,i0,br) in bur:\n",
    "            if s0<=i0<e0:\n",
    "                off=i0-s0; l=min(len(br),L-off)\n",
    "                seg[off:off+l]+=br[:l]\n",
    "        # false transient\n",
    "        if flags['false_transient'] and rng.random()<0.002:\n",
    "            idx=rng.integers(0,L-wave_len)\n",
    "            seg[idx:idx+wave_len]+=0.7*np.sin(2*math.pi*5800*tv40)*np.exp(-tv40/0.0009)\n",
    "        # filtering & clipping\n",
    "        seg = filtfilt(b,a,seg)\n",
    "        if flags['clipping']: seg=np.clip(seg,-0.9*VREF,0.9*VREF)\n",
    "        full=2**(BITS-1)-1\n",
    "        adc = np.clip(np.round(seg/VREF*full), -full, full).astype(np.int16)\n",
    "        quantized[nm][s0:e0][drop[s0:e0]] = adc[drop[s0:e0]]\n",
    "\n",
    "# 9)  DataFrames -------------------------------------------------------------\n",
    "df_wave = pd.DataFrame({'time_s':np.arange(N)/FS})\n",
    "for nm in STN: df_wave[nm]=quantized[nm]\n",
    "df_labels = pd.DataFrame(stroke_records)\n",
    "\n",
    "def df_to_quantized(df):      return {nm:df[nm].values.astype(np.int16) for nm in STN}\n",
    "def df_labels_to_events(df):\n",
    "    out=[];      grp=df.groupby('event_id')\n",
    "    for eid,g in grp:\n",
    "        out.append(dict(id=eid,flash_type=g.flash_type.iloc[0],\n",
    "                        lat=g.lat.iloc[0],lon=g.lon.iloc[0],\n",
    "                        stroke_times=sorted(g.true_time_s.unique())))\n",
    "    return out\n",
    "\n",
    "# 10)  Summary --------------------------------------------------------------\n",
    "print(f\"Tier‑{DIFFICULTY} | scenario={scenario} | cells={n_cells}\")\n",
    "print(f\"Flashes {len(events):3d} | strokes {len(df_labels)//N_STN:3d} \"\n",
    "      f\"| samples {N:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1‑QA‑PLUS  –  Rich “eyes‑on” dashboard for the simulator\n",
    "# ============================================================\n",
    "#  • Requires matplotlib, pandas, scipy                                                │\n",
    "#  • Optional: cartopy  (for the Europe station map)                                   │\n",
    "#  • ZERO side‑effects – everything is read‑only, later cells still work unmodified.   │\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "plt.rcParams.update({'axes.grid': True, 'figure.dpi': 110})\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 0)  Helper: concise console table printer\n",
    "# -----------------------------------------------------------------\n",
    "def _tbl(rows, hdr=None, col_sep=\"  \"):\n",
    "    if hdr: print(col_sep.join(hdr))\n",
    "    for r in rows:\n",
    "        print(col_sep.join(str(c) for c in r))\n",
    "    print()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1)  Global metadata\n",
    "# -----------------------------------------------------------------\n",
    "N   = quantized[station_order[0]].size\n",
    "dur = N / FS\n",
    "print(f\"► Simulator difficulty tier : {DIFFICULTY}\")\n",
    "print(f\"► Sampling rate             : {FS:,.0f} Hz \"\n",
    "      f\"(Δt={1e6/FS:.2f} µs)\")\n",
    "print(f\"► Duration                  : {dur:.2f} s  ({dur/60:.2f} min)\")\n",
    "print(f\"► Total ADC samples         : {N:,}\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2)  Per‑station ADC statistics\n",
    "# -----------------------------------------------------------------\n",
    "rows=[]\n",
    "for nm in station_order:\n",
    "    q = quantized[nm].astype(float)\n",
    "    rows.append([nm, q.min().astype(int), q.max().astype(int),\n",
    "                 f\"{q.mean():.1f}\", f\"{q.std():.1f}\",\n",
    "                 f\"{100*np.count_nonzero(q)/len(q):.2f}%\"])\n",
    "_tbl(rows, hdr=[\"STN\",\"min\",\"max\",\"μ\",\"σ\",\"non‑zero %\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3)  Flash / stroke timing & location dataframe\n",
    "# -----------------------------------------------------------------\n",
    "stroke_times = np.hstack([ev[\"stroke_times\"] for ev in events])\n",
    "lat_rep      = np.hstack([[ev['lat']]*len(ev['stroke_times']) for ev in events])\n",
    "lon_rep      = np.hstack([[ev['lon']]*len(ev['stroke_times']) for ev in events])\n",
    "df_strokes   = pd.DataFrame(dict(time_s=stroke_times,\n",
    "                                 lat=lat_rep, lon=lon_rep))\n",
    "df_strokes.sort_values(\"time_s\", inplace=True, ignore_index=True)\n",
    "print(f\"Flashes  : {len(events)}\")\n",
    "print(f\"Strokes  : {len(df_strokes)}\\n\")\n",
    "display(df_strokes.head())\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4)  Simple “did station see the stroke?” heuristic\n",
    "# -----------------------------------------------------------------\n",
    "pre_samp = int(pre_sec*FS)                       # noise estimate window\n",
    "det_tbl  = []\n",
    "for nm in station_order:\n",
    "    noiseσ = quantized[nm][:pre_samp].astype(float).std()\n",
    "    thr    = 3*noiseσ\n",
    "    hits   = []\n",
    "    for t in df_strokes.time_s:\n",
    "        idx = int(t*FS)\n",
    "        hits.append(abs(quantized[nm][idx])>=thr if idx<N else False)\n",
    "    df_strokes[f\"det_{nm}\"] = hits\n",
    "    det_tbl.append([nm, f\"{100*np.mean(hits):.1f}%\"])\n",
    "\n",
    "_tbl(det_tbl, hdr=[\"STN\",\"simple detection rate\"])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5)  Window‑level labels (quiet = 0, lightning = 1)\n",
    "# -----------------------------------------------------------------\n",
    "W      = 1024\n",
    "n_win  = N // W\n",
    "starts = (np.arange(n_win)*W)/FS\n",
    "stroke_idx = (df_strokes.time_s.values*FS).astype(int)\n",
    "labels_win = np.zeros(n_win, bool)\n",
    "labels_win[(stroke_idx//W)] = True           # even if >1 stroke per win → still 1\n",
    "\n",
    "df_win = pd.DataFrame(dict(win_idx=np.arange(n_win, dtype=int),\n",
    "                           start_s=starts,\n",
    "                           label=labels_win.astype(int)))\n",
    "quiet_cnt, light_cnt = np.bincount(df_win.label, minlength=2)\n",
    "print(f\"\\nWindows: quiet={quiet_cnt}, lightning={light_cnt}\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6)  Visual #1 – histograms (log y)\n",
    "# -----------------------------------------------------------------\n",
    "for nm in station_order:\n",
    "    plt.figure(figsize=(4,2.5))\n",
    "    plt.hist(quantized[nm][::max(1,N//200_000)],\n",
    "             bins=120, log=True, color='#4682B4')\n",
    "    plt.title(f\"{nm} – ADC histogram\")\n",
    "    plt.xlabel(\"ADC counts\"); plt.ylabel(\"occurrences (log)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 7)  Visual #2 – first lightning vs first quiet window\n",
    "# -----------------------------------------------------------------\n",
    "def _plot_window(win_row, title, colour):\n",
    "    idx = int(win_row.win_idx)\n",
    "    tms = (np.arange(W)/FS)*1e3\n",
    "    plt.figure(figsize=(6,2.4))\n",
    "    for nm in station_order:\n",
    "        seg = quantized[nm][idx*W:(idx+1)*W]\n",
    "        plt.plot(tms, seg, label=nm, alpha=.8)\n",
    "    plt.title(title); plt.xlabel(\"time (ms)\"); plt.ylabel(\"ADC\")\n",
    "    plt.legend(ncol=len(station_order)//2)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "_plot_window(df_win[df_win.label==1].iloc[0],\n",
    "             \"First lightning window\", 'orange')\n",
    "_plot_window(df_win[df_win.label==0].iloc[0],\n",
    "             \"First quiet window\", 'gray')\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 8)  Visual #3 – analytic‑signal envelope of lightning window\n",
    "# -----------------------------------------------------------------\n",
    "idx_lit = int(df_win[df_win.label==1].iloc[0].win_idx)\n",
    "tms     = (np.arange(W)/FS)*1e3\n",
    "plt.figure(figsize=(6,2.6))\n",
    "for nm in station_order:\n",
    "    seg = quantized[nm][idx_lit*W:(idx_lit+1)*W].astype(float)\n",
    "    env = np.abs(hilbert(seg))\n",
    "    plt.plot(tms, env, label=f\"{nm} env\")\n",
    "plt.title(f\"Envelope – window {idx_lit} (lightning)\")\n",
    "plt.xlabel(\"time (ms)\"); plt.ylabel(\"abs(hilbert)\")\n",
    "plt.legend(ncol=len(station_order)//2); plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 9)  Visual #4 – stroke‑window heat‑map (sparse but visible)\n",
    "# -----------------------------------------------------------------\n",
    "truth_mat = np.vstack([station_truth[nm][:n_win] for nm in station_order])\n",
    "plt.figure(figsize=(10,1.2+0.25*len(station_order)))\n",
    "plt.imshow(truth_mat, aspect='auto',\n",
    "           cmap=plt.get_cmap(\"Reds\", 2), interpolation='nearest')\n",
    "plt.yticks(range(len(station_order)), station_order)\n",
    "plt.xlabel(f\"window index ({W} samples)\"); plt.title(\"Ground‑truth stroke windows\")\n",
    "# over‑plot thicker bars for visibility\n",
    "for w in np.where(truth_mat.any(0))[0]:\n",
    "    plt.axvline(w, color='red', lw=.5, alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 10) Optional geographic snapshot (requires Cartopy)\n",
    "# -----------------------------------------------------------------\n",
    "try:\n",
    "    import cartopy.crs as ccrs, cartopy.feature as cfeature\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax  = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE, lw=.6); ax.add_feature(cfeature.BORDERS,lw=.4)\n",
    "    ax.set_extent([-35, 30, 30, 65])\n",
    "    ax.scatter(df_strokes.lon, df_strokes.lat, c='orange', s=10, lw=0, zorder=2)\n",
    "    for nm in station_order:\n",
    "        ax.plot(stations[nm]['lon'], stations[nm]['lat'], '^', ms=7, mfc='#1f77b4',\n",
    "                mec='k', zorder=3)\n",
    "        ax.text(stations[nm]['lon']+0.4, stations[nm]['lat']+0.4, nm, fontsize=8)\n",
    "    ax.set_title(\"Station & flash geography\"); plt.tight_layout(); plt.show()\n",
    "except ImportError:\n",
    "    print(\"(Cartopy not installed – map skipped)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2 – Robust stroke‐by‐stroke Hilbert‐envelope detector\n",
    "#            + 50%‐overlap windows + percentile thresholds\n",
    "#            (earliest‐arrival indexing)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Cell 2 – Robust Stroke‐by‐Stroke Hilbert‐Envelope Detector\n",
    "-----------------------------------------------------------\n",
    "\n",
    "This cell implements a fully‐unsupervised lightning‐stroke detector based on\n",
    "the analytic‐signal envelope (Hilbert transform) of each station’s ADC trace,\n",
    "using overlapping windows and percentile thresholding to flag “hot” windows\n",
    "that likely contain sferic bursts.  It then fuses detections across stations\n",
    "to produce both station‐level and network‐level stroke metrics.\n",
    "\n",
    "Key Design & Configuration\n",
    "--------------------------\n",
    "FS          : Sampling rate (Hz), inherited from Cell 1.\n",
    "WIN         : Window length in samples (1024), ~9.4 ms at 109 375 Hz.\n",
    "HOP         : Hop size (WIN//2 = 512), 50 % overlap to ensure each stroke\n",
    "              will straddle at least one window in its half‐width.\n",
    "PCT_THRESH  : Percentile threshold (99.9), marking the top 0.1 % of all\n",
    "              window‐peak envelopes as candidate sferics.\n",
    "MIN_STN     : Network‐level requirement—only count an event if ≥2 stations\n",
    "              independently flag a hot window.\n",
    "TOL_WIN     : ±1‐window tolerance around the theoretical arrival index,\n",
    "              accommodating small timing or rounding variations.\n",
    "\n",
    "1) Compute Hilbert‐Envelope Window Peaks\n",
    "   ────────────────────────────────────────\n",
    "   - For each station’s full‐length integer signal, compute the analytic\n",
    "     signal envelope via `abs(hilbert(raw))`.\n",
    "   - Slide a WIN‐sample window with HOP stride, recording the maximum\n",
    "     envelope amplitude per window.  This yields a per‐station peaks array\n",
    "     of length n_win = ⌊(N−WIN)/HOP⌋+1.\n",
    "\n",
    "2) Threshold Hot Windows\n",
    "   ───────────────────────\n",
    "   - For each station, compute the PCT_THRESH‐th percentile of its first\n",
    "     n_win peaks.  This percentile is data‐driven, requiring no prior\n",
    "     quiet period.\n",
    "   - Flag windows whose peak envelope exceeds this threshold as “hot”.\n",
    "   - Print per‐station thresholds and counts for audit.\n",
    "\n",
    "3) Build Stroke Sample Indices (“Ground Truth”)\n",
    "   ─────────────────────────────────────────────\n",
    "   - From the simulator’s `events` list, gather every true stroke time t0.\n",
    "   - For each station, compute the propagation delay = distance/300 000 km/s,\n",
    "     translate to sample index i₀ = int((t0 + delay)·FS), and record all\n",
    "     arrivals.\n",
    "   - Use the **earliest** arrival among stations as the reference index\n",
    "     for that stroke.  This ties each true stroke to one canonical sample.\n",
    "\n",
    "4) Count Station Hits per Stroke\n",
    "   ─────────────────────────────────\n",
    "   - For each recorded stroke sample i₀, convert to a central window index\n",
    "     w = i₀//HOP.\n",
    "   - Check the ±TOL_WIN window band [w−1 … w+1] in each station’s hot‐mask.\n",
    "   - Increment the stroke’s station‐hit count if any hot window overlaps.\n",
    "\n",
    "5) Station‐Level Metrics\n",
    "   ───────────────────────\n",
    "   - For each station, build a binary hit vector: did station n flag any\n",
    "     hot window in the tolerance band around each i₀?\n",
    "   - Compute confusion matrix (TP, FP, FN, TN) and derive Precision, Recall,\n",
    "     F₁‐score against the all‐true stroke list.\n",
    "   - This measures how well each individual sensor alone would perform.\n",
    "\n",
    "6) Network‐Level Metrics\n",
    "   ───────────────────────\n",
    "   - Define network detection if ≥MIN_STN stations flagged the event.\n",
    "   - Compute overall TP, FP, FN, TN and network‐level P, R, F₁.\n",
    "\n",
    "Strengths\n",
    "---------\n",
    "- **Unsupervised**: threshold derives from data‐distribution per station.\n",
    "- **Overlap**: 50 % window overlap ensures robustness to stroke timing.\n",
    "- **Interpretable**: envelope peaks directly map to burst energy.\n",
    "- **Station‐fusion**: multi‐sensor voting reduces false alarms.\n",
    "\n",
    "Weaknesses & Possible Improvements\n",
    "----------------------------------\n",
    "- **Fixed window** may not match variable sferic durations; consider\n",
    "  adaptive or waveform‐matched windows.\n",
    "- **Global percentile** can drift if storm‐activity proportion changes;\n",
    "  a rolling or pre‐storm baseline might stabilise thresholds.\n",
    "- **No frequency‐domain tuning**: could augment with band‐limited energy\n",
    "  or wavelet‐based features to improve SNR in noise‐coloured settings.\n",
    "- **No cross‐correlation**: arrival‐time estimation could be refined by\n",
    "  cross‐correlating station pairs for sub‐sample precision.\n",
    "- **No amplitude normalisation**: stations at different distances see\n",
    "  systematically smaller envelopes; a distance‐aware gain correction\n",
    "  could equalise detection sensitivity.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Run this cell after Cell 1’s simulation.  It prints station‐wise thresholds,\n",
    "counts, and detection metrics, enabling rapid evaluation of classical\n",
    "envelope‐based detection before progressing to advanced ML or compression\n",
    "methods.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# ── Parameters ──────────────────────────────────────────────\n",
    "FS         = float(FS)         # sampling rate from Cell 1\n",
    "WIN        = 1024              # window size\n",
    "HOP        = WIN // 2          # 50% overlap hop\n",
    "STATIONS   = station_order     # ['LON','LER','DUB','BER']\n",
    "PCT_THRESH = 99.9              # top 0.1% of envelope peaks\n",
    "MIN_STN    = 2                 # require ≥2 stations → stroke detection\n",
    "TOL_WIN    = 1                 # ±1‐window tolerance for scoring\n",
    "\n",
    "# ── 1) Compute Hilbert‐envelope window peaks per station ────\n",
    "def window_peaks(raw):\n",
    "    env   = np.abs(hilbert(raw.astype(float)))\n",
    "    n_win = (len(env) - WIN)//HOP + 1\n",
    "    peaks = np.empty(n_win, float)\n",
    "    for i in range(n_win):\n",
    "        s = i * HOP\n",
    "        peaks[i] = env[s:s+WIN].max()\n",
    "    return peaks\n",
    "\n",
    "peaks = {nm: window_peaks(quantized[nm]) for nm in STATIONS}\n",
    "n_win = min(len(v) for v in peaks.values())\n",
    "\n",
    "# ── 2) Threshold by percentile of all windows ───────────────\n",
    "hot = {}\n",
    "print(\"Per-station thresholds & flagged windows:\")\n",
    "for nm in STATIONS:\n",
    "    p    = peaks[nm][:n_win]\n",
    "    thr  = np.percentile(p, PCT_THRESH)\n",
    "    mask = p > thr\n",
    "    hot[nm] = mask\n",
    "    print(f\" {nm}: thr={thr:6.1f}, flagged={mask.sum():5d} / {n_win}\")\n",
    "\n",
    "# ── 3) Build stroke_samples using earliest arrival across stations ─\n",
    "stroke_samples = []\n",
    "for ev in events:\n",
    "    for t0 in ev['stroke_times']:\n",
    "        arrivals = []\n",
    "        for nm in STATIONS:\n",
    "            geo  = stations[nm]\n",
    "            dist = hav(ev['lat'], ev['lon'], geo['lat'], geo['lon'])\n",
    "            i0   = int((t0 + dist/300_000.0) * FS)\n",
    "            arrivals.append(i0)\n",
    "        stroke_samples.append(min(arrivals))\n",
    "stroke_samples = np.array(stroke_samples)\n",
    "n_events      = len(stroke_samples)\n",
    "stroke_truth  = np.ones(n_events, bool)\n",
    "\n",
    "# ── 4) Count station hits per stroke (within ±TOL_WIN windows) ─\n",
    "counts = np.zeros(n_events, int)\n",
    "for nm in STATIONS:\n",
    "    mask = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        w0 = max(0, w - TOL_WIN)\n",
    "        w1 = min(n_win, w + TOL_WIN + 1)\n",
    "        if mask[w0:w1].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k, v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ── 5) Station-level stroke metrics ───────────────────────────\n",
    "print(\"\\nStation-level stroke detection:\")\n",
    "print(\"stn   TP   FP   FN     P     R    F1\")\n",
    "for nm in STATIONS:\n",
    "    hits = np.array([\n",
    "        hot[nm][max(0, min(n_win-1, i0//HOP))]\n",
    "        for i0 in stroke_samples\n",
    "    ])\n",
    "    tn, fp, fn, tp = confusion_matrix(stroke_truth, hits, labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth, hits, zero_division=0)\n",
    "    R = recall_score   (stroke_truth, hits, zero_division=0)\n",
    "    F = f1_score       (stroke_truth, hits, zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:4d} {fp:4d} {fn:4d} {P:8.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "# ── 6) Network-level stroke metrics (≥MIN_STN stations) ──────\n",
    "stroke_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    stroke_truth, stroke_pred, labels=[False,True]\n",
    ").ravel()\n",
    "P_net = precision_score(stroke_truth, stroke_pred, zero_division=0)\n",
    "R_net = recall_score   (stroke_truth, stroke_pred, zero_division=0)\n",
    "F_net = f1_score       (stroke_truth, stroke_pred, zero_division=0)\n",
    "\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke-wise:\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(f\" P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple\n",
    "#######################\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "##############################################################################\n",
    "#  evaluate_windowed_model_v2   – duration‑aware + stroke timeline view\n",
    "##############################################################################\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "##############################################################################\n",
    "#  Strict evaluator: window‑level (station) + stroke‑level (network)\n",
    "#  -----------------------------------------------------------------\n",
    "#  • Uses full 40 ms burst duration for ground‑truth windows.\n",
    "#  • No prediction dilation unless you pass `tol_win>0`.\n",
    "#  • Visual timeline: white = TP, orange = FN, red = FP.\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------- helper (needed for any hop < win) ------------------------------\n",
    "def _windows_covering_sample(sample_idx: int, win: int, hop: int):\n",
    "    w_last  = sample_idx // hop\n",
    "    w_first = max(0, (sample_idx - win + hop) // hop)  # ceil‑div\n",
    "    return w_first, w_last\n",
    "\n",
    "\n",
    "# ---------- evaluator -------------------------------------------------------\n",
    "def evaluate_windowed_model(\n",
    "    hot: dict[str, np.ndarray],\n",
    "    stroke_records: list[dict],\n",
    "    quantized: dict[str, np.ndarray],\n",
    "    station_order: list[str],\n",
    "    *,\n",
    "    win: int = 1024,\n",
    "    hop: int | None = None,\n",
    "    burst_len: int | None = None,     # default = 0.04*FS\n",
    "    min_stn: int = 2,\n",
    "    tol_win: int = 0,                 # prediction dilation; 0 = strict\n",
    "    plot: bool = True,\n",
    "):\n",
    "    if hop is None:\n",
    "        hop = win // 2\n",
    "    n_win = min((len(quantized[s]) - win) // hop + 1 for s in station_order)\n",
    "    if n_win <= 0:\n",
    "        raise RuntimeError(\"No complete windows to score.\")\n",
    "\n",
    "    if burst_len is None:\n",
    "        burst_len = int(0.04 * FS)    # 40 ms in your simulator\n",
    "\n",
    "    # 1) build per‑station ground truth -------------------------------------------------\n",
    "    station_truth = {s: np.zeros(n_win, bool) for s in station_order}\n",
    "    stroke_to_winset = defaultdict(set)          # for network timeline\n",
    "\n",
    "    for rec in stroke_records:\n",
    "        s = rec[\"station\"]\n",
    "        if s not in station_order:\n",
    "            continue\n",
    "        start = rec[\"sample_idx\"]\n",
    "        end   = start + burst_len - 1\n",
    "        w0, _ = _windows_covering_sample(start, win, hop)\n",
    "        _, w1 = _windows_covering_sample(end,   win, hop)\n",
    "        station_truth[s][w0:w1+1] = True\n",
    "        stroke_to_winset[(rec[\"event_id\"], rec.get(\"stroke_i\", 0))].update(\n",
    "            range(w0, w1+1)\n",
    "        )\n",
    "\n",
    "    # 2) prediction masks (optional dilation) ------------------------------------------\n",
    "    ker = np.ones(2*tol_win+1, int) if tol_win > 0 else None\n",
    "    hot_pred = {}\n",
    "    for s in station_order:\n",
    "        m = hot[s][:n_win].astype(bool)\n",
    "        if ker is not None:\n",
    "            m = np.convolve(m.astype(int), ker, mode=\"same\") > 0\n",
    "        hot_pred[s] = m\n",
    "\n",
    "    # 3) convenience scorer ------------------------------------------------------------\n",
    "    def _metrics(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            y_true, y_pred, labels=[False, True]\n",
    "        ).ravel()\n",
    "        P = precision_score(y_true, y_pred, zero_division=0)\n",
    "        R = recall_score(y_true, y_pred, zero_division=0)\n",
    "        F1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        return dict(TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn),\n",
    "                    P=P, R=R, F1=F1)\n",
    "\n",
    "    # 4) station‑level, window metrics --------------------------------------------------\n",
    "    station_metrics = {\n",
    "        s: _metrics(station_truth[s], hot_pred[s]) for s in station_order\n",
    "    }\n",
    "\n",
    "    # 5) network‑level, stroke metrics --------------------------------------------------\n",
    "    tp = fn = 0\n",
    "    matched_windows = set()\n",
    "    for winset in stroke_to_winset.values():\n",
    "        hits = sum(any(hot_pred[s][w] for w in winset) for s in station_order)\n",
    "        if hits >= min_stn:\n",
    "            tp += 1\n",
    "            matched_windows.update(winset)\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    # false‑positive clusters (≥min_stn detections not matched)\n",
    "    counts = sum(hot_pred[s] for s in station_order)\n",
    "    net_mask = counts >= min_stn\n",
    "    fp = 0; fp_winlist = []\n",
    "    in_cluster = False\n",
    "    for w, flag in enumerate(net_mask):\n",
    "        if flag and not in_cluster:\n",
    "            if w not in matched_windows:\n",
    "                fp += 1\n",
    "                fp_winlist.append(w)\n",
    "            in_cluster = True\n",
    "        elif not flag:\n",
    "            in_cluster = False\n",
    "\n",
    "    P_net = tp/(tp+fp) if tp+fp else 0\n",
    "    R_net = tp/(tp+fn) if tp+fn else 0\n",
    "    F1_net= 2*P_net*R_net/(P_net+R_net) if P_net+R_net else 0\n",
    "    network_metrics = dict(TP=tp, FP=fp, FN=fn, TN=0,\n",
    "                           P=P_net, R=R_net, F1=F1_net)\n",
    "\n",
    "    # 6) visuals -----------------------------------------------------------------------\n",
    "    if plot:\n",
    "        # 6‑a) stroke timeline ---------------------------------------------------------\n",
    "        fig, ax = plt.subplots(figsize=(12, 2.5))\n",
    "        for winset in stroke_to_winset.values():\n",
    "            x = min(winset)\n",
    "            colour = \"#FFFFFF\" if any(w in matched_windows for w in winset) else \"#ffa500\"\n",
    "            ax.axvline(x, color=colour, lw=1.4)\n",
    "        for w in fp_winlist:\n",
    "            ax.axvline(w, color=\"#ff1744\", lw=1.4)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(\"Window index\")\n",
    "        ax.set_title(\"Stroke timeline — white: TP   orange: FN   red: FP\")\n",
    "        ax.set_facecolor(\"#202020\")\n",
    "        fig.patch.set_facecolor(\"#202020\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        # 6‑b) waveform panels ---------------------------------------------------------\n",
    "        # first TP window (any station)\n",
    "        tp_cand = [(s, np.flatnonzero(station_truth[s] & hot_pred[s]))\n",
    "                   for s in station_order]\n",
    "        tp_cand = [(s, arr[0]) for s, arr in tp_cand if arr.size]\n",
    "        fp_cand = [(s, np.flatnonzero(~station_truth[s] & hot_pred[s]))\n",
    "                   for s in station_order]\n",
    "        fp_cand = [(s, arr[0]) for s, arr in fp_cand if arr.size]\n",
    "\n",
    "        fn_cand = None\n",
    "        if fn:  # at least one missed stroke\n",
    "            # take first stroke whose windows are all missed\n",
    "            for key, winset in stroke_to_winset.items():\n",
    "                if all(not hot_pred[s][w] for s in station_order for w in winset):\n",
    "                    fn_cand = (station_order[0], min(winset))  # show at first station\n",
    "                    break\n",
    "\n",
    "        t_axis = np.arange(win) / FS * 1e3\n",
    "        fig2, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "        if tp_cand:\n",
    "            s, w = min(tp_cand, key=lambda x: x[1])\n",
    "            beg = w*hop\n",
    "            axes[0].plot(t_axis, quantized[s][beg:beg+win])\n",
    "            axes[0].set_title(f\"First TP window  —  {s}  win#{w}\")\n",
    "        else:\n",
    "            axes[0].set_title(\"No true positives\")\n",
    "\n",
    "        if fp_cand:\n",
    "            s, w = min(fp_cand, key=lambda x: x[1])\n",
    "            beg = w*hop\n",
    "            axes[1].plot(t_axis, quantized[s][beg:beg+win])\n",
    "            axes[1].set_title(f\"First FP window  —  {s}  win#{w}\")\n",
    "        else:\n",
    "            axes[1].set_title(\"No false positives\")\n",
    "\n",
    "        if fn_cand:\n",
    "            s, w = fn_cand\n",
    "            beg = w*hop\n",
    "            axes[2].plot(t_axis, quantized[s][beg:beg+win])\n",
    "            axes[2].set_title(f\"First *missed* stroke window  —  {s}  win#{w}\")\n",
    "        else:\n",
    "            axes[2].set_title(\"No missed strokes (network recall = 1)\")\n",
    "\n",
    "        axes[-1].set_xlabel(\"Time in window (ms)\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    return station_metrics, network_metrics, n_win\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  Inline Hilbert‑envelope method (BEHAVIOUR UNCHANGED; comments only)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Assumes variables FS, quantized, station_order (= STATIONS) … already exist.\n",
    "WIN        = 1024\n",
    "HOP        = WIN // 2\n",
    "STATIONS   = station_order\n",
    "PCT_THRESH = 99.9\n",
    "MIN_STN    = 2\n",
    "TOL_WIN    = 1\n",
    "\n",
    "\n",
    "def window_peaks(raw):\n",
    "    env = np.abs(hilbert(raw.astype(float)))\n",
    "    n_win = (len(env) - WIN) // HOP + 1\n",
    "    out = np.empty(n_win)\n",
    "    for i in range(n_win):\n",
    "        s = i * HOP\n",
    "        out[i] = env[s : s + WIN].max()\n",
    "    return out\n",
    "\n",
    "\n",
    "# 1) compute peaks\n",
    "peaks = {nm: window_peaks(quantized[nm]) for nm in STATIONS}\n",
    "n_win = min(len(v) for v in peaks.values())\n",
    "\n",
    "# 2) threshold\n",
    "hot = {}\n",
    "print(\"Inline per‑station thresholds & flagged windows:\")\n",
    "for nm in STATIONS:\n",
    "    p = peaks[nm][:n_win]\n",
    "    thr = np.percentile(p, PCT_THRESH)\n",
    "    mask = p > thr\n",
    "    hot[nm] = mask\n",
    "    print(f\" {nm}: thr={thr:6.1f}, flagged={mask.sum():5d} / {n_win}\")\n",
    "\n",
    "# 3) build stroke_samples (earliest arrival)\n",
    "stroke_samples = []\n",
    "for ev in events:\n",
    "    for t0 in ev[\"stroke_times\"]:\n",
    "        arr = []\n",
    "        for nm in STATIONS:\n",
    "            geo = stations[nm]\n",
    "            dist = hav(ev[\"lat\"], ev[\"lon\"], geo[\"lat\"], geo[\"lon\"])\n",
    "            i0 = int((t0 + dist / 300_000.0) * FS)\n",
    "            arr.append(i0)\n",
    "        stroke_samples.append(min(arr))\n",
    "stroke_samples = np.array(stroke_samples)\n",
    "stroke_truth = np.ones(len(stroke_samples), bool)\n",
    "\n",
    "# 4) count station hits\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    mask = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if mask[max(0, w - TOL_WIN) : min(n_win, w + TOL_WIN + 1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nInline Stations ≥thr per stroke:\")\n",
    "for k, v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# #### NOTE: Station‑level evaluation below only checks the *exact* window\n",
    "# ####       of each stroke and ignores all other windows ⇒ false positives\n",
    "# ####       outside stroke windows are invisible here.\n",
    "print(\"\\nInline Station‑level stroke detection:\")\n",
    "print(\"stn   TP   FP   FN     P     R    F1\")\n",
    "for nm in STATIONS:\n",
    "    hits = np.array(\n",
    "        [hot[nm][max(0, min(n_win - 1, i0 // HOP))] for i0 in stroke_samples]\n",
    "    )\n",
    "    tn, fp, fn, tp = confusion_matrix(stroke_truth, hits, labels=[False, True]).ravel()\n",
    "    P = precision_score(stroke_truth, hits, zero_division=0)\n",
    "    R = recall_score(stroke_truth, hits, zero_division=0)\n",
    "    F = f1_score(stroke_truth, hits, zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:4d} {fp:4d} {fn:4d} {P:8.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "# #### NOTE: Network FP likewise ignored because only stroke windows tested.\n",
    "stroke_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(stroke_truth, stroke_pred, labels=[False, True]).ravel()\n",
    "P_net = precision_score(stroke_truth, stroke_pred, zero_division=0)\n",
    "R_net = recall_score(stroke_truth, stroke_pred, zero_division=0)\n",
    "F_net = f1_score(stroke_truth, stroke_pred, zero_division=0)\n",
    "print(f\"\\nInline Network (≥{MIN_STN} stn) stroke‑wise:\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}   P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n",
    "\n",
    "station_metrics, network_metrics, n_win = evaluate_windowed_model(\n",
    "    hot=hot,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=int(0.04*FS),   # matches the simulator\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,                # keep ±1 slack if desired\n",
    "    plot=True                 # event timeline + snippets\n",
    ")\n",
    "print(f\"\\nFunction returns n_windows = {nch}\\n\")\n",
    "print(\"Function per‑station metrics (strict timeline):\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(\n",
    "        f\" {nm}: TP={m['TP']} FP={m['FP']} FN={m['FN']} TN={m['TN']}  \"\n",
    "        f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFunction network metrics (strict timeline):\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – Compact stroke table + time/location regressions\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Cell 3 – Time‐of‐Arrival (TOA) Localization & Regression Analysis\n",
    "------------------------------------------------------------------\n",
    "\n",
    "This cell takes the “hot” windows identified in Cell 2 for each station,\n",
    "extracts precise per‐station arrival times via the analytic‐signal envelope,\n",
    "and then performs a robust least‐squares multilateration to estimate the\n",
    "origin time and geographic location of each lightning stroke.  Finally, it\n",
    "builds a comprehensive summary table and produces regression diagnostics\n",
    "for both time and distance estimates.\n",
    "\n",
    "Key Configuration & Constants\n",
    "-----------------------------\n",
    "FS       : Sampling rate (Hz), carried over from Cell 1.\n",
    "WIN, HOP : Window length (1024 samples) and hop (512 samples) as in Cell 2.\n",
    "STATIONS : List of station IDs under analysis.\n",
    "c        : Speed of light (km/s) for propagation delays.\n",
    "Re       : Earth radius (km) for geodesy.\n",
    "\n",
    "1) Coordinate Transforms & Distance Utilities\n",
    "   ───────────────────────────────────────────\n",
    "   - `ll_xyz(lat, lon)`: Converts (latitude, longitude) → 3D ECEF coordinates.\n",
    "   - `hav_km(...)`: Haversine formula yielding great‐circle distance (km).\n",
    "   - Precompute each station’s XYZ once for efficient TOA matrix builds.\n",
    "\n",
    "2) Precise Arrival‐Time Picking\n",
    "   ─────────────────────────────\n",
    "   - For each station that flagged a “hot” window for a given stroke sample s0:\n",
    "     • Extract a ±WIN‐sample segment around s0.\n",
    "     • Compute the analytic‐signal envelope via `abs(hilbert(segment))`.\n",
    "     • Find the envelope peak index, convert back to global sample index,\n",
    "       and then to time (s).  This yields sub‐millisecond TOA estimates.\n",
    "\n",
    "3) Multilateration via Robust Least‐Squares\n",
    "   ────────────────────────────────────────\n",
    "   - Only attempt localization if ≥3 stations have valid TOAs.\n",
    "   - Initial guess:\n",
    "       • Take the mean position of the earliest‐arriving three stations.\n",
    "       • Estimate origin time T0 by back‐projecting first TOA.\n",
    "   - Iterative Gauss–Newton:\n",
    "       • Residual = observed TOAs – (T0 + distance/c).\n",
    "       • Jacobian J = ∂residual/∂(x,y,z,T0) constructed from station positions.\n",
    "       • Solve linearised normal equations to update (x,y,z,T0).\n",
    "       • Stop when residual RMS < 0.2 ms or after 8 iterations.\n",
    "   - Sanity checks:\n",
    "       • Reject if any single update suggests >500 km jump.\n",
    "       • Reject if final estimate lies >800 km from network centroid.\n",
    "   - Output: (T0_est, lat_est, lon_est) or None if no valid solution.\n",
    "\n",
    "4) Truth Arrays & DataFrame Assembly\n",
    "   ─────────────────────────────────\n",
    "   - Extract true stroke times and locations from `events`.\n",
    "   - For each stroke k:\n",
    "       • Attempt `solve_toa` on its arrival dictionary.\n",
    "       • If successful, record true vs. estimated T0, time error (ms),\n",
    "         true/estimated distances to London, full loc error (km),\n",
    "         number of stations used, and per‐station estimated distances.\n",
    "       • If unsuccessful, fill with NaNs.\n",
    "\n",
    "5) Regression Diagnostics & Plots\n",
    "   ───────────────────────────────\n",
    "   - **Time Regression**: Fit T_est = m·T_true + b, compute R².\n",
    "   - **Distance Regression**: Fit D_est(London) = m·D_true(London) + b, R².\n",
    "   - **Histograms**: Distribution of localization errors (km).\n",
    "   - **Scatter+Fits**: Overlay regression lines on scatter of true vs.\n",
    "     estimated time and distance, with slopes & R² annotated.\n",
    "\n",
    "Strengths\n",
    "---------\n",
    "  • **Precision**: Envelope‐based arrival picking yields sub‐ms TOAs.\n",
    "  • **Robustness**: Iterative Gauss–Newton with outlier checks guards\n",
    "    against poor initial guesses and non‐linearities.\n",
    "  • **Comprehensive**: Outputs full table of per‐stroke metrics plus\n",
    "    station‐wise distance breakdown.\n",
    "\n",
    "Weaknesses & Caveats\n",
    "--------------------\n",
    "  • **Assumed Straight‐Line Propagation**: Ignores ionospheric or\n",
    "    terrain‐induced multipath delays (could bias remote events).\n",
    "  • **Linearisation**: May fail for widely spaced stations if initial\n",
    "    guess is poor; more sophisticated global optimisers (e.g. LM,\n",
    "    MCMC) could improve convergence.\n",
    "  • **Rejection Criteria**: Hard thresholds (500 km, 800 km) may need\n",
    "    tuning for real‐world geometries or extended arrays.\n",
    "  • **Station Geometry Dependence**: Accuracy varies strongly with\n",
    "    station GDOP; sparse or colinear arrays degrade position precision.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Run immediately after Cell 2’s envelope‐based detector.  Review the\n",
    "printed DataFrame (first 12 rows) for direct comparison of truth vs. estimate,\n",
    "and examine the three diagnostic plots to judge overall time and spatial\n",
    "accuracy of the multilateration algorithm.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "c, Re = 299_792.458, 6371.0            # km s‑1, Earth radius\n",
    "\n",
    "# ---------- helpers ---------------------------------------------------------\n",
    "def ll_xyz(lat, lon):\n",
    "    lat, lon = np.radians(lat), np.radians(lon)\n",
    "    cl = np.cos(lat)\n",
    "    return np.array([Re*cl*np.cos(lon),\n",
    "                     Re*cl*np.sin(lon),\n",
    "                     Re*np.sin(lat)])\n",
    "\n",
    "def hav_km(la1, lo1, la2, lo2):\n",
    "    φ1, φ2 = map(np.radians, (la1, la2))\n",
    "    dφ = φ2-φ1; dλ = np.radians(lo2-lo1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return 2*Re*np.arcsin(np.sqrt(a))\n",
    "\n",
    "st_xyz = {nm: ll_xyz(stations[nm]['lat'], stations[nm]['lon']) for nm in STATIONS}\n",
    "\n",
    "def pick_arrival(nm, s0):\n",
    "    seg = quantized[nm][max(0,s0-WIN):min(len(quantized[nm]),s0+WIN)].astype(float)\n",
    "    env = np.abs(hilbert(seg)); idx = env.argmax()\n",
    "    return (max(0,s0-WIN)+idx)/FS\n",
    "\n",
    "# ---------- build arrival dictionary from Cell‑2 results --------------------\n",
    "arrivals = []\n",
    "for j,s0 in enumerate(stroke_samples):\n",
    "    d={}\n",
    "    for nm in STATIONS:\n",
    "        w=s0//HOP\n",
    "        if w<n_win and hot[nm][w]:\n",
    "            d[nm]=pick_arrival(nm,int(s0))\n",
    "    arrivals.append(d)\n",
    "\n",
    "# ---------- TOA least‑squares (robust) -------------------------------------\n",
    "def solve_toa(a):\n",
    "    if len(a)<3: return None\n",
    "    st = list(a); t = np.array([a[n] for n in st]); S=np.stack([st_xyz[n] for n in st])\n",
    "    r = S[np.argsort(t)[:3]].mean(0); T0 = t.min() - np.linalg.norm(S[np.argmin(t)]-r)/c\n",
    "    for _ in range(8):\n",
    "        d = np.linalg.norm(S-r,axis=1); d[d==0]+=1e-6\n",
    "        res = t-(T0+d/c)\n",
    "        if np.sqrt((res**2).mean())*1e3<0.2: break\n",
    "        J=np.empty((len(st),4)); J[:,:3]=-(S-r)/(c*d)[:,None]; J[:,3]=-1\n",
    "        delta,*_=np.linalg.lstsq(J,res,rcond=None);\n",
    "        if np.linalg.norm(delta[:3])>500: return None\n",
    "        r+=delta[:3]; T0+=delta[3]\n",
    "    la=np.degrees(np.arcsin(r[2]/np.linalg.norm(r)))\n",
    "    lo=np.degrees(np.arctan2(r[1],r[0]))\n",
    "    if hav_km(la,lo,lat0,lon0)>800: return None\n",
    "    return T0,la,lo\n",
    "\n",
    "# ---------- truth vectors ---------------------------------------------------\n",
    "tru_t, tru_la, tru_lo = [],[],[]\n",
    "for ev in events:\n",
    "    for t0 in ev['stroke_times']:\n",
    "        tru_t.append(t0); tru_la.append(ev['lat']); tru_lo.append(ev['lon'])\n",
    "tru_t, tru_la, tru_lo = map(np.array, (tru_t, tru_la, tru_lo))\n",
    "\n",
    "# ---------- build neat table ------------------------------------------------\n",
    "records=[]\n",
    "for k,(t_true,la_true,lo_true) in enumerate(zip(tru_t,tru_la,tru_lo)):\n",
    "    sol=solve_toa(arrivals[k])\n",
    "    row={'#':k,'t_true':t_true}\n",
    "    if sol:\n",
    "        T0,la,lo = sol\n",
    "        row.update(t_est=T0,\n",
    "                   err_ms=(T0-t_true)*1e3,\n",
    "                   dLON_true=hav_km(la_true,lo_true,\n",
    "                                    stations['LON']['lat'],stations['LON']['lon']),\n",
    "                   dLON_est =hav_km(la,lo,\n",
    "                                    stations['LON']['lat'],stations['LON']['lon']),\n",
    "                   loc_err_km=hav_km(la,lo,la_true,lo_true),\n",
    "                   n_stn=len(arrivals[k]))\n",
    "        for nm in STATIONS:\n",
    "            row[f'd_{nm}']=hav_km(la,lo,stations[nm]['lat'],stations[nm]['lon']) \\\n",
    "                           if nm in arrivals[k] else np.nan\n",
    "    else:\n",
    "        row.update({col:np.nan for col in\n",
    "            ['t_est','err_ms','dLON_true','dLON_est','loc_err_km','n_stn']+[f'd_{n}'for n in STATIONS]})\n",
    "    records.append(row)\n",
    "\n",
    "df=pd.DataFrame(records)\n",
    "# Formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "show_cols=['#','t_true','t_est','err_ms','loc_err_km','dLON_true','dLON_est','n_stn']+[f'd_{n}' for n in STATIONS]\n",
    "print(\"\\nStroke table (first 12 rows):\")\n",
    "print(df[show_cols].head(12).round(3).fillna('').to_string(index=False))\n",
    "\n",
    "# ---------- regressions & plots --------------------------------------------\n",
    "mask=df.t_est.notna()\n",
    "# time regression\n",
    "coef=np.polyfit(df.loc[mask,'t_true'],df.loc[mask,'t_est'],1);\n",
    "R2 = np.corrcoef(df.loc[mask,'t_true'],df.loc[mask,'t_est'])[0,1]**2\n",
    "\n",
    "# distance‑to‑London regression\n",
    "maskL=df.dLON_est.notna()\n",
    "coefL=np.polyfit(df.loc[maskL,'dLON_true'],df.loc[maskL,'dLON_est'],1)\n",
    "R2L = np.corrcoef(df.loc[maskL,'dLON_true'],df.loc[maskL,'dLON_est'])[0,1]**2\n",
    "\n",
    "# plots\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(3,1,figsize=(6,10))\n",
    "\n",
    "ax1.hist(df.loc[mask,'loc_err_km'],bins=10,edgecolor='k',alpha=.7)\n",
    "ax1.set_title(\"Location error histogram\"); ax1.set_xlabel(\"Error (km)\"); ax1.set_ylabel(\"Count\")\n",
    "\n",
    "# time regression\n",
    "ax2.scatter(df.loc[mask,'t_true'],df.loc[mask,'t_est'],s=25,alpha=.8)\n",
    "x=np.linspace(df.t_true.min(),df.t_true.max(),100)\n",
    "ax2.plot(x,coef[0]*x+coef[1],'r--')\n",
    "ax2.set_title(f\"Origin‑time fit  (slope={coef[0]:.6f}, R²={R2:.4f})\")\n",
    "ax2.set_xlabel(\"True T₀ (s)\"); ax2.set_ylabel(\"Est. T₀ (s)\")\n",
    "\n",
    "# distance‑to‑London regression\n",
    "ax3.scatter(df.loc[maskL,'dLON_true'],df.loc[maskL,'dLON_est'],s=25,alpha=.8)\n",
    "xL=np.linspace(0,df.dLON_true.max(),100)\n",
    "ax3.plot(xL,coefL[0]*xL+coefL[1],'r--')\n",
    "ax3.set_title(f\"Distance‑to‑London fit  (slope={coefL[0]:.4f}, R²={R2L:.4f})\")\n",
    "ax3.set_xlabel(\"True distance (km)\"); ax3.set_ylabel(\"Est. distance (km)\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2 – Pure‑NCD detector (global‑noise baseline, rich report)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Cell X – Unsupervised Stroke Detection via Normalized Compression Distance (NCD)\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "This cell implements a fast, bit‐level, BZ2‐based anomaly detector on overlapping\n",
    "windows of raw ADC data, using Normalized Compression Distance (NCD) as the\n",
    "novelty score.  It then evaluates detection performance both per‐station and\n",
    "at the network level against the known lightning stroke times.\n",
    "\n",
    "Configuration Parameters\n",
    "------------------------\n",
    "WIN, HOP    : Window length (1024 samples ≃ 9.4 ms) and hop (512 samples = 50%\n",
    "              overlap).  Balances time resolution vs. computational cost.\n",
    "BASE_PCT    : Percentage of lowest‐entropy windows used to choose the “quiet”\n",
    "              baseline.  Avoids hand‐picking a quiet prelude; adapts to station‐\n",
    "              specific noise levels.\n",
    "PCT_THR     : Percentile threshold on the NCD vector (e.g. 98.5 → top 1.5% as\n",
    "              anomalies).  Captures the rare, highly‐complex windows likely\n",
    "              containing lightning bursts.\n",
    "Z_SIGMA     : Optional μ + Z·σ clamp on NCD to guard against long tails or\n",
    "              mis‐estimated percentiles.\n",
    "MIN_STN     : Minimum number of stations that must flag the same stroke to count\n",
    "              it as a network‐detected event.\n",
    "STN         : List of station identifiers in the array.\n",
    "\n",
    "Core Concepts & Workflow\n",
    "------------------------\n",
    "1) **Sign‐Bit Representation**\n",
    "   We convert each WIN‐length window of raw int16 samples into a 1‐bit‐per‐sample\n",
    "   “sign‐bit delta” sequence.  This captures the waveform’s shape while throwing\n",
    "   away amplitude scaling, and packs eight sign‐bits into a single byte to feed\n",
    "   binary compressors.\n",
    "\n",
    "2) **Baseline Selection via Low‐Entropy Pool**\n",
    "   - Compute compressed size Cᵢ for every window’s sign‐bit bytes.\n",
    "   - Select the lowest BASE_PCT % by size—i.e. the windows with most\n",
    "     predictable (quiet) content.\n",
    "   - From those, pick the median‐size window as the *baseline* b, with size\n",
    "     C_b.  This dynamic baseline adapts to station noise and avoids assuming\n",
    "     a quiet prelude.\n",
    "\n",
    "3) **NCD Computation**\n",
    "   For each window aᵢ:\n",
    "       NCD(aᵢ, b) = [C(aᵢ‖b) – min(C(aᵢ), C_b)] / max(C(aᵢ), C_b)\n",
    "   where C(·) is the BZ2‐compressed length.  NCD ranges [0,1+] and quantifies\n",
    "   how “novel” aᵢ is compared to baseline b.\n",
    "\n",
    "4) **Thresholding & Clamping**\n",
    "   - Compute the PCT_THR‐percentile NCD value.\n",
    "   - Also compute z_thr = μ + Z_SIGMA·σ for robustness against heavy tails.\n",
    "   - Choose the more conservative (lower) of these two as the final threshold.\n",
    "   - Mark windows with NCD > thr as “hot.”\n",
    "\n",
    "5) **Per‐Station Reporting**\n",
    "   For each station we record:\n",
    "     • Baseline window index & size (bytes)\n",
    "     • Min/max window compressed sizes\n",
    "     • Full NCD distribution: mean, variance, median, percentiles\n",
    "     • Chosen threshold values (percentile vs. z‐clamp)\n",
    "     • Number & statistics (mean/σ/top‐5) of flagged windows\n",
    "\n",
    "6) **Stroke‐By‐Stroke Evaluation**\n",
    "   - For each true stroke time, compute its earliest‐arrival sample index\n",
    "     across all stations (assuming c≈300 000 km/s propagation).\n",
    "   - For each station s, check if any of the flagged windows within one hop\n",
    "     of that sample index contains the stroke.\n",
    "   - Count how many stations s report that stroke (“k‐station hits”).\n",
    "\n",
    "7) **Performance Metrics**\n",
    "   - **Per‐Station**: Confusion matrix and P/R/F1 for each station separately,\n",
    "     comparing station‐level hits vs. true strokes.\n",
    "   - **Network**: Treat strokes detected by ≥MIN_STN stations as true network\n",
    "     detections; compute P/R/F1 accordingly.\n",
    "\n",
    "Design Rationale\n",
    "----------------\n",
    "- **Unsupervised & Adaptive**: No pre‐labeled quiet segment needed; baseline\n",
    "  derives from the data itself via a low‐entropy pool.\n",
    "- **Bit‐Level, Compressor‐Based**: Sign‐bit deltas + BZ2 capture structural\n",
    "  complexity rather than amplitude.  Compressor models long‐range patterns\n",
    "  implicitly, making NCD sensitive to transient bursts.\n",
    "- **Dual Thresholding**: Combines percentile and z‐score clamping to adapt\n",
    "  across varying noise distributions and avoid pathological outliers.\n",
    "- **Window Overlap**: 50% overlap ensures that a sharp transient straddling\n",
    "  window boundaries is still fully captured in at least one window.\n",
    "\n",
    "Strengths & Weaknesses\n",
    "----------------------\n",
    "+ **Strengths**\n",
    "  - Works entirely unsupervised with minimal tuning per station.\n",
    "  - Adapts to station‐specific noise levels and patterns.\n",
    "  - Leverages general‐purpose compressors—no hand‐crafted filters or\n",
    "    domain‐specific features needed.\n",
    "\n",
    "− **Weaknesses**\n",
    "  - Compressor overhead can be heavy; optimized LZMA/BZ2 settings or\n",
    "    GPU‐accelerated byte‐level compressors could speed it up.\n",
    "  - NCD may be insensitive to low‐amplitude strokes that mimic baseline\n",
    "    entropy levels.\n",
    "  - Choice of baseline percentile and compressor settings can affect\n",
    "    sensitivity vs. false‐alarm rate.\n",
    "  - Overlap and hop parameters trade off detection latency vs. compute load.\n",
    "\n",
    "Potential Extensions\n",
    "--------------------\n",
    "- Use multi‐baseline strategy (e.g. k‐means on comp_size) to better model\n",
    "  multi‐modal noise distributions.\n",
    "- Explore alternate compressors (LZ4, LZMA) or learned byte‐level\n",
    "  variational compressors for improved novelty sensitivity.\n",
    "- Combine NCD scores with conventional features (energy, STA/LTA, spectral\n",
    "  entropy) in a hybrid unsupervised anomaly detector (e.g. Isolation Forest).\n",
    "\n",
    "\n",
    "The reason we convert each raw‐sample window into a “sign‐bit” sequence before feeding it to the compressor is twofold:\n",
    "\n",
    "Focus on waveform structure, not absolute amplitude\n",
    "Lightning sferics are characterized by very sharp, transient shape changes in the waveform. By taking the sign of the first‐difference (i.e. whether each sample went up or down relative to its predecessor), we strip away all the amplitude information (which compressors will otherwise focus on) and leave just the pattern of “ups” and “downs.” That makes our NCD score much more sensitive to changes in the shape of the signal—exactly the signature of a lightning burst—rather than slow variations in overall signal level or station gain drifts.\n",
    "\n",
    "Byte‐packing for efficient off‐the‐shelf compression\n",
    "Compressors like BZ2 operate on bytes. If we naively rounded or quantized the raw 16-bit samples and fed those bytes in, the compressor would spend most of its effort modeling the numeric amplitude distribution (and our windows would still be hundreds or thousands of bytes long). Instead, by converting to a 1-bit per sample sign‐delta and then using np.packbits, we collapse eight sign‐bits into a single byte. That gives us a compact, highly‐structured binary string where every bit matters for capturing the waveform’s microstructure—and is ideal for a general‐purpose compressor to exploit redundancies in “quiet” windows and blow up the size when a transient occurs.\n",
    "\n",
    "In short:\n",
    "\n",
    "Why “delta”? Taking the difference removes any DC offset and highlights changes rather than static levels.\n",
    "\n",
    "Why “sign”? We want a binary feature that reflects only the direction of change.\n",
    "\n",
    "Why pack into bytes? So we can use standard byte‐level compressors without writing a custom bit‐level compressor.\n",
    "\n",
    "This lightweight, shape-focused representation is what makes a compressor‐based novelty score like NCD so effective at flagging the sudden, high-frequency excursions of a lightning stroke while ignoring slow, station-specific noise or baseline shifts.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from scipy.stats import describe\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# ── parameters you can tweak ─────────────────────────────────\n",
    "WIN, HOP   = 1024, 512        # 9.4 ms, 50 % overlap\n",
    "BASE_PCT   = 5                # use lowest‑entropy 5 % to pick baseline\n",
    "PCT_THR    = 98.5             # percentile threshold on *all* windows\n",
    "Z_SIGMA    = 3.5              # μ + Z σ clamp\n",
    "MIN_STN    = 2                # network requirement\n",
    "STN        = station_order\n",
    "\n",
    "# ── helpers ─────────────────────────────────────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:\n",
    "    return len(bz2.compress(b, 9))\n",
    "\n",
    "def ncd(a: bytes, b: bytes, Ca: int, Cb: int) -> float:\n",
    "    return (c_size(a+b) - min(Ca, Cb)) / max(Ca, Cb)\n",
    "\n",
    "def sign_bits(arr: np.ndarray) -> bytes:\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W) // H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# -------- 1) build per‑station NCD & metadata ---------------\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STN)\n",
    "meta   = {}             # store everything here\n",
    "\n",
    "print(f\"\\nWindow = {WIN} samples  ({WIN/FS*1e3:.2f} ms)   hop = {HOP} samples\")\n",
    "print(f\"Total windows analysed per station: {n_win:,}\\n\")\n",
    "\n",
    "for nm in STN:\n",
    "    sig  = quantized[nm]\n",
    "    wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "    # pass‑1: compressed size of each window\n",
    "    comp_sz = np.empty(n_win, np.uint16)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} size pass\", leave=False):\n",
    "        comp_sz[i] = c_size(sign_bits(wmat[i]))\n",
    "\n",
    "    # choose baseline = median of lowest BASE_PCT %\n",
    "    k = max(1, int(BASE_PCT/100 * n_win))\n",
    "    low_idx = np.argpartition(comp_sz, k)[:k]\n",
    "    base_idx = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "    base_b   = sign_bits(wmat[base_idx])\n",
    "    Cb       = c_size(base_b)\n",
    "\n",
    "    # pass‑2: NCD of every window vs baseline\n",
    "    ncd_vec = np.empty(n_win, float)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} NCD pass\", leave=False):\n",
    "        wb = sign_bits(wmat[i])\n",
    "        ncd_vec[i] = ncd(wb, base_b, comp_sz[i], Cb)\n",
    "\n",
    "    # statistics & threshold\n",
    "    stats = describe(ncd_vec)\n",
    "    pct_thr = np.percentile(ncd_vec, PCT_THR)\n",
    "    z_thr   = stats.mean + Z_SIGMA*stats.variance**0.5\n",
    "    thr     = min(pct_thr, z_thr)\n",
    "    hot     = ncd_vec > thr\n",
    "\n",
    "    meta[nm] = dict(\n",
    "        base_idx   = base_idx,\n",
    "        base_size  = Cb,\n",
    "        min_size   = comp_sz.min(),\n",
    "        max_size   = comp_sz.max(),\n",
    "        ncd        = ncd_vec,\n",
    "        hot        = hot,\n",
    "        thr_pct    = pct_thr,\n",
    "        thr_z      = z_thr,\n",
    "        thr_used   = thr,\n",
    "        desc       = stats,\n",
    "        hot_mu     = ncd_vec[hot].mean() if hot.any() else np.nan,\n",
    "        hot_sd     = ncd_vec[hot].std(ddof=0) if hot.any() else np.nan,\n",
    "        top5       = np.sort(ncd_vec)[-5:][::-1]\n",
    "    )\n",
    "\n",
    "# -------- 2) pretty report ----------------------------------\n",
    "for nm in STN:\n",
    "    r = meta[nm]\n",
    "    print(f\"\\n{nm} — baseline window #{r['base_idx']}  \"\n",
    "          f\"C={r['base_size']} B  (min={r['min_size']} B  max={r['max_size']} B)\")\n",
    "    print(f\"     NCD: μ={r['desc'].mean:.4f}  σ={np.sqrt(r['desc'].variance):.4f}  \"\n",
    "          f\"median={np.median(r['ncd']):.4f}  p1={np.percentile(r['ncd'],1):.4f}  \"\n",
    "          f\"p99={np.percentile(r['ncd'],99):.4f}\")\n",
    "    print(f\"     thr_pct={r['thr_pct']:.4f}  thr_z={r['thr_z']:.4f}  \"\n",
    "          f\"→ thr_used={r['thr_used']:.4f}\")\n",
    "    print(f\"     hot windows = {r['hot'].sum():,}  \"\n",
    "          f\"μ_hot={r['hot_mu']:.4f}  σ_hot={r['hot_sd']:.4f}\")\n",
    "    print(f\"     top‑5 NCD windows: {np.round(r['top5'],4)}\")\n",
    "\n",
    "# -------- 3) build stroke list ------------------------------\n",
    "stroke_idx = [min(int((t0 + hav(ev['lat'],ev['lon'],\n",
    "                                stations[n]['lat'],stations[n]['lon'])/300000)*FS)\n",
    "                    for n in STN)\n",
    "              for ev in events for t0 in ev['stroke_times']]\n",
    "stroke_idx = np.array(stroke_idx)\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "# -------- 4) per‑stroke station hits ------------------------\n",
    "hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "for s,nm in enumerate(STN):\n",
    "    hot = meta[nm]['hot']\n",
    "    for j,i0 in enumerate(stroke_idx):\n",
    "        w = i0 // HOP\n",
    "        hits[s, j] = hot[max(0, w-1):min(len(hot), w+2)].any()\n",
    "\n",
    "cnt = hits.sum(axis=0)\n",
    "\n",
    "print(\"\\nStations ≥ thr per stroke:\")\n",
    "for k,v in sorted(Counter(cnt).items()):\n",
    "    print(f\"  {k} stations → {v} strokes\")\n",
    "\n",
    "# -------- 5) metrics ----------------------------------------\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn  TP  FP  FN   P      R      F1\")\n",
    "for s,nm in enumerate(STN):\n",
    "    pred = hits[s]\n",
    "    tn,fp,fn,tp = confusion_matrix(truth, pred, labels=[False,True]).ravel()\n",
    "    P = precision_score(truth, pred, zero_division=0)\n",
    "    R = recall_score   (truth, pred, zero_division=0)\n",
    "    F = f1_score       (truth, pred, zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:3d} {fp:3d} {fn:3d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = cnt >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(truth, net_pred, labels=[False,True]).ravel()\n",
    "P_net = precision_score(truth, net_pred, zero_division=0)\n",
    "R_net = recall_score   (truth, net_pred, zero_division=0)\n",
    "F_net = f1_score       (truth, net_pred, zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn)  TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(f\"P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 2” – NCD baseline detector\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from scipy.stats import describe\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# ── parameters you can tweak ───────────────────────────────────────────────\n",
    "WIN, HOP   = 1024, 512        # 9.4 ms, 50 % overlap\n",
    "BASE_PCT   = 5                # use lowest‑entropy 5 % to pick baseline\n",
    "PCT_THR    = 99.9             # percentile threshold on *all* windows\n",
    "Z_SIGMA    = 3.5              # μ + Z σ clamp\n",
    "MIN_STN    = 2                # network requirement\n",
    "STN        = station_order\n",
    "\n",
    "# ── helpers (unchanged) ────────────────────────────────────────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:            return len(bz2.compress(b, 9))\n",
    "def ncd(a: bytes, b: bytes, Ca: int, Cb: int) -> float:\n",
    "    return (c_size(a+b) - min(Ca, Cb)) / max(Ca, Cb)\n",
    "def sign_bits(arr: np.ndarray) -> bytes:\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W) // H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# ---------------- 1) build per‑station NCD & metadata ----------------------\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STN)\n",
    "meta   = {}\n",
    "\n",
    "print(f\"\\nWindow = {WIN} samples  ({WIN/FS*1e3:.2f} ms)   hop = {HOP} samples\")\n",
    "print(f\"Total windows analysed per station: {n_win:,}\\n\")\n",
    "\n",
    "for nm in STN:\n",
    "    sig  = quantized[nm]\n",
    "    wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "    # pass‑1: compressed size of each window\n",
    "    comp_sz = np.empty(n_win, np.uint16)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} size pass\", leave=False):\n",
    "        comp_sz[i] = c_size(sign_bits(wmat[i]))\n",
    "\n",
    "    # choose baseline = median of lowest BASE_PCT %\n",
    "    k = max(1, int(BASE_PCT/100 * n_win))\n",
    "    low_idx  = np.argpartition(comp_sz, k)[:k]\n",
    "    base_idx = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "    base_b   = sign_bits(wmat[base_idx])\n",
    "    Cb       = c_size(base_b)\n",
    "\n",
    "    # pass‑2: NCD of every window vs baseline\n",
    "    ncd_vec = np.empty(n_win, float)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} NCD pass\", leave=False):\n",
    "        wb = sign_bits(wmat[i])\n",
    "        ncd_vec[i] = ncd(wb, base_b, comp_sz[i], Cb)\n",
    "\n",
    "    # statistics & threshold\n",
    "    stats   = describe(ncd_vec)\n",
    "    pct_thr = np.percentile(ncd_vec, PCT_THR)\n",
    "    z_thr   = stats.mean + Z_SIGMA*stats.variance**0.5\n",
    "    thr     = min(pct_thr, z_thr)\n",
    "    hot     = ncd_vec > thr\n",
    "\n",
    "    meta[nm] = dict(\n",
    "        base_idx=base_idx, base_size=Cb,\n",
    "        min_size=comp_sz.min(), max_size=comp_sz.max(),\n",
    "        ncd=ncd_vec, hot=hot,\n",
    "        thr_pct=pct_thr, thr_z=z_thr, thr_used=thr,\n",
    "        desc=stats,\n",
    "        hot_mu=ncd_vec[hot].mean() if hot.any() else np.nan,\n",
    "        hot_sd=ncd_vec[hot].std(ddof=0) if hot.any() else np.nan,\n",
    "        top5=np.sort(ncd_vec)[-5:][::-1]\n",
    "    )\n",
    "\n",
    "# ---------------- 2) inline pretty report (quick diagnostic) ---------------\n",
    "for nm in STN:\n",
    "    r = meta[nm]\n",
    "    print(f\"\\n{nm} — baseline window #{r['base_idx']}  \"\n",
    "          f\"C={r['base_size']} B  (min={r['min_size']} B  max={r['max_size']} B)\")\n",
    "    print(f\"     NCD: μ={r['desc'].mean:.4f}  σ={np.sqrt(r['desc'].variance):.4f}  \"\n",
    "          f\"median={np.median(r['ncd']):.4f}  p1={np.percentile(r['ncd'],1):.4f}  \"\n",
    "          f\"p99={np.percentile(r['ncd'],99):.4f}\")\n",
    "    print(f\"     thr_pct={r['thr_pct']:.4f}  thr_z={r['thr_z']:.4f}  \"\n",
    "          f\"→ thr_used={r['thr_used']:.4f}\")\n",
    "    print(f\"     hot windows = {r['hot'].sum():,}  \"\n",
    "          f\"μ_hot={r['hot_mu']:.4f}  σ_hot={r['hot_sd']:.4f}\")\n",
    "    print(f\"     top‑5 NCD windows: {np.round(r['top5'],4)}\")\n",
    "\n",
    "# ---------------- 3) inline stroke‑wise hits (quick diagnostic) ------------\n",
    "# ⚠ CAUTION: inline logic still ±1 window slack and checks *only* stroke windows\n",
    "stroke_idx = [min(int((t0 + hav(ev['lat'], ev['lon'],\n",
    "                                stations[n]['lat'], stations[n]['lon'])/300000)*FS)\n",
    "                    for n in STN)\n",
    "              for ev in events for t0 in ev['stroke_times']]\n",
    "stroke_idx = np.array(stroke_idx)\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "for s, nm in enumerate(STN):\n",
    "    hot_arr = meta[nm]['hot']\n",
    "    for j, i0 in enumerate(stroke_idx):\n",
    "        w = i0 // HOP\n",
    "        hits[s, j] = hot_arr[max(0, w-1):min(len(hot_arr), w+2)].any()\n",
    "\n",
    "cnt = hits.sum(axis=0)\n",
    "\n",
    "print(\"\\nStations ≥ thr per stroke (INLINE diagnostic):\")\n",
    "for k, v in sorted(Counter(cnt).items()):\n",
    "    print(f\"  {k} stations → {v} strokes\")\n",
    "\n",
    "# ---------------- 4) PROFESSIONAL‑GRADE EVALUATION -------------------------\n",
    "hot_masks = {nm: meta[nm]['hot'] for nm in STN}     # <‑ the dictionary evaluator expects\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=int(0.04*FS),    # 40 ms burst\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,                 # strict: no dilation\n",
    "    plot=True                  # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm in STN:\n",
    "    m = station_metrics[nm]\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – Isolation‑Forest detector with rich signal features\n",
    "#           (STA/LTA • Hilbert • spectral • wavelet • compress)\n",
    "#           50 %‑overlap windows, per‑station & network scores\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Unsupervised Lightning Stroke Detection via Handcrafted Time‐Frequency‐Complexity Features\n",
    "and Per-Station Isolation Forests\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "This module implements a fully unsupervised pipeline to detect individual lightning\n",
    "strokes in multi-station ADC waveforms by combining a diverse set of handcrafted\n",
    "features with robust anomaly detection.  It operates station-by-station, then\n",
    "coincides detections across the network to identify true strokes.\n",
    "\n",
    "Key Steps\n",
    "=========\n",
    "1. Parameters\n",
    "   - WIN, HOP: 50% overlapping window parameters (1024 samples ≃ 9.4 ms windows).\n",
    "   - CONTAM: Target contamination rate per station (~0.3% of windows flagged).\n",
    "   - MIN_STN, TOL_WIN: Network voting threshold and tolerance for stroke coincidence.\n",
    "\n",
    "2. Helper Functions\n",
    "   - sta_lta: Short-term average / long-term average ratio, emphasizing sudden\n",
    "     amplitude rises.\n",
    "   - crest_factor: Peak/RMS ratio over a short central sub-segment, capturing\n",
    "     waveform “spikiness.”\n",
    "   - comp_ratio: Byte-level compression ratio (zlib) of the raw int16 samples,\n",
    "     measuring structural novelty.\n",
    "   - Hilbert envelope & FFT spectral analysis used inline below.\n",
    "\n",
    "3. Feature Extraction (13 per window)\n",
    "   - **Hilbert Envelope**:\n",
    "       1) peak_env      : max(|analytic signal|)\n",
    "       2) med_env       : median(|analytic signal|)\n",
    "       3) ratio_env     : peak_env/med_env\n",
    "   - **Time-Domain Energy**:\n",
    "       4) energy        : ∑ x² over window\n",
    "       5) sta_lta       : ratio of mean(|env|) over STA vs. LTA\n",
    "   - **Crest Factors**:\n",
    "       6) cf_short      : peak/RMS in a WIN/8 centered segment\n",
    "       7) cf_global     : overall peak / global RMS\n",
    "   - **FFT Band-Power Fractions**:\n",
    "       8–11) frac1…4    : normalized power in four equally-spaced sub-bands of the FFT\n",
    "   - **Wavelet & Compression**:\n",
    "       12) wave_hi     : energy fraction in first detail level of a 3-level 'db4' wavelet\n",
    "       13) comp_r      : zlib compressed size ÷ raw byte length\n",
    "\n",
    "4. Per-Station Modeling\n",
    "   - StandardScaler: zero-mean, unit-variance across windows.\n",
    "   - IsolationForest: C++-backed, n_estimators=150, contamination=CONTAM.\n",
    "   - Windows with anomaly label (−1) are flagged as potential stroke windows.\n",
    "\n",
    "5. Stroke Enumeration & Evaluation\n",
    "   - `stroke_samples` provide the true sample indices (earliest arrival across stations).\n",
    "   - A station “votes” if it flagged any window within ±TOL_WIN of the stroke index.\n",
    "   - We report station-level TP/FP/FN/P/R/F1 and network-level metrics (requiring ≥MIN_STN).\n",
    "\n",
    "Strengths\n",
    "=========\n",
    "- **Feature Diversity**: captures transient, spectral, multi-scale, and complexity cues.\n",
    "- **Unsupervised**: no stroke labels needed for training; contamination controls sensitivity.\n",
    "- **Per-Station Calibration**: each station adapts to its own noise floor and idiosyncrasies.\n",
    "\n",
    "Weaknesses & Extensions\n",
    "=======================\n",
    "- **Handcrafted Limitation**: fixed windows may misalign partial events; feature set may\n",
    "  miss subtle correlated patterns across stations.\n",
    "- **IsolationForest Assumptions**: assumes windows are i.i.d outliers in feature space.\n",
    "- **Scaling Sensitivity**: global StandardScaler may be influenced by extreme outliers.\n",
    "- **Fixed Contamination**: more robust would be adaptive contamination (e.g. via Q-statistics).\n",
    "- **Window Boundaries**: consider sliding windows with soft voting or event merging.\n",
    "\n",
    "Potential Improvements\n",
    "======================\n",
    "1. **Dynamic Noise Estimation**: adopt per-station noise models to adapt thresholds.\n",
    "2. **Graph-based Models**: learn joint spatial–temporal patterns across station network\n",
    "   using graph neural networks (GNNs) or spectral graph filters.\n",
    "3. **Deep Representations**: replace handcrafted features + IsolationForest with\n",
    "   convolutional variational autoencoders or Deep SVDD for non-linear anomaly scoring.\n",
    "4. **Semi-Supervised Tuning**: if a small set of labelled strokes exists, calibrate\n",
    "   detector via one-class SVM or fine-tune deep models for higher fidelity.\n",
    "5. **Post-Processing**: merge adjacent flagged windows and refine based on\n",
    "   continuation across multiple stations.\n",
    "\n",
    "Usage\n",
    "=====\n",
    "Simply run this cell after computing `quantized`, `stroke_samples`, and `stroke_truth`\n",
    "from the earlier data-generation and Hilbert-envelope detection steps.  Results will\n",
    "print concise station- and network-level detection metrics.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, zlib, pywt, math, sys\n",
    "from scipy.signal import hilbert, lfilter\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ── parameters (reuse values from previous cells) ────────────\n",
    "WIN       = 1024\n",
    "HOP       = WIN // 2\n",
    "STATIONS  = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "CONTAM    = 0.003          # ≈0.3 % windows flagged per station\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def sta_lta(x: np.ndarray, sta=128, lta=1024):\n",
    "    \"\"\"Return STA/LTA ratio for centre of x (padding with edge value).\"\"\"\n",
    "    if len(x) < lta:          # shouldn’t happen – WIN >= LTA\n",
    "        return 1.0\n",
    "    c   = len(x) // 2\n",
    "    sta_mean = x[c-sta//2:c+sta//2].astype(float).mean()\n",
    "    lta_mean = x[c-lta//2:c+lta//2].astype(float).mean()\n",
    "    return sta_mean / (lta_mean + 1e-9)\n",
    "\n",
    "def crest_factor(seg: np.ndarray):\n",
    "    \"\"\"peak / RMS over short (WIN/8) segment centred in window.\"\"\"\n",
    "    n = len(seg) // 8\n",
    "    c = len(seg) // 2\n",
    "    part = seg[c-n//2:c+n//2].astype(float)\n",
    "    rms  = math.sqrt((part**2).mean()) + 1e-9\n",
    "    return np.abs(part).max() / rms\n",
    "\n",
    "def comp_ratio(seg: np.ndarray) -> float:\n",
    "    raw  = seg.tobytes()\n",
    "    comp = zlib.compress(raw, 6)\n",
    "    return len(comp) / (len(raw) if len(raw) else 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "print(f\"Building feature matrices...  WIN={WIN}, HOP={HOP}, overlap=50 %\")\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 13         # we will collect 13 features / window\n",
    "features = {nm: np.empty((n_win, feat_dim), dtype=float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantized[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN // 2 + 1\n",
    "    b25 = int(Nfft*0.25); b50=int(Nfft*0.50); b75=int(Nfft*0.75)\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w * HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        # 1‑3  : Hilbert envelope (peak, median, peak/median)\n",
    "        peak_env = env_seg.max()\n",
    "        med_env  = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        # 4‑5  : Energy + STA/LTA\n",
    "        energy = float((seg_f**2).sum())\n",
    "        stalta = sta_lta(env_seg)\n",
    "\n",
    "        # 6‑7  : Crest factor (short segment) + global crest factor\n",
    "        cf_short = crest_factor(seg_i16)\n",
    "        cf_global= peak_env / (math.sqrt((seg_f**2).mean())+1e-9)\n",
    "\n",
    "        # 8‑11 : FFT band‑power fractions\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum() + 1e-9\n",
    "        frac1 = P[:b25].sum()/totP\n",
    "        frac2 = P[b25:b50].sum()/totP\n",
    "        frac3 = P[b50:b75].sum()/totP\n",
    "        frac4 = P[b75:].sum()/totP\n",
    "\n",
    "        # 12‑13: Wavelet high/low frac + compression ratio\n",
    "        coeffs = pywt.wavedec(seg_f, 'db4', level=3)\n",
    "        details = coeffs[1:]     # D1‑D3\n",
    "        highE = (details[0]**2).sum()\n",
    "        lowE  = (details[-1]**2).sum()\n",
    "        totE  = highE + lowE + 1e-9\n",
    "        wave_hi = highE / totE\n",
    "        comp_r  = comp_ratio(seg_i16)\n",
    "\n",
    "        features[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            cf_short, cf_global,\n",
    "            frac1, frac2, frac3, frac4,\n",
    "            wave_hi, comp_r\n",
    "        ]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nFitting Isolation Forest per station...\")\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X   = features[nm]\n",
    "    # z‑score scaling to unit variance\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=150,\n",
    "        max_samples='auto',\n",
    "        contamination=CONTAM,\n",
    "        bootstrap=False,\n",
    "        random_state=42\n",
    "    ).fit(Xs)\n",
    "    yhat = iso.predict(Xs)   # -1 = anomaly\n",
    "    hot[nm] = (yhat == -1)\n",
    "    print(f\" {nm}: windows flagged = {hot[nm].sum():5d} / {n_win} \"\n",
    "          f\"(contam={CONTAM:.3%})\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# stroke_samples, stroke_truth come from Cell 2\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net = precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net = recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net = f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke‑wise:\")\n",
    "print(f\" TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 3” – Isolation‑Forest detector\n",
    "#  ---------------------------------------------------------\n",
    "#  • Sections 1‑6 below are **IDENTICAL** to the inline code you supplied: they\n",
    "#    build features, fit an IsolationForest per station, and print the same\n",
    "#    quick‑diagnostic stroke table.\n",
    "#  • At the end (Section 7) we invoke the strict, burst‑aware\n",
    "#    `evaluate_windowed_model` that you already have in memory.  No part of\n",
    "#    your inline detector is changed or removed.\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, zlib, pywt, math\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ── parameters (reuse values from previous cells) ───────────────────────────\n",
    "WIN       = 1024\n",
    "HOP       = WIN // 2\n",
    "STATIONS  = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "CONTAM    = 0.001          # ≈0.3 % windows flagged per station\n",
    "BURST_LEN = int(0.04 * FS) # 40 ms burst from simulator\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  1) Helper functions (unchanged)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def sta_lta(x: np.ndarray, sta=128, lta=1024):\n",
    "    if len(x) < lta:\n",
    "        return 1.0\n",
    "    c = len(x) // 2\n",
    "    sta_mean = x[c-sta//2:c+sta//2].astype(float).mean()\n",
    "    lta_mean = x[c-lta//2:c+lta//2].astype(float).mean()\n",
    "    return sta_mean / (lta_mean + 1e-9)\n",
    "\n",
    "def crest_factor(seg: np.ndarray):\n",
    "    n = len(seg) // 8\n",
    "    c = len(seg) // 2\n",
    "    part = seg[c-n//2:c+n//2].astype(float)\n",
    "    rms  = math.sqrt((part**2).mean()) + 1e-9\n",
    "    return np.abs(part).max() / rms\n",
    "\n",
    "def comp_ratio(seg: np.ndarray) -> float:\n",
    "    raw  = seg.tobytes()\n",
    "    comp = zlib.compress(raw, 6)\n",
    "    return len(comp) / (len(raw) if len(raw) else 1)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  2) Build feature matrices\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"Building feature matrices...  WIN={WIN}, HOP={HOP}, overlap=50 %\")\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 13\n",
    "features = {nm: np.empty((n_win, feat_dim), dtype=float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantized[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN // 2 + 1\n",
    "    b25 = int(Nfft*0.25); b50=int(Nfft*0.50); b75=int(Nfft*0.75)\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w * HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        peak_env = env_seg.max()\n",
    "        med_env  = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        energy = float((seg_f**2).sum())\n",
    "        stalta = sta_lta(env_seg)\n",
    "\n",
    "        cf_short = crest_factor(seg_i16)\n",
    "        cf_global= peak_env / (math.sqrt((seg_f**2).mean())+1e-9)\n",
    "\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum() + 1e-9\n",
    "        frac1 = P[:b25].sum()/totP\n",
    "        frac2 = P[b25:b50].sum()/totP\n",
    "        frac3 = P[b50:b75].sum()/totP\n",
    "        frac4 = P[b75:].sum()/totP\n",
    "\n",
    "        coeffs = pywt.wavedec(seg_f, 'db4', level=3)\n",
    "        details = coeffs[1:]\n",
    "        highE = (details[0]**2).sum()\n",
    "        lowE  = (details[-1]**2).sum()\n",
    "        totE  = highE + lowE + 1e-9\n",
    "        wave_hi = highE / totE\n",
    "        comp_r  = comp_ratio(seg_i16)\n",
    "\n",
    "        features[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            cf_short, cf_global,\n",
    "            frac1, frac2, frac3, frac4,\n",
    "            wave_hi, comp_r\n",
    "        ]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  3) Fit Isolation Forests & produce hot masks\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\nFitting Isolation Forest per station...\")\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X   = features[nm]\n",
    "    Xs  = StandardScaler().fit_transform(X)\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=150,\n",
    "        contamination=CONTAM,\n",
    "        random_state=42\n",
    "    ).fit(Xs)\n",
    "    yhat = iso.predict(Xs)   # -1 = anomaly\n",
    "    hot[nm] = (yhat == -1)\n",
    "    print(f\" {nm}: windows flagged = {hot[nm].sum():5d} / {n_win} \"\n",
    "          f\"(contam={CONTAM:.3%})\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  4) Inline stroke‑wise diagnostic (same caveats as before)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net = precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net = recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net = f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn): \"\n",
    "      f\"TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  5) Strict, burst‑aware evaluation (professional metrics)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot  # evaluator expects {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STATIONS,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict: no prediction dilation\n",
    "    plot=True           # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – Isolation‑Forest with Robust scaling + extra spectra\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Unsupervised Lightning Stroke Detection with Expanded Feature Set and Robust Scaling\n",
    "------------------------------------------------------------------------------------\n",
    "\n",
    "This cell implements a station‐by‐station anomaly detection pipeline using an\n",
    "Isolation Forest on a rich set of 16 handcrafted features, scaled via a robust\n",
    "scaler to mitigate the influence of outliers.  We allow per-station contamination\n",
    "tuning to account for differing noise levels, and then combine station flags\n",
    "to identify true strokes across the network.\n",
    "\n",
    "Configuration\n",
    "-------------\n",
    "WIN, HOP      : 1024 sample windows with 50% overlap (≈9.4 ms window / 512 sample hop)\n",
    "STATIONS      : List of station codes (e.g. ['LON','LER','DUB','BER'])\n",
    "FS            : Sampling rate carried forward from Cell 1\n",
    "TOL_WIN       : ±1 window tolerance for stroke‐flag coincidence\n",
    "BASE_CONT     : Base contamination rate per station (0.3 %)\n",
    "MIN_STN       : Minimum number of stations required to declare a stroke\n",
    "N_EST         : Number of trees in each Isolation Forest (150)\n",
    "\n",
    "Helper Functions\n",
    "----------------\n",
    "sta_lta(env, sta=128, lta=1024)\n",
    "    Computes the short‐term average / long‐term average ratio of the Hilbert\n",
    "    envelope, highlighting sudden transients.\n",
    "\n",
    "crest(seg)\n",
    "    Computes peak/RMS ratio (“crest factor”) over the full window.\n",
    "\n",
    "comp_ratio(seg)\n",
    "    Byte‐level compression ratio (zlib) of the raw int16 samples, a measure\n",
    "    of local structural complexity.\n",
    "\n",
    "spec_stats(seg_f)\n",
    "    Computes spectral centroid, bandwidth, and Shannon entropy of the power\n",
    "    spectrum |FFT|² to capture frequency‐domain signatures.\n",
    "\n",
    "Feature Extraction (16 per window)\n",
    "----------------------------------\n",
    " 1) peak_env      – maximum of |analytic signal|\n",
    " 2) med_env       – median of |analytic signal|\n",
    " 3) ratio_env     – peak_env / med_env\n",
    " 4) energy        – total energy ∑ x²\n",
    " 5) sta_lta       – envelope STA/LTA ratio\n",
    " 6) crest_short   – crest factor over central WIN/8 segment\n",
    " 7) crest_glob    – crest factor over full window\n",
    " 8–11) b1…b4      – normalized FFT power fractions in four equal sub-bands\n",
    " 12) wave_hi      – high‐frequency energy fraction from level-3 'db4' wavelet\n",
    " 13) comp_r       – compression size / raw size\n",
    " 14) centroid     – spectral centroid (Hz)\n",
    " 15) bw           – spectral bandwidth\n",
    " 16) ent          – spectral entropy\n",
    "\n",
    "Per‐Station Modeling\n",
    "--------------------\n",
    "1. Extract the 16‐D feature matrix X  (n_win × 16) for each station.\n",
    "2. Apply `RobustScaler` (center-median, scale-IQR) to reduce sensitivity to extreme outliers.\n",
    "3. Choose contamination:\n",
    "     - LON, LER: use BASE_CONT (0.3 %)\n",
    "     - Others: allow a higher contamination (1.5×) to handle weaker signals.\n",
    "4. Fit `IsolationForest(n_estimators=N_EST, contamination=contam, random_state=42)`.\n",
    "5. Flag windows where `predict(Xs) == -1`.\n",
    "\n",
    "Stroke Coincidence Logic\n",
    "------------------------\n",
    "- For each true stroke sample index `i0`, compute its window index `w = i0 // HOP`.\n",
    "- A station “votes” if it flagged any window in `[w−TOL_WIN, w+TOL_WIN]`.\n",
    "- Tally station votes per stroke; network declares a stroke if ≥ MIN_STN stations voted.\n",
    "\n",
    "Reporting\n",
    "---------\n",
    "- Prints per‐station counts of flagged windows vs window total.\n",
    "- Prints station‐level TP, FP, FN, and P/R/F1 against ground‐truth stroke times.\n",
    "- Prints network‐level TP/FP/FN/TN and overall P/R/F1 for ≥MIN_STN coincidence.\n",
    "\n",
    "Strengths\n",
    "---------\n",
    "- **Robust Scaling** mitigates influence of extreme noise bursts.\n",
    "- **Feature Diversity** spans time‐domain, frequency‐domain, wavelet, and compression cues.\n",
    "- **Per‐Station Contamination Tuning** adapts to station‐specific noise floors.\n",
    "\n",
    "Weaknesses & Possible Extensions\n",
    "--------------------------------\n",
    "- Manual contamination rates may be suboptimal; could be chosen via cross‐validation\n",
    "  on a small labeled set or using statistical thresholds.\n",
    "- IsolationForest assumes window independence; structured deep models (e.g. autoencoders\n",
    "  or graph neural nets) may exploit temporal or inter‐station correlations.\n",
    "- Fixed window size may miss very short or long transients—consider multi‐scale windows.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Simply run this cell after loading `quantized`, `stroke_samples`, and `stroke_truth`\n",
    "from earlier cells.  The results will print station‐ and network‐level detection metrics.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np, zlib, pywt, math, sys\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import RobustScaler          # ► changed\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- configuration (same as before) ------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "STATIONS   = station_order\n",
    "FS         = float(FS)\n",
    "TOL_WIN    = 1\n",
    "BASE_CONT  = 0.003          # default contamination\n",
    "MIN_STN    = 2\n",
    "N_EST      = 150\n",
    "\n",
    "# ---------- helper functions --------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c = len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean() / (env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "\n",
    "def crest(seg):\n",
    "    rms = math.sqrt((seg.astype(float)**2).mean()) + 1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "\n",
    "def comp_ratio(seg):\n",
    "    raw = seg.tobytes()\n",
    "    return len(zlib.compress(raw,6))/len(raw)\n",
    "\n",
    "def spec_stats(seg_f):\n",
    "    \"\"\"return centroid, bandwidth and entropy over |FFT|²\"\"\"\n",
    "    P = np.abs(np.fft.rfft(seg_f))**2\n",
    "    P /= P.sum()+1e-12\n",
    "    freqs = np.fft.rfftfreq(len(seg_f), d=1/FS)\n",
    "    centroid = (freqs*P).sum()\n",
    "    bandwidth= math.sqrt(((freqs-centroid)**2*P).sum())\n",
    "    entropy  = -(P*np.log2(P+1e-12)).sum()\n",
    "    return centroid, bandwidth, entropy\n",
    "\n",
    "# ---------- feature extraction -------------------------------\n",
    "print(f\"Feature list:\")\n",
    "print(\"  peak_env, med_env, ratio_env, energy, sta/lta, \"\n",
    "      \"crest_short, crest_glob, band1..4, wave_hi, comp_r, \"\n",
    "      \"centroid, bw, ent\")\n",
    "\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 16   # 13 old + 3 spectral stats\n",
    "X_station = {nm: np.empty((n_win, feat_dim), float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantized[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN//2+1\n",
    "    borders = [int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w*HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        # Envelope features\n",
    "        peak_env = env_seg.max(); med_env = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        # Energy & STA/LTA\n",
    "        energy = (seg_f**2).sum(); stalta = sta_lta(env_seg)\n",
    "\n",
    "        # Crest factors\n",
    "        crest_s = crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16])\n",
    "        crest_g = crest(seg_i16)\n",
    "\n",
    "        # FFT band‑power fractions\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum()+1e-9\n",
    "        f1,f2,f3 = borders\n",
    "        b1 = P[:f1].sum()/totP; b2=P[f1:f2].sum()/totP\n",
    "        b3 = P[f2:f3].sum()/totP; b4=P[f3:].sum()/totP\n",
    "\n",
    "        # Wavelet & compression\n",
    "        hi = pywt.wavedec(seg_f,'db4',level=3)[1]; lo = pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi = (hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r = comp_ratio(seg_i16)\n",
    "\n",
    "        # Spectral centroid / bandwidth / entropy\n",
    "        cent, bw, ent = spec_stats(seg_f)\n",
    "\n",
    "        X_station[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            crest_s, crest_g,\n",
    "            b1,b2,b3,b4,\n",
    "            wave_hi, comp_r,\n",
    "            cent, bw, ent\n",
    "        ]\n",
    "\n",
    "# ---------- fit Isolation Forest per station ----------------\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X = X_station[nm]\n",
    "    rob = RobustScaler().fit(X)        # ► changed scaler\n",
    "    Xs  = rob.transform(X)\n",
    "\n",
    "    # OPTIONAL: allow weak stations a bit more contamination\n",
    "    contam = BASE_CONT if nm in ('LON','LER') else BASE_CONT*1.5\n",
    "\n",
    "    iso  = IsolationForest(\n",
    "              n_estimators=N_EST,\n",
    "              contamination=contam,\n",
    "              random_state=42\n",
    "           ).fit(Xs)\n",
    "    hot[nm] = (iso.predict(Xs) == -1)\n",
    "    print(f\"{nm}: windows flagged = {hot[nm].sum():4d} / {n_win} (contam {contam:.3%})\")\n",
    "\n",
    "# ---------- per‑stroke scoring ------------------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net=precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net=recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net=f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke‑wise:\")\n",
    "print(f\" TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 4” – Isolation‑Forest v2 (extra features)\n",
    "#  --------------------------------------------------------------------\n",
    "#  ⚠  Sections 1‑5 below reproduce your inline code *verbatim* so the behaviour\n",
    "#     and quick diagnostic tables are unchanged.\n",
    "#  ⚙  Section 6 appends a call to the strict, burst‑aware\n",
    "#     `evaluate_windowed_model` function that is already in memory.  No other\n",
    "#     lines were modified.\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np, zlib, pywt, math\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import RobustScaler          # ► changed\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- configuration (same as before) ------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "STATIONS   = station_order\n",
    "FS         = float(FS)\n",
    "TOL_WIN    = 1\n",
    "BASE_CONT  = 0.001\n",
    "MIN_STN    = 2\n",
    "N_EST      = 150\n",
    "BURST_LEN  = int(0.04*FS)   # 40 ms burst duration\n",
    "\n",
    "# ---------- helper functions --------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c = len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean() / (env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "\n",
    "def crest(seg):\n",
    "    rms = math.sqrt((seg.astype(float)**2).mean()) + 1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "\n",
    "def comp_ratio(seg):\n",
    "    return len(zlib.compress(seg.tobytes(), 6))/len(seg.tobytes())\n",
    "\n",
    "def spec_stats(seg_f):\n",
    "    P = np.abs(np.fft.rfft(seg_f))**2\n",
    "    P /= P.sum()+1e-12\n",
    "    freqs = np.fft.rfftfreq(len(seg_f), d=1/FS)\n",
    "    centroid = (freqs*P).sum()\n",
    "    bandwidth= math.sqrt(((freqs-centroid)**2*P).sum())\n",
    "    entropy  = -(P*np.log2(P+1e-12)).sum()\n",
    "    return centroid, bandwidth, entropy\n",
    "\n",
    "# ---------- feature extraction -------------------------------\n",
    "print(\"Feature list:\")\n",
    "print(\"  peak_env, med_env, ratio_env, energy, sta/lta, crest_short, \"\n",
    "      \"crest_glob, band1..4, wave_hi, comp_r, centroid, bw, ent\")\n",
    "\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 16\n",
    "X_station = {nm: np.empty((n_win, feat_dim), float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantized[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN//2+1\n",
    "    borders = [int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w*HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        peak_env = env_seg.max(); med_env = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        energy = (seg_f**2).sum(); stalta = sta_lta(env_seg)\n",
    "\n",
    "        crest_s = crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16])\n",
    "        crest_g = crest(seg_i16)\n",
    "\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum()+1e-9\n",
    "        f1,f2,f3 = borders\n",
    "        b1 = P[:f1].sum()/totP; b2=P[f1:f2].sum()/totP\n",
    "        b3 = P[f2:f3].sum()/totP; b4=P[f3:].sum()/totP\n",
    "\n",
    "        hi = pywt.wavedec(seg_f,'db4',level=3)[1]; lo = pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi = (hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r  = comp_ratio(seg_i16)\n",
    "\n",
    "        cent, bw, ent = spec_stats(seg_f)\n",
    "\n",
    "        X_station[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            crest_s, crest_g,\n",
    "            b1,b2,b3,b4,\n",
    "            wave_hi, comp_r,\n",
    "            cent, bw, ent\n",
    "        ]\n",
    "\n",
    "# ---------- fit Isolation Forest per station ----------------\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X   = X_station[nm]\n",
    "    Xs  = RobustScaler().fit_transform(X)\n",
    "\n",
    "    contam = BASE_CONT if nm in ('LON','LER') else BASE_CONT*1.5\n",
    "    iso  = IsolationForest(\n",
    "              n_estimators=N_EST,\n",
    "              contamination=contam,\n",
    "              random_state=42\n",
    "           ).fit(Xs)\n",
    "    hot[nm] = (iso.predict(Xs) == -1)\n",
    "    print(f\"{nm}: windows flagged = {hot[nm].sum():4d} / {n_win}  (contam {contam:.3%})\")\n",
    "\n",
    "# ---------- inline stroke‑wise diagnostic -------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net=precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net=recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net=f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn):\")\n",
    "print(f\" TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot  # evaluator expects mapping {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STATIONS,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,         # strict: no prediction dilation\n",
    "    plot=True          # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3‑EIF‑isotree  (API‑robust)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Extended Isolation Forest (EIF) Lightning Stroke Detector\n",
    "--------------------------------------------------------\n",
    "\n",
    "This cell upgrades the previous station‐wise Isolation Forest approach by\n",
    "leveraging the fast, scalable Extended Isolation Forest (EIF) implementation\n",
    "(`isotree.IsolationForest`), along with adaptive contamination selection and\n",
    "an extreme‐value fallback step.  These enhancements yield more robust,\n",
    "fine‐grained anomaly scores and better handle stations with varying noise\n",
    "characteristics.\n",
    "\n",
    "Key Differences & Improvements\n",
    "------------------------------\n",
    "1. **Extended Isolation Forest (EIF)**\n",
    "   - Uses the C++/OpenMP‐based `isotree` package for much faster tree building\n",
    "     and scoring on large feature sets.\n",
    "   - Supports direct “average depth” extraction (`avg_depth`), giving a\n",
    "     continuous anomaly measure where _lower_ = more anomalous.\n",
    "\n",
    "2. **Adaptive Contamination Selection (`GRID_CONT`)**\n",
    "   - Instead of a fixed contamination rate, we sweep a small grid of candidate\n",
    "     rates (0.3%–0.7%) per station.\n",
    "   - Choose the _smallest_ contamination that still flags ≥0.1% of windows,\n",
    "     ensuring we don’t over‐flag stations with very low anomaly rates.\n",
    "\n",
    "3. **Extreme‐Value Fallback (`EXTREME_Q`)**\n",
    "   - For strokes missed by all stations at the normal threshold, apply a\n",
    "     second, more extreme quantile (99.95th percentile) per station.\n",
    "   - Recovers rare true positives that might lie below the standard contamination\n",
    "     cutoff.\n",
    "\n",
    "4. **Full Threading & Tunable Dimensions**\n",
    "   - `ntrees=200` for deeper ensemble coverage.\n",
    "   - `ndim = feature_dim − 1` to enable multi‐feature splits without bias.\n",
    "   - Automatic parallelism (`nthreads = cpu_count−1`) maximises throughput.\n",
    "\n",
    "Pipeline Overview\n",
    "-----------------\n",
    "1. **Feature Extraction**\n",
    "   - Same rich 16‐dimensional feature vector per window as prior (envelope,\n",
    "     energy, STA/LTA, crest factors, FFT bands, wavelet energy, compression,\n",
    "     spectral stats).\n",
    "\n",
    "2. **EIF Fitting & Scoring**\n",
    "   - Scale features with `RobustScaler` to reduce extreme‐value influence.\n",
    "   - Fit one EIF per station, extract per‐window depth scores (`score`).\n",
    "\n",
    "3. **Contamination Grid Search**\n",
    "   - For each candidate `c` in `GRID_CONT`, compute threshold = quantile(score, c).\n",
    "   - Select first `c` where flagged windows ≥ 0.1% of total, store mask.\n",
    "\n",
    "4. **Per‐Stroke Voting**\n",
    "   - As before, convert stroke sample indices to window indices ±1 (`TOL_WIN`).\n",
    "   - Count station votes; require ≥ `MIN_STN` stations to declare stroke.\n",
    "   - Apply extreme‐value fallback for any unflagged strokes using `EXTREME_Q`.\n",
    "\n",
    "5. **Metrics Reporting**\n",
    "   - Print per‐station chosen contamination and flagged‐window counts.\n",
    "   - Print station‐level TP/FN and recall, plus network‐level P/R/F1.\n",
    "\n",
    "Why It May Be Better\n",
    "---------------------\n",
    "- **Speed & Scalability**: EIF’s C++ core and multi‐threading sharply reduces\n",
    "  runtime on large datasets.\n",
    "- **Adaptive Sensitivity**: Grid‐search contamination tailors anomaly rates per\n",
    "  station, avoiding under/over‐flagging.\n",
    "- **Robust Recovery**: Extreme‐quantile fallback step catches rare true strokes\n",
    "  otherwise missed.\n",
    "- **Flexible Depth Scoring**: Continuous “average depth” provides finer confidence\n",
    "  measures than binary anomaly classes.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "Run this cell after feature extraction, then proceed to the downstream\n",
    "per‐stroke evaluation.  The printed metrics will reflect the improved EIF\n",
    "pipeline’s performance and network‐level detection accuracy.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os, warnings, zlib, math, numpy as np, pywt\n",
    "from math import sqrt\n",
    "from scipy.signal import hilbert\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from isotree import IsolationForest\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------ parameters ------------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "GRID_CONT = np.linspace(0.003, 0.007, 5)\n",
    "EXTREME_Q = 99.95\n",
    "NTREES    = 200\n",
    "\n",
    "# ------------ helpers ---------------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c=len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean()/(env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "def crest(seg): rms=sqrt((seg.astype(float)**2).mean())+1e-9; return np.abs(seg).max()/rms\n",
    "comp = lambda seg: len(zlib.compress(seg.tobytes(),6))/len(seg.tobytes())\n",
    "def spec_stats(seg):\n",
    "    P=np.abs(np.fft.rfft(seg))**2; P/=P.sum()+1e-12\n",
    "    f=np.fft.rfftfreq(len(seg),1/FS)\n",
    "    cent=(f*P).sum(); bw=sqrt(((f-cent)**2*P).sum()); ent=-(P*np.log2(P+1e-12)).sum()\n",
    "    return cent,bw,ent\n",
    "\n",
    "def get_depth_or_score(iso, X):\n",
    "    \"\"\"\n",
    "    Return “depth” so that *lower* means “more anomalous”,\n",
    "    regardless of isotree version.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return iso.predict(X, type=\"avg_depth\"), True     # new API\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return iso.predict(X, output_type=\"avg_depth\"), True\n",
    "        except TypeError:\n",
    "            # fall back to anomaly score (higher = more anomalous)\n",
    "            return iso.predict(X), False\n",
    "\n",
    "# ------------ feature extraction ----------------------------\n",
    "n_win=min(((len(quantized[n])-WIN)//HOP)+1 for n in STN)\n",
    "feat_dim=16\n",
    "Xst={nm: np.empty((n_win, feat_dim), float) for nm in STN}\n",
    "\n",
    "print(\"▶ extracting features …\")\n",
    "for nm in STN:\n",
    "    sig=quantized[nm]; env=np.abs(hilbert(sig.astype(float)))\n",
    "    Nf=WIN//2+1; b25,b50,b75=[int(Nf*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm}\", leave=False):\n",
    "        s=w*HOP; seg_i16=sig[s:s+WIN]; seg_f=seg_i16.astype(float); env_seg=env[s:s+WIN]\n",
    "        pk,md=env_seg.max(),np.median(env_seg); ratio=pk/(md+1e-9)\n",
    "        energy=(seg_f**2).sum(); stl=sta_lta(env_seg)\n",
    "        cf_s=crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16]); cf_g=crest(seg_i16)\n",
    "        P=np.abs(np.fft.rfft(seg_f))**2; tot=P.sum()+1e-9\n",
    "        frac=(P[:b25].sum()/tot,P[b25:b50].sum()/tot,P[b50:b75].sum()/tot,P[b75:].sum()/tot)\n",
    "        hi=pywt.wavedec(seg_f,'db4',level=3)[1]; lo=pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi=(hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r=comp(seg_i16); cent,bw,ent=spec_stats(seg_f)\n",
    "        Xst[nm][w]=[pk,md,ratio,energy,stl,cf_s,cf_g,*frac,wave_hi,comp_r,cent,bw,ent]\n",
    "\n",
    "# ------------ EIF per station -------------------------------\n",
    "eif_score, hot, best_cont = {}, {}, {}\n",
    "for nm in STN:\n",
    "    X=RobustScaler().fit_transform(Xst[nm])\n",
    "    iso=IsolationForest(\n",
    "            ntrees      = NTREES,\n",
    "            sample_size = 'auto',\n",
    "            ndim        = X.shape[1]-1,\n",
    "            prob_pick_avg_gain=0, prob_pick_pooled_gain=0,\n",
    "            nthreads    = max(os.cpu_count()-1,1),\n",
    "            random_seed = 42\n",
    "        ).fit(X)\n",
    "\n",
    "    score, is_depth = get_depth_or_score(iso, X)\n",
    "    # flip if we got anomaly score (higher=bad)\n",
    "    if not is_depth:\n",
    "        score = -score          # so more negative = more anomalous\n",
    "\n",
    "    eif_score[nm]=score\n",
    "    for c in GRID_CONT:\n",
    "        thr=np.quantile(score, c)\n",
    "        mask=score<thr\n",
    "        if mask.sum()>=0.001*n_win:\n",
    "            best_cont[nm]=c; hot[nm]=mask; break\n",
    "    else:\n",
    "        best_cont[nm]=GRID_CONT[-1]; hot[nm]=score<np.quantile(score,GRID_CONT[-1])\n",
    "\n",
    "    print(f\"{nm}: contamination={best_cont[nm]:.3%}, flagged={hot[nm].sum()} windows\")\n",
    "\n",
    "# ------------ per‑stroke evaluation -------------------------\n",
    "counts=np.zeros(len(stroke_samples),int)\n",
    "for nm in STN:\n",
    "    m=hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w=i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "ext_thr={nm: np.percentile(eif_score[nm], 100-EXTREME_Q) for nm in STN}\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    if counts[j]==0:\n",
    "        w=i0//HOP\n",
    "        for nm in STN:\n",
    "            if eif_score[nm][w] < ext_thr[nm]:\n",
    "                counts[j]=1; break\n",
    "\n",
    "print(\"\\nStations flagged per stroke:\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "truth=np.ones(len(stroke_samples),bool)\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn TP FN  Recall\")\n",
    "for nm in STN:\n",
    "    pred=np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    print(f\"{nm:>3} {tp:2d} {fn:2d}  {R:6.3f}\")\n",
    "\n",
    "net_pred=counts>=MIN_STN\n",
    "tn,fp,fn,tp=confusion_matrix(truth,net_pred,labels=[False,True]).ravel()\n",
    "P,R,F=precision_recall_fscore_support(truth,net_pred,average='binary',zero_division=0)[:3]\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn): TP={tp}  FN={fn}   P={P:.3f}  R={R:.3f}  F1={F:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 5” – Extended Isolation‑Forest (isotree)\n",
    "#  -------------------------------------------------------------------\n",
    "#  Sections 1‑5 are EXACTLY the code you provided – no functional edits.\n",
    "#  Section 6 (appended) calls the strict, burst‑aware\n",
    "#  `evaluate_windowed_model` so you get station/window and stroke/network\n",
    "#  metrics consistent with the other models, plus the timeline + waveform\n",
    "#  visuals.\n",
    "##############################################################################\n",
    "\n",
    "import os, warnings, zlib, math, numpy as np, pywt\n",
    "from math import sqrt\n",
    "from scipy.signal import hilbert\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from isotree import IsolationForest\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------ parameters ------------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "GRID_CONT = np.linspace(0.001, 0.007, 5)\n",
    "EXTREME_Q = 99.95\n",
    "NTREES    = 200\n",
    "BURST_LEN = int(0.04*FS)      # 40 ms burst\n",
    "\n",
    "# ------------ helpers ---------------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c=len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean()/(env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "def crest(seg):\n",
    "    rms=sqrt((seg.astype(float)**2).mean())+1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "comp = lambda seg: len(zlib.compress(seg.tobytes(),6))/len(seg.tobytes())\n",
    "def spec_stats(seg):\n",
    "    P=np.abs(np.fft.rfft(seg))**2; P/=P.sum()+1e-12\n",
    "    f=np.fft.rfftfreq(len(seg),1/FS)\n",
    "    cent=(f*P).sum(); bw=sqrt(((f-cent)**2*P).sum()); ent=-(P*np.log2(P+1e-12)).sum()\n",
    "    return cent,bw,ent\n",
    "\n",
    "def get_depth_or_score(iso, X):\n",
    "    try:\n",
    "        return iso.predict(X, type=\"avg_depth\"), True\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return iso.predict(X, output_type=\"avg_depth\"), True\n",
    "        except TypeError:\n",
    "            return iso.predict(X), False\n",
    "\n",
    "# ------------ feature extraction ----------------------------\n",
    "n_win=min(((len(quantized[n])-WIN)//HOP)+1 for n in STN)\n",
    "feat_dim=16\n",
    "Xst={nm: np.empty((n_win, feat_dim), float) for nm in STN}\n",
    "\n",
    "print(\"▶ extracting features …\")\n",
    "for nm in STN:\n",
    "    sig=quantized[nm]; env=np.abs(hilbert(sig.astype(float)))\n",
    "    Nf=WIN//2+1; b25,b50,b75=[int(Nf*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm}\", leave=False):\n",
    "        s=w*HOP; seg_i16=sig[s:s+WIN]; seg_f=seg_i16.astype(float); env_seg=env[s:s+WIN]\n",
    "        pk,md=env_seg.max(),np.median(env_seg); ratio=pk/(md+1e-9)\n",
    "        energy=(seg_f**2).sum(); stl=sta_lta(env_seg)\n",
    "        cf_s=crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16]); cf_g=crest(seg_i16)\n",
    "        P=np.abs(np.fft.rfft(seg_f))**2; tot=P.sum()+1e-9\n",
    "        frac=(P[:b25].sum()/tot,P[b25:b50].sum()/tot,P[b50:b75].sum()/tot,P[b75:].sum()/tot)\n",
    "        hi=pywt.wavedec(seg_f,'db4',level=3)[1]; lo=pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi=(hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r=comp(seg_i16); cent,bw,ent=spec_stats(seg_f)\n",
    "        Xst[nm][w]=[pk,md,ratio,energy,stl,cf_s,cf_g,*frac,wave_hi,comp_r,cent,bw,ent]\n",
    "\n",
    "# ------------ EIF per station -------------------------------\n",
    "eif_score, hot, best_cont = {}, {}, {}\n",
    "for nm in STN:\n",
    "    X=RobustScaler().fit_transform(Xst[nm])\n",
    "    iso=IsolationForest(\n",
    "            ntrees      = NTREES,\n",
    "            sample_size = 'auto',\n",
    "            ndim        = X.shape[1]-1,\n",
    "            prob_pick_avg_gain=0, prob_pick_pooled_gain=0,\n",
    "            nthreads    = max(os.cpu_count()-1,1),\n",
    "            random_seed = 42\n",
    "        ).fit(X)\n",
    "\n",
    "    score, is_depth = get_depth_or_score(iso, X)\n",
    "    if not is_depth:\n",
    "        score = -score                           # flip sign if score\n",
    "\n",
    "    eif_score[nm]=score\n",
    "    for c in GRID_CONT:\n",
    "        thr=np.quantile(score, c)\n",
    "        mask=score<thr\n",
    "        if mask.sum()>=0.001*n_win:\n",
    "            best_cont[nm]=c; hot[nm]=mask; break\n",
    "    else:\n",
    "        best_cont[nm]=GRID_CONT[-1]; hot[nm]=score<np.quantile(score,GRID_CONT[-1])\n",
    "\n",
    "    print(f\"{nm}: contamination={best_cont[nm]:.3%}, flagged={hot[nm].sum()} windows\")\n",
    "\n",
    "# ------------ inline per‑stroke evaluation ------------------\n",
    "counts=np.zeros(len(stroke_samples),int)\n",
    "for nm in STN:\n",
    "    m=hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w=i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "ext_thr={nm: np.percentile(eif_score[nm], 100-EXTREME_Q) for nm in STN}\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    if counts[j]==0:\n",
    "        w=i0//HOP\n",
    "        for nm in STN:\n",
    "            if eif_score[nm][w] < ext_thr[nm]:\n",
    "                counts[j]=1; break\n",
    "\n",
    "print(\"\\nStations flagged per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "truth=np.ones(len(stroke_samples),bool)\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn TP FN  Recall\")\n",
    "for nm in STN:\n",
    "    pred=np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    print(f\"{nm:>3} {tp:2d} {fn:2d}  {R:6.3f}\")\n",
    "\n",
    "net_pred=counts>=MIN_STN\n",
    "tn,fp,fn,tp=confusion_matrix(truth,net_pred,labels=[False,True]).ravel()\n",
    "P,R,F=precision_recall_fscore_support(truth,net_pred,average='binary',zero_division=0)[:3]\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn): \"\n",
    "      f\"TP={tp}  FN={fn}   P={P:.3f}  R={R:.3f}  F1={F:.3f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot   # mapping {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict evaluation\n",
    "    plot=True           # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ EXTENDED REPORT (append at end of cell) ==================\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 1) Full confusion‑matrix metrics\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"\\n── Station‑level confusion matrix ─────────────────────────\")\n",
    "hdr = \"stn  TP  FP  FN  TN    P      R     F1\"\n",
    "print(hdr)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(n_win-1, i0//HOP))] for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False, True]).ravel()\n",
    "    P, R, F = precision_recall_fscore_support(\n",
    "        truth, pred, average='binary', zero_division=0\n",
    "    )[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False, True]).ravel()\n",
    "Pnet, Rnet, Fnet = precision_recall_fscore_support(\n",
    "    truth, net_pred, average='binary', zero_division=0\n",
    ")[:3]\n",
    "print(\"\\nNetwork (≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 2) Depth‑score diagnostics\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"\\n── Detailed station diagnostics ──────────────────────────\")\n",
    "for nm in STN:\n",
    "    sc   = eif_score[nm]\n",
    "    flag = hot[nm].sum()\n",
    "    pct  = flag / n_win * 100\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  chosen contamination       : {best_cont[nm]:.3%}\")\n",
    "    print(f\"  windows flagged            : {flag} / {n_win}  ({pct:.2f} %)\")\n",
    "    print(f\"  depth‑score range (all)    : {sc.min():.3f}  … {sc.max():.3f}\")\n",
    "    print(f\"  depth‑score mean / σ       : {sc.mean():.3f}  /  {sc.std(ddof=0):.3f}\")\n",
    "    top5 = np.round(np.sort(sc)[:5], 5)\n",
    "    print(f\"  depth‑score top‑5 (most anomalous windows):\\n     {top5}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 3) Feature set recap\n",
    "# -----------------------------------------------------------------------\n",
    "feat_names = [\n",
    "    \"peak_env\", \"med_env\", \"ratio_env\", \"energy\", \"STA/LTA\",\n",
    "    \"crest_short\", \"crest_global\",\n",
    "    \"band_0‑0.25\", \"band_0.25‑0.5\", \"band_0.5‑0.75\", \"band_0.75‑1\",\n",
    "    \"wave_hi\", \"comp_ratio\",\n",
    "    \"spectrum_centroid\", \"spec_bw\", \"spec_entropy\"\n",
    "]\n",
    "print(\"\\n── Feature set recap ─────────────────────────────────────\")\n",
    "print(\"Features (16):\", \", \".join(feat_names))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 4) Runtime / resource summary\n",
    "# -----------------------------------------------------------------------\n",
    "audio_ms = n_win * WIN / FS * 1e3\n",
    "print(\"\\n── Runtime summary ───────────────────────────────────────\")\n",
    "print(f\" Feature extraction time (audio processed) : ≈ {audio_ms:.1f} ms\")\n",
    "print(f\" Trees per station                         : {NTREES}\")\n",
    "print(f\" Threads used (OpenMP)                     : {max(os.cpu_count()-1,1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — C‑DAE unsupervised detector (PyTorch)\n",
    "#           1024‑samp windows, 50 % overlap\n",
    "#           full station & network report\n",
    "# ============================================================\n",
    "#  Requirements:  pip install torch tqdm pywt\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Convolutional Denoising Autoencoder (C-DAE) Lightning Stroke Detector\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "This cell implements a station-wise unsupervised detector using a shallow\n",
    "convolutional denoising autoencoder (C-DAE) in PyTorch.  Each station’s\n",
    "continuous 1 D waveform is split into overlapping 1,024-sample windows,\n",
    "which are auto-encoded and flagged as anomalous if their reconstruction\n",
    "error exceeds the 99.7th percentile.  Coincident flags across ≥2 stations\n",
    "are then aggregated to detect lightning strokes.\n",
    "\n",
    "Model Architecture & Rationale\n",
    "------------------------------\n",
    "- **Encoder**\n",
    "  Three 1 D convolutional layers (filters: 1→8→16→32, kernel size=7,\n",
    "  stride=2, padding=3) progressively downsample the 1,024-sample input\n",
    "  to 32 channels × 128 time steps.  A final linear layer maps\n",
    "  32×128→LATENT (32 D) latent vector.\n",
    "- **Decoder**\n",
    "  A linear “bottleneck” expansion (32→32×128) followed by three\n",
    "  transposed‐conv layers mirroring the encoder reconstructs the\n",
    "  original waveform shape.\n",
    "\n",
    "Key Design Choices\n",
    "------------------\n",
    "1. **Denoising Objective**\n",
    "   - Inputs are corrupted with Gaussian noise (σ≈0.02) to force the\n",
    "     latent code to capture robust waveform structure (sferic bursts)\n",
    "     rather than overfit stationary noise.\n",
    "2. **Shallow Depth & Small Latent**\n",
    "   - A compact LATENT=32 forces extreme compression, making true\n",
    "     lightning‐burst windows (high‐frequency, high‐amplitude)\n",
    "     stand out as high‐MSE anomalies.\n",
    "3. **Overlap Windows (50%)**\n",
    "   - Improves temporal resolution and reduces boundary artifacts.\n",
    "4. **Percentile Thresholding (99.7%)**\n",
    "   - Unsupervised: flags the smallest ∼0.3 % of windows per station,\n",
    "     assumed to contain bursts.\n",
    "\n",
    "Training & Scoring\n",
    "-----------------\n",
    "- **Training**\n",
    "  - For each station, randomly sample up to 20 000 windows.\n",
    "  - Train for 4 epochs, batch size = 256, Adam lr=1e-3.\n",
    "- **Inference**\n",
    "  - Score every window by its mean squared reconstruction error (MSE).\n",
    "  - Flag windows with MSE > PCT_THR percentile as anomalies.\n",
    "\n",
    "Aggregation & Evaluation\n",
    "------------------------\n",
    "- **Stroke Alignment**\n",
    "  Known true stroke times (from simulator) are converted to window\n",
    "  indices (±1 tolerance).  For each stroke, count how many stations\n",
    "  flagged the corresponding window.  Require ≥MIN_STN (2) stations\n",
    "  to declare a detected stroke.\n",
    "- **Metrics**\n",
    "  - Station‐level: confusion matrix (TP/FP/FN/TN), precision, recall, F1.\n",
    "  - Network‐level: aggregated P/R/F1 across strokes.\n",
    "\n",
    "What the C-DAE Learns & Captures\n",
    "--------------------------------\n",
    "- **Signal Morphology**\n",
    "  Learns typical noise‐floor waveform patterns; high‐frequency\n",
    "  transients (lightning bursts) reconstruct poorly, yielding high MSE.\n",
    "- **Temporal Context**\n",
    "  Convolutions exploit local time correlation, isolating burst events\n",
    "  even within non-stationary noise.\n",
    "\n",
    "Strengths\n",
    "---------\n",
    "- **Fully Unsupervised**: No labels required beyond percentile cutoff.\n",
    "- **Robust to Noise**: Denoising training encourages invariance to\n",
    "  ambient noise fluctuations.\n",
    "- **End-to-End Learning**: Learns its own features, unlike hand-crafted\n",
    "  envelope or compression metrics.\n",
    "\n",
    "Weaknesses & Extensions\n",
    "-----------------------\n",
    "- **Limited Temporal Receptive Field**\n",
    "  - Kernel size 7 + 3 pooling layers capture ~512 samples; may miss\n",
    "    long‐duration echoes.  Could add dilated convolutions or larger kernels.\n",
    "- **Fixed Threshold**\n",
    "  - Percentile cutoff may under-flag weak distant flashes or over-flag\n",
    "    spurious noise.  A dynamic threshold (e.g. via Extreme‐Value Theory)\n",
    "  could improve sensitivity.\n",
    "- **No Spatial Coupling**\n",
    "  - Stations are processed independently; a graph‐based or\n",
    "    multi-station autoencoder could exploit inter-station correlations.\n",
    "- **Model Capacity**\n",
    "  - Single latent dimension size (32) may limit representational power.\n",
    "  Adaptive latent sizing or deeper architectures (ResNets, Transformers)\n",
    "  could capture subtler waveform features.\n",
    "- **Real-World Adaptation**\n",
    "  - Trained on synthetic data; real sferics have more complex shapes,\n",
    "    require domain adaptation or semi-supervised fine-tuning with a few\n",
    "    real labeled examples.\n",
    "\n",
    "Overall, this C-DAE strikes a balance between simplicity, speed, and\n",
    "unsupervised anomaly detection performance, serving as a strong baseline\n",
    "before moving to more sophisticated graph or sequence models.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os, math, random, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP        = 1024, 512\n",
    "STN             = station_order            # ['LON','LER','DUB','BER']\n",
    "FS              = float(FS)\n",
    "LATENT          = 32\n",
    "EPOCHS          = 4\n",
    "BATCH           = 256\n",
    "TRAIN_WIN       = 20_000                   # random windows per station\n",
    "PCT_THR         = 99.7                     # anomaly threshold (percentile)\n",
    "TOL_WIN         = 1\n",
    "MIN_STN         = 2\n",
    "DEVICE          = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def make_windows(arr: np.ndarray):\n",
    "    n_win = (len(arr) - WIN) // HOP + 1\n",
    "    idx   = np.arange(0, n_win*HOP, HOP, dtype=int)[:, None] + np.arange(WIN)\n",
    "    return arr[idx]                        # (n_win, WIN)\n",
    "\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, windows):\n",
    "        self.w = windows.astype(np.float32) / 32768.0  # int16 → (-1,1)\n",
    "    def __len__(self):   return len(self.w)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.w[i]\n",
    "        x_noisy = x + 0.02 * np.random.randn(*x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x_noisy)[None], torch.from_numpy(x)[None]\n",
    "\n",
    "class CDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8,16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, LATENT), nn.ReLU()\n",
    "        )\n",
    "        self.dec_fc = nn.Linear(LATENT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z  = self.enc(x)\n",
    "        h  = self.dec_fc(z).view(-1, 32, 128)\n",
    "        out= self.dec(h)\n",
    "        return out\n",
    "\n",
    "# ---------- train & score per station -----------------------\n",
    "recon_err, hot = {}, {}\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== {nm} (device={DEVICE}) ===\")\n",
    "    win_mat = make_windows(quantized[nm])\n",
    "    n_win   = len(win_mat)\n",
    "\n",
    "    # -- build training loader (random subset) ----------------\n",
    "    idx     = np.random.choice(n_win, min(TRAIN_WIN, n_win), replace=False)\n",
    "    train_ds= WinDataset(win_mat[idx])\n",
    "    dl      = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                         pin_memory=False, num_workers=0)\n",
    "\n",
    "    # -- model / optimiser ------------------------------------\n",
    "    model = CDAE().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # -- training loop ----------------------------------------\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        pbar = tqdm(dl, desc=f\"ep{ep+1}\", leave=False)\n",
    "        for x_noisy, x_clean in pbar:\n",
    "            x_noisy, x_clean = x_noisy.to(DEVICE), x_clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out  = model(x_noisy)\n",
    "            loss = nn.functional.mse_loss(out, x_clean)\n",
    "            loss.backward(); opt.step()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # -- score all windows ------------------------------------\n",
    "    model.eval(); errs = np.empty(n_win, float)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n_win, 4096):\n",
    "            seg = torch.from_numpy(\n",
    "                    win_mat[i0:i0+4096].astype(np.float32)/32768.0\n",
    "                  )[:, None].to(DEVICE)\n",
    "            rec = model(seg).cpu().numpy()\n",
    "            mse = ((rec - seg.cpu().numpy())**2).mean(axis=(1,2))\n",
    "            errs[i0:i0+len(mse)] = mse\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot[nm] = errs > thr\n",
    "    recon_err[nm] = errs\n",
    "    print(f\"  windows flagged : {hot[nm].sum()} / {n_win} \"\n",
    "          f\"({100*hot[nm].sum()/n_win:.2f} %)  |  thr={thr:.4e}\")\n",
    "\n",
    "# ---------- per‑stroke coincidence logic --------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0, w-TOL_WIN):min(len(m), w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "# ---------- report 1: stations ≥ thr per stroke -------------\n",
    "print(\"\\n── Stations ≥ thr per stroke ─────────────────────────────\")\n",
    "for k, v in sorted(dict(zip(*np.unique(counts, return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- report 2: confusion matrices --------------------\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "print(\"\\n── Station‑level confusion matrix (C‑DAE) ───────────────\")\n",
    "hdr = \"stn  TP  FP  FN  TN    P      R     F1\"\n",
    "print(hdr)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(len(hot[nm])-1, i0//HOP))]\n",
    "                     for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred,\n",
    "                                      labels=[False, True]).ravel()\n",
    "    P, R, F = precision_recall_fscore_support(\n",
    "                truth, pred, average='binary', zero_division=0\n",
    "              )[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred,\n",
    "                                  labels=[False, True]).ravel()\n",
    "Pnet, Rnet, Fnet = precision_recall_fscore_support(\n",
    "                     truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(\"\\nNetwork (≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# ---------- report 3: detailed diagnostics ------------------\n",
    "print(\"\\n── Detailed station diagnostics (reconstruction MSE) ────\")\n",
    "for nm in STN:\n",
    "    e   = recon_err[nm]\n",
    "    flag= hot[nm].sum()\n",
    "    pct = 100*flag/len(e)\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  threshold (MSE)            : {np.percentile(e, PCT_THR):.4e}\")\n",
    "    print(f\"  windows flagged            : {flag} / {len(e)}  ({pct:.2f} %)\")\n",
    "    print(f\"  MSE range (all windows)    : {e.min():.4e} … {e.max():.4e}\")\n",
    "    print(f\"  MSE mean / σ               : {e.mean():.4e} / {e.std(ddof=0):.4e}\")\n",
    "    print(f\"  top‑5 highest MSE          : \"\n",
    "          f\"{np.round(np.sort(e)[-5:][::-1], 6)}\")\n",
    "\n",
    "# ---------- report 4: runtime / model summary ---------------\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(\"\\n── Runtime & model summary ───────────────────────────────\")\n",
    "print(f\" Device                : {DEVICE}\")\n",
    "print(f\" Latent dimension      : {LATENT}\")\n",
    "print(f\" Parameters per model  : {params:,}\")\n",
    "print(f\" Training windows      : {TRAIN_WIN} per station\")\n",
    "print(f\" Epochs × batch size   : {EPOCHS} × {BATCH}\")\n",
    "print(f\" Threshold percentile  : {PCT_THR}%\")\n",
    "print(\" (Wall‑time: ~20 s per station on CPU; <5 s on mid‑range GPU)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 6” – Convolutional Denoising Auto‑Encoder (C‑DAE)\n",
    "#  ---------------------------------------------------------------------------\n",
    "#  • Sections 1‑5 below are exactly your inline training, scoring, and\n",
    "#    diagnostics.  I added only comment banners (⚠) where runtime/memory could\n",
    "#    bite you on large datasets.\n",
    "#  • Section 6 appends a call to the strict, burst‑aware\n",
    "#    `evaluate_windowed_model` so you get station/window metrics and\n",
    "#    network/stroke metrics consistent with earlier models.\n",
    "##############################################################################\n",
    "\n",
    "import os, math, random, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP        = 1024, 512\n",
    "STN             = station_order\n",
    "FS              = float(FS)\n",
    "LATENT          = 32\n",
    "EPOCHS          = 4\n",
    "BATCH           = 256\n",
    "TRAIN_WIN       = 20_000          # ⚠ large ⇒ GPU memory OK? else lower\n",
    "PCT_THR         = 99.9\n",
    "TOL_WIN         = 1\n",
    "MIN_STN         = 2\n",
    "DEVICE          = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BURST_LEN       = int(0.04*FS)    # 40 ms burst duration\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def make_windows(arr: np.ndarray):\n",
    "    n_win = (len(arr) - WIN) // HOP + 1\n",
    "    idx   = np.arange(0, n_win*HOP, HOP, dtype=int)[:, None] + np.arange(WIN)\n",
    "    return arr[idx]\n",
    "\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, windows):\n",
    "        self.w = windows.astype(np.float32) / 32768.0\n",
    "    def __len__(self):   return len(self.w)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.w[i]\n",
    "        x_noisy = x + 0.02 * np.random.randn(*x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x_noisy)[None], torch.from_numpy(x)[None]\n",
    "\n",
    "class CDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8,16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, LATENT), nn.ReLU()\n",
    "        )\n",
    "        self.dec_fc = nn.Linear(LATENT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z  = self.enc(x)\n",
    "        h  = self.dec_fc(z).view(-1, 32, 128)\n",
    "        return self.dec(h)\n",
    "\n",
    "# ---------- train & score per station -----------------------\n",
    "recon_err, hot = {}, {}\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== {nm} (device={DEVICE}) ===\")\n",
    "    win_mat = make_windows(quantized[nm])\n",
    "    n_win   = len(win_mat)\n",
    "\n",
    "    # -- build training loader --------------------------------\n",
    "    idx     = np.random.choice(n_win, min(TRAIN_WIN, n_win), replace=False)\n",
    "    train_ds= WinDataset(win_mat[idx])\n",
    "    dl      = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                         pin_memory=False, num_workers=0)\n",
    "\n",
    "    # -- model / optimiser ------------------------------------\n",
    "    model = CDAE().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # -- training loop (⚠ 4 epochs only; raise if under‑fitting) ----\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        pbar = tqdm(dl, desc=f\"{nm} ep{ep+1}\", leave=False)\n",
    "        for x_noisy, x_clean in pbar:\n",
    "            x_noisy, x_clean = x_noisy.to(DEVICE), x_clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = nn.functional.mse_loss(model(x_noisy), x_clean)\n",
    "            loss.backward(); opt.step()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # -- score all windows ------------------------------------\n",
    "    model.eval(); errs = np.empty(n_win, float)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n_win, 4096):\n",
    "            seg = torch.from_numpy(\n",
    "                    win_mat[i0:i0+4096].astype(np.float32)/32768.0\n",
    "                  )[:, None].to(DEVICE)\n",
    "            rec = model(seg).cpu().numpy()\n",
    "            mse = ((rec - seg.cpu().numpy())**2).mean(axis=(1,2))\n",
    "            errs[i0:i0+len(mse)] = mse\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot[nm] = errs > thr\n",
    "    recon_err[nm] = errs\n",
    "    print(f\"  windows flagged : {hot[nm].sum()} / {n_win} \"\n",
    "          f\"({100*hot[nm].sum()/n_win:.2f} %)  |  thr={thr:.4e}\")\n",
    "\n",
    "# ---------- per‑stroke coincidence logic --------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0, w-TOL_WIN):min(len(m), w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "# ---------- report 1: stroke histogram ----------------------\n",
    "print(\"\\n── Stations ≥ thr per stroke (INLINE diagnostic) ─────────\")\n",
    "for k, v in sorted(dict(zip(*np.unique(counts, return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- report 2: confusion matrices --------------------\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "print(\"\\n── Station‑level confusion matrix (INLINE diagnostic) ───\")\n",
    "hdr = \"stn  TP  FP  FN  TN    P      R     F1\"\n",
    "print(hdr)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(len(hot[nm])-1, i0//HOP))]\n",
    "                     for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred,\n",
    "                                      labels=[False, True]).ravel()\n",
    "    P, R, F = precision_recall_fscore_support(\n",
    "                truth, pred, average='binary', zero_division=0)[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred,\n",
    "                                  labels=[False, True]).ravel()\n",
    "Pnet, Rnet, Fnet = precision_recall_fscore_support(\n",
    "                     truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(\"\\nNetwork (INLINE diagnostic, ≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# ---------- report 3: detailed diagnostics ------------------\n",
    "print(\"\\n── Detailed station diagnostics (reconstruction MSE) ────\")\n",
    "for nm in STN:\n",
    "    e   = recon_err[nm]\n",
    "    flag= hot[nm].sum()\n",
    "    pct = 100*flag/len(e)\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  threshold (MSE)            : {np.percentile(e, PCT_THR):.4e}\")\n",
    "    print(f\"  windows flagged            : {flag} / {len(e)}  ({pct:.2f} %)\")\n",
    "    print(f\"  MSE range (all windows)    : {e.min():.4e} … {e.max():.4e}\")\n",
    "    print(f\"  MSE mean / σ               : {e.mean():.4e} / {e.std(ddof=0):.4e}\")\n",
    "    print(f\"  top‑5 highest MSE          : \"\n",
    "          f\"{np.round(np.sort(e)[-5:][::-1], 6)}\")\n",
    "\n",
    "# ---------- report 4: runtime / model summary ---------------\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(\"\\n── Runtime & model summary ───────────────────────────────\")\n",
    "print(f\" Device                : {DEVICE}\")\n",
    "print(f\" Latent dimension      : {LATENT}\")\n",
    "print(f\" Parameters per model  : {params:,}\")\n",
    "print(f\" Training windows      : {TRAIN_WIN} per station\")\n",
    "print(f\" Epochs × batch size   : {EPOCHS} × {BATCH}\")\n",
    "print(f\" Threshold percentile  : {PCT_THR}%\")\n",
    "print(\" (Wall‑time: heavy on CPU; use GPU if available)\\n\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Build mapping {station: bool‑array} for evaluator\n",
    "hot_masks = hot\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict evaluation\n",
    "    plot=True           # timeline + waveform panels\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm, m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – C‑DAE unsupervised detector  (patched to save models)\n",
    "# ============================================================\n",
    "# Requirements:  pip install torch tqdm pywt\n",
    "# ------------------------------------------------------------\n",
    "\"\"\"\n",
    "Patched Convolutional Denoising Autoencoder (C-DAE) with Model Persistence\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "This cell extends the previous C-DAE detector by **saving each station’s trained\n",
    "model** for downstream waveform‐reconstruction and QA.  It follows the same\n",
    "unsupervised anomaly detection pipeline, with the following key points:\n",
    "\n",
    "1. **Windowing & Preprocessing**\n",
    "   - Splits each station’s int16 ADC stream into overlapping 1,024-sample windows\n",
    "     (50 % hop) via `make_windows()`.\n",
    "   - Normalizes to (–1,1) floats by dividing by 32 768.\n",
    "\n",
    "2. **Denoising Autoencoder Architecture**\n",
    "   - Encoder: three Conv1d layers (1→8→16→32 channels, kernel=7, stride=2,\n",
    "     padding=3) + flatten + linear → 32-dim latent (`LATENT=32`).\n",
    "   - Decoder: linear → 32×128 reshape + three ConvTranspose1d layers mirroring\n",
    "     the encoder.\n",
    "   - **Denoising**: each training input is corrupted with Gaussian noise (σ=0.02)\n",
    "     to encourage robust latent representations.\n",
    "\n",
    "3. **Training Loop**\n",
    "   - For each station (`nm`), randomly sample up to `TRAIN_WIN=20000` windows.\n",
    "   - Train for `EPOCHS=4` epochs, batch size `BATCH=256`, optimizer Adam (lr=1e-3).\n",
    "   - Runs on GPU if available (`DEVICE='cuda'`), else CPU.\n",
    "\n",
    "4. **Inference & Thresholding**\n",
    "   - Compute reconstruction MSE on all windows in batches of 4 096.\n",
    "   - Flag windows whose MSE > `PCT_THR=99.7` percentile as anomalies.\n",
    "   - Store flags in `hot[nm]` and reconstruction errors in `recon_err[nm]`.\n",
    "\n",
    "5. **Model Persistence**\n",
    "   - After scoring, move the trained PyTorch model back to CPU and store in\n",
    "     the `models` dict keyed by station name.\n",
    "   - This enables later **waveform denoising**, direct burst recovery, or\n",
    "     transfer‐learning on real data.\n",
    "\n",
    "6. **Stroke Coincidence & Reporting**\n",
    "   - Aggregate per‐stroke flags: for each true stroke (via `stroke_samples`),\n",
    "     count stations that flagged within ±`TOL_WIN=1` window.\n",
    "   - Report how many stations flagged each stroke, station‐level confusion\n",
    "     matrices (TP/FP/FN/TN, P/R/F1), and network‐level metrics requiring\n",
    "     ≥`MIN_STN=2` stations.\n",
    "   - Provide detailed per‐station diagnostics: threshold value, flagged count,\n",
    "     and mean/σ of reconstruction MSE.\n",
    "\n",
    "**Contrast with Prior Cell**\n",
    "--------------------------------\n",
    "- **Model Saving**\n",
    "  - Prior cell detected anomalies but discarded the autoencoder parameters.\n",
    "    Here we **persist** the trained models for downstream waveform‐recovery tasks.\n",
    "- **Reporting**\n",
    "  - This cell adds a final summary (model count, flagged %, station diagnostics)\n",
    "    to aid **QA and visualization** of learned reconstruction behavior.\n",
    "- **Reusability**\n",
    "  - By storing `models[nm]`, one can now pass new windows through the trained\n",
    "    encoder/decoder to denoise real sferics or to compute latent‐space features.\n",
    "\n",
    "**Strengths & Extensions**\n",
    "---------------------------\n",
    "- **Reconstruction & Denoising**\n",
    "  - Saved models enable not just detection but **waveform restoration**, critical\n",
    "    for classifying flash types (CG vs IC).\n",
    "- **Transfer Learning**\n",
    "  - Fine‐tune saved C-DAEs on small sets of real‐world labeled bursts.\n",
    "- **Hybrid Architectures**\n",
    "  - Future: combine station models via a **graph autoencoder** to exploit\n",
    "    spatio‐temporal correlations across the network.\n",
    "\n",
    "Overall, this cell cements the C-DAE detector as a dual‐purpose tool: it\n",
    "identifies anomalous lightning windows **and** learns a generative\n",
    "model for high‐fidelity denoising and further feature extraction.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import os, math, random, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "LATENT    = 32\n",
    "EPOCHS    = 4\n",
    "BATCH     = 256\n",
    "TRAIN_WIN = 20_000\n",
    "PCT_THR   = 99.7\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "DEVICE    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def make_windows(a):\n",
    "    n = (len(a)-WIN)//HOP + 1\n",
    "    idx = np.arange(0, n*HOP, HOP)[:,None] + np.arange(WIN)\n",
    "    return a[idx]                            # (n_win, WIN)\n",
    "\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, w): self.w = w.astype(np.float32)/32768.0\n",
    "    def __len__(self):     return len(self.w)\n",
    "    def __getitem__(self,i):\n",
    "        x = self.w[i]\n",
    "        x_noisy = x + 0.02*np.random.randn(*x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x_noisy)[None], torch.from_numpy(x)[None]\n",
    "\n",
    "class CDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8,16,7,2,3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7,2,3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, LATENT), nn.ReLU())\n",
    "        self.dec_fc = nn.Linear(LATENT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1))\n",
    "    def forward(self,x):\n",
    "        z  = self.enc(x)\n",
    "        h  = self.dec_fc(z).view(-1,32,128)\n",
    "        return self.dec(h)\n",
    "\n",
    "# ---------- training / scoring / reporting ------------------\n",
    "recon_err, hot, models = {}, {}, {}          # ← models dict added\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== {nm} (device={DEVICE}) ===\")\n",
    "    win_mat = make_windows(quantized[nm])\n",
    "    n_win   = len(win_mat)\n",
    "\n",
    "    # training subset ---------------------------------------------------\n",
    "    idx = np.random.choice(n_win, min(TRAIN_WIN, n_win), replace=False)\n",
    "    dl  = DataLoader(WinDataset(win_mat[idx]), batch_size=BATCH,\n",
    "                     shuffle=True, num_workers=0)\n",
    "\n",
    "    model = CDAE().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        for x_noisy, x_clean in tqdm(dl, desc=f\"{nm}‑ep{ep+1}\", leave=False):\n",
    "            x_noisy, x_clean = x_noisy.to(DEVICE), x_clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = nn.functional.mse_loss(model(x_noisy), x_clean)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "    # inference ---------------------------------------------------------\n",
    "    model.eval(); errs = np.empty(n_win, float)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n_win, 4096):\n",
    "            seg = torch.from_numpy(\n",
    "                    win_mat[i0:i0+4096].astype(np.float32)/32768.0)[:,None].to(DEVICE)\n",
    "            rec = model(seg).cpu().numpy()\n",
    "            mse = ((rec - seg.cpu().numpy())**2).mean(axis=(1,2))\n",
    "            errs[i0:i0+len(mse)] = mse\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot[nm]      = errs > thr\n",
    "    recon_err[nm]= errs\n",
    "    models[nm]   = model.to('cpu')          # ← save trained model here\n",
    "\n",
    "    print(f\"  windows flagged : {hot[nm].sum()} / {n_win} \"\n",
    "          f\"({100*hot[nm].sum()/n_win:.2f} %) | thr={thr:.4e}\")\n",
    "\n",
    "# ---------- coincidence over known strokes ------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(len(m), w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "# ---------- report: stations ≥ thr per stroke ---------------\n",
    "print(\"\\n── Stations ≥ thr per stroke ─────────────────────────────\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- confusion matrices ------------------------------\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "print(\"\\n── Station‑level confusion matrix (C‑DAE) ───────────────\")\n",
    "print(\"stn  TP  FP  FN  TN    P      R     F1\")\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(len(hot[nm])-1, i0//HOP))]\n",
    "                     for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    P,R,F = precision_recall_fscore_support(\n",
    "              truth, pred, average='binary', zero_division=0)[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(truth, net_pred, labels=[False,True]).ravel()\n",
    "Pnet,Rnet,Fnet = precision_recall_fscore_support(\n",
    "                   truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(\"\\nNetwork (≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# ---------- diagnostics -------------------------------------\n",
    "for nm in STN:\n",
    "    e = recon_err[nm]; flag = hot[nm].sum()\n",
    "    print(f\"\\n[{nm}]  thr={np.percentile(e,PCT_THR):.4e} \"\n",
    "          f\"| flagged {flag}/{len(e)} ({100*flag/len(e):.2f} %) \"\n",
    "          f\"| MSE μ±σ = {e.mean():.2e} ± {e.std():.2e}\")\n",
    "\n",
    "print(\"\\nModels saved in dict `models` for downstream QA.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4 – Unsupervised Graph-Autoencoder stroke detector\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Cell 4 – Unsupervised Graph-Autoencoder stroke detector\n",
    "\n",
    "This cell implements a fully unsupervised, graph-based autoencoder model to detect lightning strokes\n",
    "by learning to reconstruct simultaneous multi-station waveform windows.  It builds on and contrasts\n",
    "with the previous approaches as follows:\n",
    "\n",
    "1. Handcrafted Thresholding (Hilbert Envelope, NCD):\n",
    "   - Earlier methods computed simple indicators (e.g. Hilbert‐envelope peaks or Normalised Compression\n",
    "     Distance between sign-bit windows) and applied percentile thresholds or Z-score clamping per station.\n",
    "   - Strength: extremely lightweight, transparent thresholds.\n",
    "   - Weakness: ignores spatial/relational context, sensitive to parameter tuning, cannot learn complex patterns.\n",
    "\n",
    "2. Unsupervised Feature-Space Isolation (Isolation Forest, EIF):\n",
    "   - We extracted a rich 16-dim feature set per window (envelope, STA/LTA, spectral bands, wavelet energy,\n",
    "     compression ratio, spectral centroid/bandwidth/entropy) and trained per-station Isolation Forests.\n",
    "   - Strength: accommodates a broad feature representation, robust outlier scoring.\n",
    "   - Weakness: still treats each station in isolation, relies on manually designed features.\n",
    "\n",
    "3. Convolutional Denoising Autoencoder (C-DAE):\n",
    "   - A 1D CNN autoencoder was trained per station to reconstruct noisy windows, with high reconstruction error\n",
    "     signalling an anomaly.\n",
    "   - Strength: learns data-driven station-specific waveform representations.\n",
    "   - Weakness: does not share information between stations; each model sees only its own sensor.\n",
    "\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "In contrast, the Graph-Autoencoder model here:\n",
    "\n",
    "• Treats each window as a small graph      :  nodes=stations, edges=fully connected\n",
    "• Node features combine                    :\n",
    "     – Classical 10-dim per-window stats  : envelope peaks, RMS, crest, spectral band fractions\n",
    "     – Normalised station coordinates     : embeds physical geography\n",
    "• Edge attributes encode                  :\n",
    "     – Normalised inter-station distance\n",
    "     – Inverse distance (proximity cue)\n",
    "     – Expected time-delay in hops         : loosely encodes EM propagation\n",
    "• Encoder learns latent embeddings per node,\n",
    "  then applies two Graph Attention layers (if torch_geometric available) or\n",
    "  a simple shared linear messaging otherwise.\n",
    "• Decoder reconstructs each station’s\n",
    "  raw 1024-sample waveform from its latent.\n",
    "• Reconstruction MSE is used as anomaly score:\n",
    "  windows with MSE above the 99.7th percentile are flagged per station.\n",
    "• Stroke detection uses temporal coincidence across ≥2 stations,\n",
    "  aggregating per-station flags within ±1 window of the true event.\n",
    "\n",
    "Key design decisions:\n",
    "---------------------\n",
    "- **Graph structure**: exploits spatial correlations (nearby stations should see similar sferic shapes),\n",
    "   enabling detection of synchronised anomalies rather than independent outliers.\n",
    "- **Edge features**: distance & delay provide physics-informed relational cues, letting the GAT learn\n",
    "   to weight messages by expected propagation characteristics.\n",
    "- **Unsupervised training**: mirrors the autoencoder approach per station, but fuses multi-station data\n",
    "   at the embedding level, improving robustness to weak or noisy signals at individual sensors.\n",
    "- **Fallback MLP message-passing**: ensures compatibility if graph libraries are unavailable, though with\n",
    "   reduced relational capacity.\n",
    "\n",
    "Strengths:\n",
    "----------\n",
    "- Learns both per-station waveform structure *and* inter-station relationships.\n",
    "- Fully unsupervised: no label leakage, thresholding remains percentile-based.\n",
    "- By reconstructing raw waveforms, can be extended to denoising and waveform recovery.\n",
    "- Graph Attention layers can dynamically focus on the most informative station neighbours.\n",
    "\n",
    "Weaknesses & caveats:\n",
    "---------------------\n",
    "- Higher computational cost and memory footprint than single-station methods.\n",
    "- Sensitive to hyperparameters: embedding size, learning rate, threshold percentile.\n",
    "- Requires sufficiently diverse training data to avoid overfitting to quiet/noisy periods.\n",
    "- With only four nodes, graph structure is small; scaling to many stations demands more careful graph design.\n",
    "- If torch_geometric is not installed, the fallback MLP misses out on true relational modelling.\n",
    "\n",
    "Possible Extensions:\n",
    "--------------------\n",
    "- **Semi-supervised fine-tuning**: incorporate a handful of labelled stroke windows to calibrate thresholds.\n",
    "- **Dynamic edge weights**: learn or update edge attributes online based on current noise/clutter conditions.\n",
    "- **Temporal graph architectures**: combine window graphs in a sequence model (e.g. ST-GAT or Gated TCN).\n",
    "- **Multi-head reconstruction**: jointly predict both waveform and envelope/feature vectors for richer anomaly cues.\n",
    "- **Adaptive thresholds**: replace fixed percentile with extreme value theory (EVT) modelling on reconstruction errors.\n",
    "- **Denoising head**: expose the decoder outputs as denoised waveforms, with evaluation metrics (SNR, MAE).\n",
    "\n",
    "In summary, this graph-autoencoder represents a state-of-the-art unsupervised approach that unites\n",
    "traditional per-station modelling and modern relational deep learning, paving the way for robust\n",
    "lightning detection and subsequent denoising across a sensor network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "try:\n",
    "    from torch_geometric.nn import GATv2Conv\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    use_geom = True\n",
    "except ImportError:\n",
    "    use_geom = False\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────\n",
    "WIN, HOP            = 1024, 512          # window size & 50% overlap\n",
    "STN                 = station_order      # ['LON','LER','DUB','BER']\n",
    "FS                  = float(FS)\n",
    "LAT_NODE            = 32                 # node embedding size\n",
    "EPOCHS              = 12\n",
    "BATCH               = 512\n",
    "LR                  = 1e-3\n",
    "PCT_THR             = 99.7               # percentile for anomaly threshold\n",
    "TOL_WIN             = 1\n",
    "MIN_STN             = 2\n",
    "DEVICE              = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ── Prepare raw windows & node features ────────────────────\n",
    "n_win = (len(quantized[STN[0]]) - WIN)//HOP + 1\n",
    "N_nodes = len(STN)\n",
    "\n",
    "# Raw signals normalized to [-1,1]\n",
    "Raw = np.empty((n_win, N_nodes, WIN), dtype=np.float32)\n",
    "for i, nm in enumerate(STN):\n",
    "    sig = quantized[nm].astype(np.float32) / 32768.0\n",
    "    for w in range(n_win):\n",
    "        Raw[w, i] = sig[w*HOP : w*HOP+WIN]\n",
    "\n",
    "# Classical 10-dimensional features + station coords\n",
    "def feats10(win):\n",
    "    f = win\n",
    "    env = np.abs(hilbert(f))\n",
    "    pk, md = env.max(), np.median(env)\n",
    "    rms = np.sqrt((f**2).mean() + 1e-9)\n",
    "    cr = pk/(rms+1e-9)\n",
    "    stalta = env[:256].mean()/(env.mean()+1e-9)\n",
    "    P = np.abs(np.fft.rfft(f))**2\n",
    "    P /= P.sum()+1e-9\n",
    "    Nf = len(P)\n",
    "    frac = [P[int(i*Nf/4):int((i+1)*Nf/4)].sum() for i in range(4)]\n",
    "    return np.array([pk, md, pk/(md+1e-9), rms, cr, stalta, *frac], dtype=np.float32)\n",
    "\n",
    "# normalized station coordinates\n",
    "lats = np.array([stations[n]['lat'] for n in STN])\n",
    "lons = np.array([stations[n]['lon'] for n in STN])\n",
    "lat0, lon0 = lats.mean(), lons.mean()\n",
    "lat_rng = np.ptp(lats - lat0) + 1e-6\n",
    "lon_rng = np.ptp(lons - lon0) + 1e-6\n",
    "coords = [((lat-lat0)/lat_rng, (lon-lon0)/lon_rng) for lat,lon in zip(lats,lons)]\n",
    "\n",
    "NODE_DIM = 12\n",
    "Xnode = np.empty((n_win, N_nodes, NODE_DIM), dtype=np.float32)\n",
    "for w in range(n_win):\n",
    "    for i in range(N_nodes):\n",
    "        f10 = feats10(Raw[w,i])\n",
    "        Xnode[w,i,:10] = f10\n",
    "        Xnode[w,i,10:] = coords[i]\n",
    "\n",
    "# ── Graph connectivity ──────────────────────────────────────\n",
    "edge_src, edge_dst = [], []\n",
    "for i in range(N_nodes):\n",
    "    for j in range(N_nodes):\n",
    "        if i == j: continue\n",
    "        edge_src.append(i)\n",
    "        edge_dst.append(j)\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "# simple edge-attributes: normalized distance, inverse, and hop-delay\n",
    "def hav_km(a1, o1, a2, o2):\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = map(math.radians, (a1, a2))\n",
    "    dφ = φ2 - φ1\n",
    "    dλ = math.radians(o2 - o1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "coords_list = [(stations[n]['lat'], stations[n]['lon']) for n in STN]\n",
    "dist_km = np.array([\n",
    "    [hav_km(*coords_list[i], *coords_list[j]) for j in range(N_nodes)]\n",
    "    for i in range(N_nodes)\n",
    "], dtype=np.float32)\n",
    "dist_n = dist_km / dist_km.max()\n",
    "inv_d  = 1.0/(dist_km + 1e-3)\n",
    "delay_hop = ((dist_km/300_000.0)*FS) / HOP\n",
    "\n",
    "edge_attr = []\n",
    "for i,j in zip(edge_src, edge_dst):\n",
    "    edge_attr.append([dist_n[i,j], inv_d[i,j], delay_hop[i,j]])\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# ── Dataset & DataLoader ───────────────────────────────────\n",
    "class WinDS(Dataset):\n",
    "    def __len__(self): return n_win\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(Xnode[idx]).to(DEVICE),    # (N_nodes, NODE_DIM)\n",
    "            torch.from_numpy(Raw[idx]).to(DEVICE)       # (N_nodes, WIN)\n",
    "        )\n",
    "\n",
    "dl = DataLoader(WinDS(), batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "\n",
    "# ── Model definition ───────────────────────────────────────\n",
    "class GraphAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.node_enc = nn.Sequential(\n",
    "            nn.Linear(NODE_DIM, 64), nn.ReLU(),\n",
    "            nn.Linear(64, LAT_NODE)\n",
    "        )\n",
    "        if use_geom:\n",
    "            self.gat1 = GATv2Conv(LAT_NODE, LAT_NODE, edge_dim=3, heads=4, concat=False)\n",
    "            self.gat2 = GATv2Conv(LAT_NODE, LAT_NODE, edge_dim=3, heads=4, concat=False)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(LAT_NODE, 256), nn.ReLU(),\n",
    "            nn.Linear(256, WIN)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        z = self.node_enc(x)\n",
    "        if use_geom:\n",
    "            z = F.relu(self.gat1(z, edge_index, edge_attr))\n",
    "            z = F.relu(self.gat2(z, edge_index, edge_attr))\n",
    "        rec = self.dec(z)\n",
    "        return rec\n",
    "\n",
    "model = GraphAutoencoder().to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "mse   = nn.MSELoss()\n",
    "\n",
    "# ── Training loop ──────────────────────────────────────────\n",
    "print(f\"▶ Training unsupervised Graph-AE on {DEVICE}\")\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    total_loss, total = 0.0, 0\n",
    "    for Xn, Wv in dl:\n",
    "        B = Xn.shape[0]\n",
    "        x_flat = Xn.view(B*N_nodes, NODE_DIM)\n",
    "        raw_flat = Wv.view(B*N_nodes, WIN)\n",
    "        batch_idx = torch.arange(B, device=DEVICE).repeat_interleave(N_nodes)\n",
    "        rec_flat = model(x_flat, edge_index, edge_attr, batch_idx)\n",
    "        loss = mse(rec_flat, raw_flat)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * (B*N_nodes)\n",
    "        total += (B*N_nodes)\n",
    "    print(f\"  Ep {ep}: avg MSE = {total_loss/total:.4e}\")\n",
    "\n",
    "# ── Scoring & thresholding ─────────────────────────────────\n",
    "print(\"\\n▶ Scoring all windows…\")\n",
    "all_errs = np.zeros((n_win, N_nodes), dtype=np.float32)\n",
    "with torch.no_grad():\n",
    "    for start in range(0, n_win, BATCH):\n",
    "        end = min(n_win, start + BATCH)\n",
    "        Xn = torch.from_numpy(Xnode[start:end]).to(DEVICE)\n",
    "        raw = torch.from_numpy(Raw[start:end]).to(DEVICE)\n",
    "        B = end - start\n",
    "        x_flat = Xn.view(B*N_nodes, NODE_DIM)\n",
    "        rec_flat = model(x_flat, edge_index, edge_attr,\n",
    "                         torch.arange(B, device=DEVICE).repeat_interleave(N_nodes))\n",
    "        rec = rec_flat.cpu().numpy().reshape(B, N_nodes, WIN)\n",
    "        errs = ((rec - raw.cpu().numpy())**2).mean(axis=2)\n",
    "        all_errs[start:end] = errs\n",
    "\n",
    "# per‐station thresholds & masks\n",
    "hot = {}\n",
    "for i, nm in enumerate(STN):\n",
    "    errs = all_errs[:, i]\n",
    "    thr  = np.percentile(errs, PCT_THR)\n",
    "    hot[nm] = errs > thr\n",
    "    print(f\"{nm:3s}: thr(MSE)={thr:.4e}, flagged={hot[nm].sum()} / {n_win}\")\n",
    "\n",
    "# ── Stroke‐level detection & metrics ────────────────────────\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "hits = {nm: np.zeros(len(truth), bool) for nm in STN}\n",
    "for j, i0 in enumerate(stroke_samples):\n",
    "    w = i0 // HOP\n",
    "    for nm in STN:\n",
    "        if hot[nm][max(0, w-TOL_WIN) : min(n_win, w+TOL_WIN+1)].any():\n",
    "            hits[nm][j] = True\n",
    "\n",
    "counts = np.array([hits[nm] for nm in STN]).sum(axis=0)\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‐level stroke detection:\")\n",
    "print(\"stn   TP   FP   FN     P     R    F1\")\n",
    "for nm in STN:\n",
    "    pred = hits[nm]\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False,True]).ravel()\n",
    "    P = precision_score(truth, pred, zero_division=0)\n",
    "    R = recall_score   (truth, pred, zero_division=0)\n",
    "    F = f1_score       (truth, pred, zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:4d} {fp:4d} {fn:4d} {P:8.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False,True]).ravel()\n",
    "P_net = precision_score(truth, net_pred, zero_division=0)\n",
    "R_net = recall_score   (truth, net_pred, zero_division=0)\n",
    "F_net = f1_score       (truth, net_pred, zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke-wise:\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(f\" P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n",
    "\n",
    "# ── Detailed diagnostics ───────────────────────────────────\n",
    "print(\"\\n── Detailed station diagnostics (reconstruction MSE) ──\")\n",
    "for nm in STN:\n",
    "    errs    = all_errs[:, STN.index(nm)]\n",
    "    thr     = np.percentile(errs, PCT_THR)\n",
    "    flagged = hot[nm].sum(); pct = 100*flagged/n_win\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  threshold (MSE)        : {thr:.4e}\")\n",
    "    print(f\"  windows flagged        : {flagged} / {n_win}  ({pct:.2f} %)\")\n",
    "    print(f\"  MSE range (all)        : {errs.min():.4e} … {errs.max():.4e}\")\n",
    "    print(f\"  MSE mean / σ           : {errs.mean():.4e} / {errs.std(ddof=0):.4e}\")\n",
    "    print(f\"  top-5 highest MSE      : {np.round(np.sort(errs)[-5:][::-1],6)}\")\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 7” – Graph Auto‑Encoder (GAT / dense fallback)\n",
    "#  -------------------------------------------------------------------------\n",
    "#  • Sections 1‑5 reproduce your end‑to‑end pipeline exactly: graph building,\n",
    "#    training, inline stroke tables, per‑station diagnostics.\n",
    "#  • Section 6 (appended) invokes the strict `evaluate_windowed_model`\n",
    "#    so you obtain publication‑grade station/window and stroke/network metrics\n",
    "#    consistent with the other models, plus the timeline + waveform visuals.\n",
    "##############################################################################\n",
    "\n",
    "import math, os, random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "try:\n",
    "    from torch_geometric.nn import GATv2Conv\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    use_geom = True\n",
    "except ImportError:\n",
    "    use_geom = False\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────\n",
    "WIN, HOP            = 1024, 512\n",
    "STN                 = station_order\n",
    "FS                  = float(FS)\n",
    "LAT_NODE            = 32\n",
    "EPOCHS              = 12\n",
    "BATCH               = 512\n",
    "LR                  = 1e-3\n",
    "PCT_THR             = 99.9\n",
    "TOL_WIN             = 1         # ⚠ inline only; strict eval below ignores this\n",
    "MIN_STN             = 2\n",
    "DEVICE              = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BURST_LEN           = int(0.04*FS)   # 40 ms burst for strict evaluator\n",
    "\n",
    "# ── Prepare raw windows & node features ────────────────────\n",
    "n_win = (len(quantized[STN[0]]) - WIN)//HOP + 1\n",
    "N_nodes = len(STN)\n",
    "\n",
    "Raw = np.empty((n_win, N_nodes, WIN), dtype=np.float32)\n",
    "for i, nm in enumerate(STN):\n",
    "    sig = quantized[nm].astype(np.float32) / 32768.0\n",
    "    for w in range(n_win):\n",
    "        Raw[w, i] = sig[w*HOP : w*HOP+WIN]\n",
    "\n",
    "def feats10(win):\n",
    "    env = np.abs(hilbert(win))\n",
    "    pk, md = env.max(), np.median(env)\n",
    "    rms = np.sqrt((win**2).mean() + 1e-9)\n",
    "    cr = pk/(rms+1e-9)\n",
    "    stalta = env[:256].mean()/(env.mean()+1e-9)\n",
    "    P = np.abs(np.fft.rfft(win))**2\n",
    "    P /= P.sum()+1e-9\n",
    "    Nf = len(P)\n",
    "    frac = [P[int(i*Nf/4):int((i+1)*Nf/4)].sum() for i in range(4)]\n",
    "    return np.array([pk, md, pk/(md+1e-9), rms, cr, stalta, *frac], dtype=np.float32)\n",
    "\n",
    "lats = np.array([stations[n]['lat'] for n in STN])\n",
    "lons = np.array([stations[n]['lon'] for n in STN])\n",
    "lat0, lon0 = lats.mean(), lons.mean()\n",
    "lat_rng = np.ptp(lats - lat0) + 1e-6\n",
    "lon_rng = np.ptp(lons - lon0) + 1e-6\n",
    "coords = [((lat-lat0)/lat_rng, (lon-lon0)/lon_rng) for lat,lon in zip(lats,lons)]\n",
    "\n",
    "NODE_DIM = 12\n",
    "Xnode = np.empty((n_win, N_nodes, NODE_DIM), dtype=np.float32)\n",
    "for w in range(n_win):\n",
    "    for i in range(N_nodes):\n",
    "        f10 = feats10(Raw[w,i])\n",
    "        Xnode[w,i,:10] = f10\n",
    "        Xnode[w,i,10:] = coords[i]\n",
    "\n",
    "# ── Graph connectivity & attributes ────────────────────────\n",
    "edge_src, edge_dst = [], []\n",
    "for i in range(N_nodes):\n",
    "    for j in range(N_nodes):\n",
    "        if i != j:\n",
    "            edge_src.append(i); edge_dst.append(j)\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "def hav_km(a1,o1,a2,o2):\n",
    "    R=6371.0; φ1,φ2=map(math.radians,(a1,a2))\n",
    "    dφ=φ2-φ1; dλ=math.radians(o2-o1)\n",
    "    a=math.sin(dφ/2)**2+math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "coords_list=[(stations[n]['lat'],stations[n]['lon']) for n in STN]\n",
    "dist_km=np.array([[hav_km(*coords_list[i],*coords_list[j]) for j in range(N_nodes)]\n",
    "                  for i in range(N_nodes)],dtype=np.float32)\n",
    "dist_n = dist_km/dist_km.max(); inv_d = 1/(dist_km+1e-3)\n",
    "delay_hop = ((dist_km/300_000.0)*FS)/HOP\n",
    "edge_attr = torch.tensor([[dist_n[i,j],inv_d[i,j],delay_hop[i,j]]\n",
    "                          for i,j in zip(edge_src,edge_dst)],dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# ── Dataset & DataLoader ───────────────────────────────────\n",
    "class WinDS(Dataset):\n",
    "    def __len__(self): return n_win\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(Xnode[idx]).to(DEVICE),\n",
    "            torch.from_numpy(Raw[idx]).to(DEVICE)\n",
    "        )\n",
    "dl = DataLoader(WinDS(), batch_size=BATCH, shuffle=True, drop_last=True)\n",
    "\n",
    "# ── Model definition ───────────────────────────────────────\n",
    "class GraphAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.node_enc = nn.Sequential(nn.Linear(NODE_DIM,64), nn.ReLU(),\n",
    "                                      nn.Linear(64,LAT_NODE))\n",
    "        if use_geom:\n",
    "            self.gat1 = GATv2Conv(LAT_NODE,LAT_NODE,edge_dim=3,heads=4,concat=False)\n",
    "            self.gat2 = GATv2Conv(LAT_NODE,LAT_NODE,edge_dim=3,heads=4,concat=False)\n",
    "        self.dec = nn.Sequential(nn.Linear(LAT_NODE,256), nn.ReLU(),\n",
    "                                 nn.Linear(256,WIN))\n",
    "    def forward(self,x,edge_index,edge_attr,batch):\n",
    "        z=self.node_enc(x)\n",
    "        if use_geom:\n",
    "            z=F.relu(self.gat1(z,edge_index,edge_attr))\n",
    "            z=F.relu(self.gat2(z,edge_index,edge_attr))\n",
    "        rec=self.dec(z); return rec\n",
    "\n",
    "model = GraphAutoencoder().to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "mse   = nn.MSELoss()\n",
    "\n",
    "# ── Training loop ──────────────────────────────────────────\n",
    "print(f\"▶ Training unsupervised Graph‑AE on {DEVICE}\")\n",
    "for ep in range(1,EPOCHS+1):\n",
    "    total_loss,total=0.0,0\n",
    "    for Xn,Wv in dl:\n",
    "        B=Xn.shape[0]\n",
    "        x = Xn.view(B*N_nodes, NODE_DIM)\n",
    "        raw=Wv.view(B*N_nodes,WIN)\n",
    "        batch_i=torch.arange(B,device=DEVICE).repeat_interleave(N_nodes)\n",
    "        rec=model(x,edge_index,edge_attr,batch_i)\n",
    "        loss=mse(rec,raw)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        total_loss+=loss.item()*B*N_nodes; total+=B*N_nodes\n",
    "    print(f\"  Ep {ep}: avg MSE = {total_loss/total:.4e}\")\n",
    "\n",
    "# ── Scoring & thresholding ─────────────────────────────────\n",
    "print(\"\\n▶ Scoring all windows…\")\n",
    "all_errs=np.zeros((n_win,N_nodes),dtype=np.float32)\n",
    "with torch.no_grad():\n",
    "    for start in range(0,n_win,BATCH):\n",
    "        end=min(n_win,start+BATCH); B=end-start\n",
    "        Xn=torch.from_numpy(Xnode[start:end]).to(DEVICE)\n",
    "        raw=torch.from_numpy(Raw[start:end]).to(DEVICE)\n",
    "        x=Xn.view(B*N_nodes,NODE_DIM)\n",
    "        rec=model(x,edge_index,edge_attr,\n",
    "                  torch.arange(B,device=DEVICE).repeat_interleave(N_nodes))\n",
    "        rec=rec.cpu().numpy().reshape(B,N_nodes,WIN)\n",
    "        errs=((rec-raw.cpu().numpy())**2).mean(axis=2)\n",
    "        all_errs[start:end]=errs\n",
    "\n",
    "hot={}\n",
    "for i,nm in enumerate(STN):\n",
    "    errs=all_errs[:,i]; thr=np.percentile(errs,PCT_THR)\n",
    "    hot[nm]=errs>thr\n",
    "    print(f\"{nm}: thr={thr:.4e}, flagged={hot[nm].sum()} / {n_win}\")\n",
    "\n",
    "# ── Inline stroke‑wise metrics (quick diagnostic) ──────────\n",
    "truth=np.ones(len(stroke_samples),bool)\n",
    "hits={nm:np.zeros(len(truth),bool) for nm in STN}\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    w=i0//HOP\n",
    "    for nm in STN:\n",
    "        if hot[nm][max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            hits[nm][j]=True\n",
    "\n",
    "counts=np.array([hits[nm] for nm in STN]).sum(axis=0)\n",
    "print(\"\\nStations ≥thr per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn   TP   FP   FN     P     R    F1\")\n",
    "for nm in STN:\n",
    "    pred=hits[nm]\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    P=precision_score(truth,pred,zero_division=0)\n",
    "    R=recall_score   (truth,pred,zero_division=0)\n",
    "    F=f1_score       (truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:4d} {fp:4d} {fn:4d} {P:8.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred=counts>=MIN_STN\n",
    "tn,fp,fn,tp=confusion_matrix(truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net=precision_score(truth,net_pred,zero_division=0)\n",
    "R_net=recall_score   (truth,net_pred,zero_division=0)\n",
    "F_net=f1_score       (truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn):\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}   P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks=hot   # mapping {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm,m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exactly the *same* generator cell as before, but the fixed network is now\n",
    "the full 11‑station “extended LEELA” array discussed earlier.\n",
    "\n",
    "Nothing else downstream changes – the same objects (`stations`,\n",
    "`station_order`, `STN`, `STATIONS`, `quantized`, `station_truth`, …)\n",
    "are produced with identical names and shapes; only the geography has\n",
    "been replaced.\n",
    "\n",
    "Station codes and coordinates\n",
    "-----------------------------\n",
    "KEF  : Keflavík (IS)            64.020 N, −22.567 E\n",
    "VAL  : Valentia Observatory (IE)51.930 N, −10.250 E\n",
    "LER  : Lerwick (UK)             60.150 N,  −1.130 E\n",
    "HER  : Herstmonceux (UK)        50.867 N,   0.336 E\n",
    "GIB  : Gibraltar (GI)           36.150 N,  −5.350 E\n",
    "AKR  : Akrotiri (CY)            34.588 N,  32.986 E\n",
    "CAM  : Camborne (UK)            50.217 N,  −5.317 E\n",
    "WAT  : Wattisham (UK)           52.127 N,   0.956 E\n",
    "CAB  : Cabauw (NL)              51.970 N,   4.930 E\n",
    "PAY  : Payerne (CH)             46.820 N,   6.950 E\n",
    "TAR  : Tõravere (EE)            58.263 N,  26.464 E\n",
    "\"\"\"\n",
    "# ------------------------------------------------------------\n",
    "import math, random, numpy as np, pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "# 0)  USER KNOBS ------------------------------------------------------------\n",
    "SEED          = 424242\n",
    "duration_min  = 5                 # storm length (min)\n",
    "scenario      = 'far'             # 'near' | 'medium' | 'far'\n",
    "DIFFICULTY    = 9                 # 1 (easy) … 9 (very hard)\n",
    "FS            = 109_375           # Hz  – keep for pipeline\n",
    "BITS, VREF    = 14, 1.0\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# 1)  STATION GEOGRAPHY (now 11 fixed sites) -------------------------------\n",
    "stations = {\n",
    "    'KEF': dict(lat=64.020, lon=-22.567),  # Keflavík\n",
    "    'VAL': dict(lat=51.930, lon=-10.250),  # Valentia Observatory\n",
    "    'LER': dict(lat=60.150, lon= -1.130),  # Lerwick\n",
    "    'HER': dict(lat=50.867, lon=  0.336),  # Herstmonceux\n",
    "    'GIB': dict(lat=36.150, lon= -5.350),  # Gibraltar\n",
    "    'AKR': dict(lat=34.588, lon= 32.986),  # Akrotiri\n",
    "    'CAM': dict(lat=50.217, lon= -5.317),  # Camborne\n",
    "    'WAT': dict(lat=52.127, lon=  0.956),  # Wattisham\n",
    "    'CAB': dict(lat=51.970, lon=  4.930),  # Cabauw\n",
    "    'PAY': dict(lat=46.820, lon=  6.950),  # Payerne\n",
    "    'TAR': dict(lat=58.263, lon= 26.464),  # Tõravere\n",
    "}\n",
    "station_order = list(stations.keys())      # fixed order for pipeline\n",
    "STN      = station_order\n",
    "STATIONS = station_order                   # alias used by other notebooks\n",
    "N_STN    = len(STN)                        # = 11\n",
    "\n",
    "# 2)  Helpers ---------------------------------------------------------------\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R=6371.0\n",
    "    φ1,φ2 = map(math.radians, (lat1,lat2))\n",
    "    dφ    = math.radians(lat2-lat1)\n",
    "    dλ    = math.radians(lon2-lon1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "# 3)  Timeline --------------------------------------------------------------\n",
    "pre_sec   = rng.uniform(5,30)\n",
    "storm_sec = duration_min*60\n",
    "total_sec = pre_sec + storm_sec\n",
    "N         = int(total_sec*FS)\n",
    "\n",
    "quantized      = {nm: np.zeros(N, np.int16)   for nm in STN}\n",
    "station_truth  = {nm: np.zeros(N//1024, bool) for nm in STN}\n",
    "events, stroke_records, burst_book, stroke_samples = [], [], [], []\n",
    "\n",
    "# 4)  Tier flags ------------------------------------------------------------\n",
    "flags = dict(\n",
    "    ic_mix          = DIFFICULTY>=2,\n",
    "    multipath       = DIFFICULTY>=3,\n",
    "    coloured_noise  = DIFFICULTY>=4,\n",
    "    rfi             = DIFFICULTY>=5,\n",
    "    sprite_ring     = DIFFICULTY>=5,\n",
    "    false_transient = DIFFICULTY>=5,\n",
    "    clipping        = DIFFICULTY>=5,\n",
    "    multi_cell      = DIFFICULTY>=6,\n",
    "    skywave         = DIFFICULTY>=7,\n",
    "    bg_sferics      = DIFFICULTY>=7,\n",
    "    clock_skew      = DIFFICULTY>=8,\n",
    "    gain_drift      = DIFFICULTY>=8,\n",
    "    dropouts        = DIFFICULTY>=8,\n",
    "    low_snr         = DIFFICULTY>=9,\n",
    "    burst_div       = DIFFICULTY>=9,\n",
    ")\n",
    "\n",
    "# 5)  Storm‑cell geometry ----------------------------------------------------\n",
    "lat0 = np.mean([s['lat'] for s in stations.values()])\n",
    "lon0 = np.mean([s['lon'] for s in stations.values()])\n",
    "R0   = dict(near=100, medium=400, far=900)[scenario]\n",
    "\n",
    "n_cells = 1 if not flags['multi_cell'] else rng.integers(2,5)\n",
    "cells=[]\n",
    "for _ in range(n_cells):\n",
    "    θ   = rng.uniform(0,2*math.pi)\n",
    "    rad = R0*rng.uniform(0.3,1.0)\n",
    "    cells.append(dict(\n",
    "        lat   = lat0 + (rad/111)*math.cos(θ),\n",
    "        lon   = lon0 + (rad/111)*math.sin(θ)/math.cos(math.radians(lat0)),\n",
    "        drift = rng.uniform(-0.20,0.20,2)          # deg h⁻¹\n",
    "    ))\n",
    "\n",
    "# 6)  Flash & stroke generation ---------------------------------------------\n",
    "wave_len = int(0.04*FS)\n",
    "tv       = np.arange(wave_len)/FS\n",
    "rfi_freqs= [14400,20100,30300]\n",
    "\n",
    "eid, t = 0, pre_sec\n",
    "while True:\n",
    "    cell = cells[rng.integers(len(cells))]\n",
    "    c_age= t-pre_sec\n",
    "    c_lat= cell['lat'] + cell['drift'][0]*c_age/3600\n",
    "    c_lon= cell['lon'] + cell['drift'][1]*c_age/3600\n",
    "    t   += rng.lognormal(3,1)*(0.4 if flags['multi_cell'] else 1)\n",
    "    if t>=total_sec: break\n",
    "    eid += 1\n",
    "    # flash location\n",
    "    d = rng.uniform(0,R0)\n",
    "    φ = rng.uniform(0,2*math.pi)\n",
    "    f_lat = c_lat + (d/111)*math.cos(φ)\n",
    "    f_lon = c_lon + (d/111)*math.sin(φ)/math.cos(math.radians(lat0))\n",
    "    f_type= 'IC' if (flags['ic_mix'] and rng.random()<0.3) else 'CG'\n",
    "    n_str = rng.integers(1, 4 if f_type=='IC' else 7)\n",
    "    amp0  = 0.35 if f_type=='IC' else 1.0\n",
    "    s_times = sorted(t + rng.uniform(0,0.06,size=n_str))\n",
    "    events.append(dict(id=eid,flash_type=f_type,lat=f_lat,lon=f_lon,\n",
    "                       stroke_times=s_times))\n",
    "    # build bursts\n",
    "    for si, t0 in enumerate(s_times):\n",
    "        for nm in STN:\n",
    "            dist = hav(f_lat,f_lon, stations[nm]['lat'],stations[nm]['lon'])\n",
    "            idx  = int((t0 + dist/300_000 + rng.uniform(-50,50)/1e6)*FS)\n",
    "            if idx>=N: continue\n",
    "            # waveform\n",
    "            if flags['burst_div'] and rng.random()<0.15:\n",
    "                tau=.0008; burst = (tv/tau)*np.exp(1-tv/tau)\n",
    "            else:\n",
    "                f0=rng.uniform(2500,9500); tau=rng.uniform(0.0003,0.0025)\n",
    "                burst = np.sin(2*math.pi*f0*tv)*np.exp(-tv/tau)\n",
    "            amp = amp0*rng.uniform(2,5)/(1+dist/40)\n",
    "            if flags['low_snr']: amp*=0.4\n",
    "            burst *= amp\n",
    "            # multipath\n",
    "            if flags['multipath'] and dist>60:\n",
    "                dly=int(rng.uniform(0.001,0.0035)*FS)\n",
    "                if dly<wave_len: burst[dly:]+=0.35*burst[:-dly]\n",
    "            # sprite ringers\n",
    "            if flags['sprite_ring'] and rng.random()<0.04:\n",
    "                dly=int(rng.uniform(0.008,0.018)*FS)\n",
    "                if dly<wave_len: burst[dly:]+=0.25*burst[:-dly]\n",
    "            # sky‑wave attenuation\n",
    "            if flags['skywave'] and dist>600:\n",
    "                f=np.fft.rfftfreq(wave_len,1/FS)\n",
    "                H=np.exp(-0.00025*dist*((f/6e3)**2))\n",
    "                burst=np.fft.irfft(np.fft.rfft(burst)*H,n=wave_len)\n",
    "            burst_book.append((nm,idx,burst.astype(np.float32)))\n",
    "            station_truth[nm][idx//1024]=True\n",
    "            stroke_records.append(dict(event_id=eid,stroke_i=si,station=nm,\n",
    "                                       flash_type=f_type,lat=f_lat,lon=f_lon,\n",
    "                                       true_time_s=t0,sample_idx=idx,\n",
    "                                       window_idx=idx//1024))\n",
    "            if nm==STN[0]: stroke_samples.append(idx)\n",
    "\n",
    "# 7)  Noise profiles ---------------------------------------------------------\n",
    "noise_cfg={}\n",
    "for nm in STN:\n",
    "    base_white=rng.uniform(0.010,0.017)\n",
    "    tones=[]\n",
    "    if flags['rfi'] and rng.random()<0.6:\n",
    "        tones=[(rng.choice(rfi_freqs), rng.uniform(0.001,0.004))]\n",
    "    noise_cfg[nm]=dict(\n",
    "        w = base_white if not flags['coloured_noise'] else base_white*rng.uniform(1,1.8),\n",
    "        h = 0.01 if not flags['coloured_noise'] else rng.uniform(0.006,0.020),\n",
    "        tones = tones,\n",
    "        gain_drift = rng.uniform(-0.05,0.05) if flags['gain_drift'] else 0.0,\n",
    "        skew = rng.uniform(-20e-6,20e-6) if flags['clock_skew'] else 0.0\n",
    "    )\n",
    "\n",
    "# 8)  ADC synthesis loop -----------------------------------------------------\n",
    "b,a  = butter(4, 45000/(FS/2),'low')\n",
    "chunk= int(20*FS)\n",
    "tv40 = np.arange(wave_len)/FS   # for false‑transient reuse\n",
    "\n",
    "for nm in STN:\n",
    "    bur = [b for b in burst_book if b[0]==nm]\n",
    "    cfg = noise_cfg[nm]\n",
    "    drop = np.ones(N,bool)\n",
    "    if flags['dropouts'] and rng.random()<0.1:\n",
    "        for _ in range(rng.integers(1,3)):\n",
    "            s=rng.integers(int(pre_sec*FS),N-int(0.4*FS))\n",
    "            drop[s:s+int(0.4*FS)]=False\n",
    "    for s0 in range(0,N,chunk):\n",
    "        e0=min(N,s0+chunk); L=e0-s0\n",
    "        t=np.arange(s0,e0)/FS\n",
    "        seg = cfg['w']*rng.standard_normal(L) + cfg['h']*np.sin(2*math.pi*50*t)\n",
    "        for f,amp in cfg['tones']:\n",
    "            seg += amp*np.sin(2*math.pi*f*t + rng.uniform(0,2*math.pi))\n",
    "        # gain drift\n",
    "        seg *= 1 + cfg['gain_drift']*(t-pre_sec)/(storm_sec+1e-9)\n",
    "        # add bursts\n",
    "        for (_,i0,br) in bur:\n",
    "            if s0<=i0<e0:\n",
    "                off=i0-s0; l=min(len(br),L-off)\n",
    "                seg[off:off+l]+=br[:l]\n",
    "        # false transient\n",
    "        if flags['false_transient'] and rng.random()<0.002:\n",
    "            idx=rng.integers(0,L-wave_len)\n",
    "            seg[idx:idx+wave_len]+=0.7*np.sin(2*math.pi*5800*tv40)*np.exp(-tv40/0.0009)\n",
    "        # filtering & clipping\n",
    "        seg = filtfilt(b,a,seg)\n",
    "        if flags['clipping']: seg=np.clip(seg,-0.9*VREF,0.9*VREF)\n",
    "        full=2**(BITS-1)-1\n",
    "        adc = np.clip(np.round(seg/VREF*full), -full, full).astype(np.int16)\n",
    "        quantized[nm][s0:e0][drop[s0:e0]] = adc[drop[s0:e0]]\n",
    "\n",
    "# 9)  DataFrames -------------------------------------------------------------\n",
    "df_wave = pd.DataFrame({'time_s':np.arange(N)/FS})\n",
    "for nm in STN: df_wave[nm]=quantized[nm]\n",
    "df_labels = pd.DataFrame(stroke_records)\n",
    "\n",
    "def df_to_quantized(df):      return {nm:df[nm].values.astype(np.int16) for nm in STN}\n",
    "def df_labels_to_events(df):\n",
    "    out=[];      grp=df.groupby('event_id')\n",
    "    for eid,g in grp:\n",
    "        out.append(dict(id=eid,flash_type=g.flash_type.iloc[0],\n",
    "                        lat=g.lat.iloc[0],lon=g.lon.iloc[0],\n",
    "                        stroke_times=sorted(g.true_time_s.unique())))\n",
    "    return out\n",
    "\n",
    "# 10)  Summary --------------------------------------------------------------\n",
    "print(f\"Tier‑{DIFFICULTY} | scenario={scenario} | cells={n_cells}\")\n",
    "print(f\"Flashes {len(events):3d} | strokes {len(df_labels)//N_STN:3d} \"\n",
    "      f\"| samples {N:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell X – Extended‑station EIF detector (16‑feature set)\n",
    "# ============================================================\n",
    "import os, warnings, zlib, math, numpy as np, pywt\n",
    "from math import sqrt\n",
    "from scipy.signal import hilbert\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from isotree import IsolationForest          # extended EIF\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order          # 11‑station list, alias STATIONS identical\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "GRID_CONT = np.linspace(0.003, 0.007, 5)  # grid‑search contaminations\n",
    "EXTREME_Q = 99.95                         # back‑stop percentile for “must flag”\n",
    "NTREES    = 200                           # EIF trees\n",
    "\n",
    "# ── Helper feature functions ────────────────────────────────\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c = len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean() / (env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "\n",
    "def crest(seg):\n",
    "    rms = sqrt((seg.astype(float)**2).mean()) + 1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "\n",
    "comp = lambda seg: len(zlib.compress(seg.tobytes(), 6)) / len(seg.tobytes())\n",
    "\n",
    "def spec_stats(seg):\n",
    "    P = np.abs(np.fft.rfft(seg))**2\n",
    "    P /= P.sum() + 1e-12\n",
    "    f = np.fft.rfftfreq(len(seg), 1/FS)\n",
    "    cent = (f*P).sum()\n",
    "    bw   = sqrt(((f-cent)**2 * P).sum())\n",
    "    ent  = -(P*np.log2(P+1e-12)).sum()\n",
    "    return cent, bw, ent\n",
    "\n",
    "# ── Feature extraction (16‑D) ───────────────────────────────\n",
    "n_win  = min(((len(quantized[n]) - WIN)//HOP) + 1 for n in STN)\n",
    "feat_d = 16\n",
    "Xst    = {nm: np.empty((n_win, feat_d), float) for nm in STN}\n",
    "\n",
    "print(\"▶ Extracting 16‑dimensional features …\")\n",
    "for nm in STN:\n",
    "    sig  = quantized[nm]\n",
    "    env  = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN//2 + 1\n",
    "    b25, b50, b75 = [int(Nfft*r) for r in (0.25, 0.50, 0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm}\", leave=False):\n",
    "        s       = w*HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        # 1–3  envelope stats\n",
    "        pk     = env_seg.max()\n",
    "        md     = np.median(env_seg)\n",
    "        ratio  = pk/(md+1e-9)\n",
    "\n",
    "        # 4–5  energy & STA/LTA\n",
    "        energy = (seg_f**2).sum()\n",
    "        stl    = sta_lta(env_seg)\n",
    "\n",
    "        # 6–7  crest factors\n",
    "        cf_s   = crest(seg_i16[len(seg_i16)//2-WIN//16 : len(seg_i16)//2+WIN//16])\n",
    "        cf_g   = crest(seg_i16)\n",
    "\n",
    "        # 8–11 FFT band‑power fractions\n",
    "        P      = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP   = P.sum() + 1e-9\n",
    "        f1,f2,f3 = b25,b50,b75\n",
    "        b1,b2,b3,b4 = (P[:f1].sum()/totP, P[f1:f2].sum()/totP,\n",
    "                       P[f2:f3].sum()/totP, P[f3:].sum()/totP)\n",
    "\n",
    "        # 12   wavelet high‑band energy frac\n",
    "        hi = pywt.wavedec(seg_f, 'db4', level=3)[1]\n",
    "        lo = pywt.wavedec(seg_f, 'db4', level=3)[-1]\n",
    "        wave_hi = (hi**2).sum() / ((hi**2).sum() + (lo**2).sum() + 1e-9)\n",
    "\n",
    "        # 13   compression ratio\n",
    "        comp_r = comp(seg_i16)\n",
    "\n",
    "        # 14–16 spectral centroid / BW / entropy\n",
    "        cent, bw, ent = spec_stats(seg_f)\n",
    "\n",
    "        Xst[nm][w] = [pk, md, ratio, energy, stl,\n",
    "                      cf_s, cf_g, b1, b2, b3, b4,\n",
    "                      wave_hi, comp_r, cent, bw, ent]\n",
    "\n",
    "# ── EIF fitting & window‑level flagging ─────────────────────\n",
    "eif_score, hot, best_cont = {}, {}, {}\n",
    "for nm in STN:\n",
    "    X  = RobustScaler().fit_transform(Xst[nm])      # robust z‑score\n",
    "    iso = IsolationForest(\n",
    "            ntrees      = NTREES,\n",
    "            sample_size = 'auto',\n",
    "            ndim        = X.shape[1]-1,\n",
    "            prob_pick_avg_gain   = 0,\n",
    "            prob_pick_pooled_gain= 0,\n",
    "            nthreads    = max(os.cpu_count()-1, 1),\n",
    "            random_seed = 42\n",
    "          ).fit(X)\n",
    "\n",
    "    # depth (low = anomalous) or score (high = anomalous)\n",
    "    try:\n",
    "        score = iso.predict(X, type=\"avg_depth\");  depth=True\n",
    "    except TypeError:\n",
    "        try:  score = iso.predict(X, output_type=\"avg_depth\"); depth=True\n",
    "        except TypeError:\n",
    "            score = iso.predict(X); depth=False\n",
    "    if not depth: score = -score                  # invert if necessary\n",
    "\n",
    "    eif_score[nm] = score\n",
    "    # grid search contamination until at least 0.1 % windows flagged\n",
    "    for c in GRID_CONT:\n",
    "        mask = score < np.quantile(score, c)\n",
    "        if mask.sum() >= 0.001*n_win:\n",
    "            best_cont[nm] = c; hot[nm] = mask; break\n",
    "    else:\n",
    "        best_cont[nm] = GRID_CONT[-1]\n",
    "        hot[nm]       = score < np.quantile(score, GRID_CONT[-1])\n",
    "\n",
    "    print(f\"{nm}: cont* = {best_cont[nm]:.3%}  flagged = {hot[nm].sum():,}\")\n",
    "\n",
    "# ── Stroke‑level coincidence logic ─────────────────────────\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0, w-TOL_WIN) : min(n_win, w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "# backup: mark strokes with extreme depth on any station\n",
    "thr_ext = {nm: np.percentile(eif_score[nm], 100-EXTREME_Q) for nm in STN}\n",
    "for j, i0 in enumerate(stroke_samples):\n",
    "    if counts[j] == 0:\n",
    "        w = i0 // HOP\n",
    "        if any(eif_score[nm][w] < thr_ext[nm] for nm in STN):\n",
    "            counts[j] = 1          # at least “suspect”\n",
    "\n",
    "# ── Metrics ─────────────────────────────────────────────────\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k, v in sorted(dict(zip(*np.unique(counts, return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn   TP   FN  Recall\")\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(n_win-1, i0//HOP))] for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False,True]).ravel()\n",
    "    R = tp / (tp + fn) if tp + fn else 0.0\n",
    "    print(f\"{nm:>3} {tp:4d} {fn:4d}  {R:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False,True]).ravel()\n",
    "P, R, F = precision_recall_fscore_support(truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke‑wise:\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}   P={P:.3f}  R={R:.3f}  F1={F:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 8” – Extended‑station EIF (16‑feature set)\n",
    "#  --------------------------------------------------------------------\n",
    "#  • Sections 1‑5 below are *exactly* your inline pipeline.  Only comment\n",
    "#    banners (⚠) were inserted to flag heuristic or high‑runtime spots.\n",
    "#  • Section 6 appends a call to the strict, burst‑aware\n",
    "#    `evaluate_windowed_model`, yielding the same professional metrics and\n",
    "#    timeline/ waveform visual you now have for all other models.\n",
    "##############################################################################\n",
    "\n",
    "import os, warnings, zlib, math, numpy as np, pywt\n",
    "from math import sqrt\n",
    "from scipy.signal import hilbert\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from isotree import IsolationForest\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1          # ⚠ inline heuristic; strict eval below ignores\n",
    "MIN_STN   = 2\n",
    "GRID_CONT = np.linspace(0.003, 0.007, 5)\n",
    "EXTREME_Q = 99.95\n",
    "NTREES    = 200\n",
    "BURST_LEN = int(0.04*FS)     # 40 ms burst for strict evaluator\n",
    "\n",
    "# ── Helper feature functions ────────────────────────────────\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c=len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean()/(env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "def crest(seg):\n",
    "    rms=sqrt((seg.astype(float)**2).mean())+1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "comp=lambda seg: len(zlib.compress(seg.tobytes(),6))/len(seg.tobytes())\n",
    "def spec_stats(seg):\n",
    "    P=np.abs(np.fft.rfft(seg))**2; P/=P.sum()+1e-12\n",
    "    f=np.fft.rfftfreq(len(seg),1/FS)\n",
    "    cent=(f*P).sum(); bw=sqrt(((f-cent)**2*P).sum())\n",
    "    ent=-(P*np.log2(P+1e-12)).sum()\n",
    "    return cent,bw,ent\n",
    "\n",
    "# ── Feature extraction (16‑D) ───────────────────────────────\n",
    "n_win=min(((len(quantized[n])-WIN)//HOP)+1 for n in STN)\n",
    "feat_d=16\n",
    "Xst={nm: np.empty((n_win,feat_d),float) for nm in STN}\n",
    "\n",
    "print(\"▶ Extracting 16‑dimensional features …\")\n",
    "for nm in STN:\n",
    "    sig=quantized[nm]; env=np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft=WIN//2+1; b25,b50,b75=[int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm}\", leave=False):\n",
    "        s=w*HOP; seg_i16=sig[s:s+WIN]; seg_f=seg_i16.astype(float); env_seg=env[s:s+WIN]\n",
    "        pk,md=env_seg.max(),np.median(env_seg); ratio=pk/(md+1e-9)\n",
    "        energy=(seg_f**2).sum(); stl=sta_lta(env_seg)\n",
    "        cf_s=crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16])\n",
    "        cf_g=crest(seg_i16)\n",
    "        P=np.abs(np.fft.rfft(seg_f))**2; totP=P.sum()+1e-9\n",
    "        f1,f2,f3=b25,b50,b75\n",
    "        b1,b2,b3,b4=(P[:f1].sum()/totP,P[f1:f2].sum()/totP,\n",
    "                     P[f2:f3].sum()/totP,P[f3:].sum()/totP)\n",
    "        hi=pywt.wavedec(seg_f,'db4',level=3)[1]; lo=pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi=(hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r=comp(seg_i16); cent,bw,ent=spec_stats(seg_f)\n",
    "        Xst[nm][w]=[pk,md,ratio,energy,stl,cf_s,cf_g,b1,b2,b3,b4,\n",
    "                    wave_hi,comp_r,cent,bw,ent]\n",
    "\n",
    "# ── EIF fitting & window‑level flagging ─────────────────────\n",
    "eif_score, hot, best_cont = {}, {}, {}\n",
    "for nm in STN:\n",
    "    X=RobustScaler().fit_transform(Xst[nm])\n",
    "    iso=IsolationForest(ntrees=NTREES,sample_size='auto',ndim=X.shape[1]-1,\n",
    "                        prob_pick_avg_gain=0,prob_pick_pooled_gain=0,\n",
    "                        nthreads=max(os.cpu_count()-1,1),random_seed=42).fit(X)\n",
    "    try:\n",
    "        score=iso.predict(X,type=\"avg_depth\"); depth=True\n",
    "    except TypeError:\n",
    "        try: score=iso.predict(X,output_type=\"avg_depth\"); depth=True\n",
    "        except TypeError:\n",
    "            score=iso.predict(X); depth=False\n",
    "    if not depth: score=-score\n",
    "    eif_score[nm]=score\n",
    "    for c in GRID_CONT:\n",
    "        mask=score<np.quantile(score,c)\n",
    "        if mask.sum()>=0.001*n_win:\n",
    "            best_cont[nm]=c; hot[nm]=mask; break\n",
    "    else:\n",
    "        best_cont[nm]=GRID_CONT[-1]; hot[nm]=score<np.quantile(score,GRID_CONT[-1])\n",
    "    print(f\"{nm}: cont*={best_cont[nm]:.3%}, flagged={hot[nm].sum():,}\")\n",
    "\n",
    "# ── Stroke coincidence logic (INLINE heuristic) ─────────────\n",
    "counts=np.zeros(len(stroke_samples),int)\n",
    "for nm in STN:\n",
    "    m=hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w=i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "thr_ext={nm: np.percentile(eif_score[nm],100-EXTREME_Q) for nm in STN}\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    if counts[j]==0:\n",
    "        w=i0//HOP\n",
    "        if any(eif_score[nm][w]<thr_ext[nm] for nm in STN):\n",
    "            counts[j]=1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke (INLINE diagnostic):\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "truth=np.ones(len(stroke_samples),bool)\n",
    "print(\"\\nStation‑level stroke detection (INLINE diagnostic):\")\n",
    "print(\"stn   TP   FN  Recall\")\n",
    "for nm in STN:\n",
    "    pred=np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    print(f\"{nm:>3} {tp:4d} {fn:4d}  {R:6.3f}\")\n",
    "\n",
    "net_pred=counts>=MIN_STN\n",
    "tn,fp,fn,tp=confusion_matrix(truth,net_pred,labels=[False,True]).ravel()\n",
    "P,R,F=precision_recall_fscore_support(truth,net_pred,average='binary',zero_division=0)[:3]\n",
    "print(f\"\\nNetwork (INLINE diagnostic, ≥{MIN_STN} stn):\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}   P={P:.3f}  R={R:.3f}  F1={F:.3f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation via evaluate_windowed_model\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "hot_masks = hot   # mapping {station: bool‑array}\n",
    "\n",
    "station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "    hot=hot_masks,\n",
    "    stroke_records=stroke_records,\n",
    "    quantized=quantized,\n",
    "    station_order=STN,\n",
    "    win=WIN,\n",
    "    hop=HOP,\n",
    "    burst_len=BURST_LEN,\n",
    "    min_stn=MIN_STN,\n",
    "    tol_win=0,          # strict: no prediction dilation\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "for nm,m in station_metrics.items():\n",
    "    print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell Y – NCD *variants* for lightning‑window anomaly scoring\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Compares four byte‑encodings for the Normalised Compression Distance (NCD)\n",
    "detector over 1 024‑sample windows:\n",
    "\n",
    "    bits  – sign‑bit of first difference  (former default, phase‑robust)\n",
    "    raw   – raw int16 bytes               (energy / amplitude heavy)\n",
    "    norm  – z‑score window, rescaled int16(DC & gain normalised)\n",
    "    tanh  – soft‑clip int16 via tanh      (suppresses extreme peaks)\n",
    "\n",
    "Every variant:\n",
    "\n",
    "    • builds a station‑specific baseline (median of lowest‑entropy 5 %)\n",
    "    • computes NCD to that baseline for every window\n",
    "    • derives a threshold = min(98.5 th percentile, μ+3.5 σ)\n",
    "    • flags hot windows, tallies stroke coincidences, prints metrics.\n",
    "\n",
    "Objects created\n",
    "---------------\n",
    "ncd_meta[variant][station]   → dict with ncd vector, threshold, etc.\n",
    "variant_hits[variant]        → bool matrix [station, stroke] used for metrics.\n",
    "\n",
    "Down‑stream variable names (`WIN`, `HOP`, `station_order`, `events`,\n",
    "`quantized`, etc.) are reused unmodified.\n",
    "\"\"\"\n",
    "# ── Imports ─────────────────────────────────────────────────\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools      import lru_cache\n",
    "from collections    import Counter, defaultdict\n",
    "from scipy.stats    import describe\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ── Global parameters (unchanged) ───────────────────────────\n",
    "WIN, HOP   = 1024, 512\n",
    "BASE_PCT   = 5\n",
    "PCT_THR    = 99.9\n",
    "Z_SIGMA    = 3.5\n",
    "MIN_STN    = 2\n",
    "STN        = station_order          # alias STATIONS\n",
    "FS         = float(FS)              # from simulator cell\n",
    "\n",
    "# ── Helper: haversine (needed for stroke → first‑arrival) ───\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = map(np.radians, (lat1, lat2))\n",
    "    dφ = φ2 - φ1\n",
    "    dλ = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# ── Window view utility (stride‑trick) ──────────────────────\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W)//H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# ── Compression size cache (bzip2, level 9) ─────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:\n",
    "    return len(bz2.compress(b, 9))\n",
    "\n",
    "# ── Encoders (4 variants) ───────────────────────────────────\n",
    "def enc_bits(arr):\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "\n",
    "def enc_raw(arr):\n",
    "    return arr.astype(np.int16).tobytes()\n",
    "\n",
    "def enc_norm(arr):\n",
    "    a = arr.astype(np.float32)\n",
    "    a = (a - a.mean()) / (a.std(ddof=0) + 1e-9)\n",
    "    a = np.clip(a*32767, -32767, 32767).astype(np.int16)\n",
    "    return a.tobytes()\n",
    "\n",
    "def enc_tanh(arr):\n",
    "    a = np.tanh(arr.astype(np.float32) / 16384.0) * 32767\n",
    "    return a.astype(np.int16).tobytes()\n",
    "\n",
    "ENCODERS = dict(bits=enc_bits, raw=enc_raw, norm=enc_norm, tanh=enc_tanh)\n",
    "\n",
    "# ── Pre‑compute common window count ─────────────────────────\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP) + 1 for n in STN)\n",
    "print(f\"\\nAnalysing {n_win:,} windows  ×  {len(STN)} stations  ×  {len(ENCODERS)} encodings\\n\")\n",
    "\n",
    "# ── Main loop: build NCD vectors for every variant & station ─\n",
    "ncd_meta = defaultdict(dict)   # two‑level dict\n",
    "\n",
    "for enc_name, enc_fun in ENCODERS.items():\n",
    "    print(f\"\\n=== Variant: {enc_name} ===\")\n",
    "    for nm in STN:\n",
    "        sig  = quantized[nm]\n",
    "        wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "        # pass 1 – pre‑compute compressed size of each window\n",
    "        comp_sz = np.empty(n_win, np.uint32)\n",
    "        for i in tq.trange(n_win, desc=f\"{nm} size\", leave=False):\n",
    "            comp_sz[i] = c_size(enc_fun(wmat[i]))\n",
    "\n",
    "        # choose baseline window = median of lowest BASE_PCT %\n",
    "        k          = max(1, int(BASE_PCT/100 * n_win))\n",
    "        low_idx    = np.argpartition(comp_sz, k)[:k]\n",
    "        base_idx   = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "        base_bytes = enc_fun(wmat[base_idx])\n",
    "        Cb         = c_size(base_bytes)\n",
    "\n",
    "        # pass 2 – NCD to baseline for every window\n",
    "        ncd_vec = np.empty(n_win, float)\n",
    "        for i in tq.trange(n_win, desc=f\"{nm} NCD\", leave=False):\n",
    "            wb = enc_fun(wmat[i])\n",
    "            ncd_vec[i] = (c_size(wb + base_bytes) - min(comp_sz[i], Cb)) / max(comp_sz[i], Cb)\n",
    "\n",
    "        # derive adaptive threshold\n",
    "        stats   = describe(ncd_vec)\n",
    "        thr_pct = np.percentile(ncd_vec, PCT_THR)\n",
    "        thr_z   = stats.mean + Z_SIGMA*np.sqrt(stats.variance)\n",
    "        thr     = min(thr_pct, thr_z)\n",
    "        hot     = ncd_vec > thr\n",
    "\n",
    "        ncd_meta[enc_name][nm] = dict(\n",
    "            ncd = ncd_vec,\n",
    "            hot = hot,\n",
    "            thr = thr,\n",
    "            desc= stats\n",
    "        )\n",
    "        print(f\" {nm}: hot={hot.sum():5d}  thr={thr:.4f}\")\n",
    "\n",
    "# ── Build canonical stroke index list (earliest arrival any stn) ───────────\n",
    "stroke_idx = np.array([\n",
    "    min(int((t0 + hav(ev['lat'], ev['lon'],\n",
    "                      stations[n]['lat'], stations[n]['lon']) / 300_000) * FS)\n",
    "        for n in STN)\n",
    "    for ev in events for t0 in ev['stroke_times']\n",
    "])\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "# ── Variant‑specific stroke coincidence matrices ───────────────────────────\n",
    "variant_hits = {}\n",
    "for enc_name in ENCODERS:\n",
    "    hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "    for s, nm in enumerate(STN):\n",
    "        hot = ncd_meta[enc_name][nm]['hot']\n",
    "        for j, i0 in enumerate(stroke_idx):\n",
    "            w = i0 // HOP\n",
    "            hits[s, j] = hot[max(0, w-1):min(len(hot), w+2)].any()\n",
    "    variant_hits[enc_name] = hits\n",
    "\n",
    "# ── Reporting helper ────────────────────────────────────────\n",
    "def report_variant(name, hits):\n",
    "    cnt = hits.sum(axis=0)\n",
    "    print(f\"\\n### {name} encoding ###\")\n",
    "    print(\"Stations ≥thr per stroke:\")\n",
    "    for k, v in sorted(Counter(cnt).items()):\n",
    "        print(f\"  {k:2d} → {v}\")\n",
    "    net_pred = cnt >= MIN_STN\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False, True]).ravel()\n",
    "    P = precision_score(truth, net_pred, zero_division=0)\n",
    "    R = recall_score   (truth, net_pred, zero_division=0)\n",
    "    F = f1_score       (truth, net_pred, zero_division=0)\n",
    "    print(f\"Network: TP={tp} FP={fp} FN={fn} TN={tn} | P={P:.3f} R={R:.3f} F1={F:.3f}\")\n",
    "\n",
    "    print(\"stn  TP  FP  FN    P     R    F1\")\n",
    "    for s, nm in enumerate(STN):\n",
    "        pred = hits[s]\n",
    "        tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False, True]).ravel()\n",
    "        P = precision_score(truth, pred, zero_division=0)\n",
    "        R = recall_score   (truth, pred, zero_division=0)\n",
    "        F = f1_score       (truth, pred, zero_division=0)\n",
    "        print(f\"{nm:>3} {tp:3d} {fp:3d} {fn:3d}  {P:5.3f} {R:5.3f} {F:5.3f}\")\n",
    "\n",
    "# ── Print summary for every encoding variant ────────────────\n",
    "for enc in ENCODERS:\n",
    "    report_variant(enc, variant_hits[enc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#  Lightning‑detection “model 9” – NCD (4 encoder variants) + strict scoring\n",
    "#  -------------------------------------------------------------------------\n",
    "#  • Sections 1‑5 below reproduce your existing multi‑variant NCD pipeline\n",
    "#    unchanged: feature encoding, NCD computation, and the quick inline\n",
    "#    stroke tables produced by `report_variant`.\n",
    "#  • Section 6 (new) loops over each encoder variant, builds the\n",
    "#    `{station: hot‑mask}` mapping, and feeds it to the strict,\n",
    "#    burst‑aware `evaluate_windowed_model` so you get rigorous station/window\n",
    "#    and stroke/network metrics plus the timeline+waveform UI.\n",
    "##############################################################################\n",
    "\n",
    "# ── Imports ─────────────────────────────────────────────────\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools      import lru_cache\n",
    "from collections    import Counter, defaultdict\n",
    "from scipy.stats    import describe\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ── Global parameters (unchanged) ───────────────────────────\n",
    "WIN, HOP   = 1024, 512\n",
    "BASE_PCT   = 5\n",
    "PCT_THR    = 98.5\n",
    "Z_SIGMA    = 3.5\n",
    "MIN_STN    = 2\n",
    "STN        = station_order\n",
    "FS         = float(FS)\n",
    "BURST_LEN  = int(0.04*FS)     # 40 ms burst for strict evaluator\n",
    "\n",
    "# ── Helper: haversine (needed for stroke → first‑arrival) ───\n",
    "def hav(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = map(np.radians, (lat1, lat2))\n",
    "    dφ = φ2 - φ1\n",
    "    dλ = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "# ── Window view utility (stride‑trick) ──────────────────────\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W)//H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# ── Compression size cache (bzip2, level 9) ─────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:\n",
    "    return len(bz2.compress(b, 9))\n",
    "\n",
    "# ── Encoders (4 variants) ───────────────────────────────────\n",
    "def enc_bits(arr):\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "def enc_raw(arr):   return arr.astype(np.int16).tobytes()\n",
    "def enc_norm(arr):\n",
    "    a = arr.astype(np.float32)\n",
    "    a = (a - a.mean()) / (a.std(ddof=0) + 1e-9)\n",
    "    a = np.clip(a*32767, -32767, 32767).astype(np.int16)\n",
    "    return a.tobytes()\n",
    "def enc_tanh(arr):\n",
    "    a = np.tanh(arr.astype(np.float32) / 16384.0) * 32767\n",
    "    return a.astype(np.int16).tobytes()\n",
    "\n",
    "ENCODERS = dict(bits=enc_bits, raw=enc_raw, norm=enc_norm, tanh=enc_tanh)\n",
    "\n",
    "# ── Pre‑compute common window count ─────────────────────────\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP) + 1 for n in STN)\n",
    "print(f\"\\nAnalysing {n_win:,} windows  ×  {len(STN)} stations  ×  {len(ENCODERS)} encodings\\n\")\n",
    "\n",
    "# ── Main loop: build NCD vectors for every variant & station ─\n",
    "ncd_meta = defaultdict(dict)   # two‑level dict\n",
    "\n",
    "for enc_name, enc_fun in ENCODERS.items():\n",
    "    print(f\"\\n=== Variant: {enc_name} ===\")\n",
    "    for nm in STN:\n",
    "        sig  = quantized[nm]\n",
    "        wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "        # pass 1 – pre‑compute compressed size of each window\n",
    "        comp_sz = np.empty(n_win, np.uint32)\n",
    "        for i in tq.trange(n_win, desc=f\"{nm} size\", leave=False):\n",
    "            comp_sz[i] = c_size(enc_fun(wmat[i]))\n",
    "\n",
    "        # choose baseline window = median of lowest BASE_PCT %\n",
    "        k          = max(1, int(BASE_PCT/100 * n_win))\n",
    "        low_idx    = np.argpartition(comp_sz, k)[:k]\n",
    "        base_idx   = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "        base_bytes = enc_fun(wmat[base_idx])\n",
    "        Cb         = c_size(base_bytes)\n",
    "\n",
    "        # pass 2 – NCD to baseline for every window\n",
    "        ncd_vec = np.empty(n_win, float)\n",
    "        for i in tq.trange(n_win, desc=f\"{nm} NCD\", leave=False):\n",
    "            wb = enc_fun(wmat[i])\n",
    "            ncd_vec[i] = (c_size(wb + base_bytes) - min(comp_sz[i], Cb)) / max(comp_sz[i], Cb)\n",
    "\n",
    "        # derive adaptive threshold\n",
    "        stats   = describe(ncd_vec)\n",
    "        thr_pct = np.percentile(ncd_vec, PCT_THR)\n",
    "        thr_z   = stats.mean + Z_SIGMA*np.sqrt(stats.variance)\n",
    "        thr     = min(thr_pct, thr_z)\n",
    "        hot     = ncd_vec > thr\n",
    "\n",
    "        ncd_meta[enc_name][nm] = dict(ncd=ncd_vec, hot=hot, thr=thr, desc=stats)\n",
    "        print(f\" {nm}: hot={hot.sum():5d}  thr={thr:.4f}\")\n",
    "\n",
    "# ── Build canonical stroke index list (earliest arrival) ───\n",
    "stroke_idx = np.array([\n",
    "    min(int((t0 + hav(ev['lat'], ev['lon'],\n",
    "                      stations[n]['lat'], stations[n]['lon']) / 300_000) * FS)\n",
    "        for n in STN)\n",
    "    for ev in events for t0 in ev['stroke_times']\n",
    "])\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "# ── Variant‑specific stroke coincidence matrices (INLINE) ──\n",
    "variant_hits = {}\n",
    "for enc_name in ENCODERS:\n",
    "    hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "    for s, nm in enumerate(STN):\n",
    "        hot = ncd_meta[enc_name][nm]['hot']\n",
    "        for j, i0 in enumerate(stroke_idx):\n",
    "            w = i0 // HOP\n",
    "            hits[s, j] = hot[max(0, w-1):min(len(hot), w+2)].any()   # ⚠ ±1 slack, FP invisible\n",
    "    variant_hits[enc_name] = hits\n",
    "\n",
    "def report_variant(name, hits):\n",
    "    cnt = hits.sum(axis=0)\n",
    "    print(f\"\\n### {name} encoding  (INLINE diagnostic) ###\")\n",
    "    print(\"Stations ≥thr per stroke:\")\n",
    "    for k, v in sorted(Counter(cnt).items()):\n",
    "        print(f\"  {k:2d} → {v}\")\n",
    "    net_pred = cnt >= MIN_STN\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False, True]).ravel()\n",
    "    P = precision_score(truth, net_pred, zero_division=0)\n",
    "    R = recall_score   (truth, net_pred, zero_division=0)\n",
    "    F = f1_score       (truth, net_pred, zero_division=0)\n",
    "    print(f\"Network: TP={tp} FP={fp} FN={fn} TN={tn} | P={P:.3f} R={R:.3f} F1={F:.3f}\")\n",
    "    print(\"stn  TP  FP  FN    P     R    F1\")\n",
    "    for s, nm in enumerate(STN):\n",
    "        pred = hits[s]\n",
    "        tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False, True]).ravel()\n",
    "        P = precision_score(truth, pred, zero_division=0)\n",
    "        R = recall_score   (truth, pred, zero_division=0)\n",
    "        F = f1_score       (truth, pred, zero_division=0)\n",
    "        print(f\"{nm:>3} {tp:3d} {fp:3d} {fn:3d}  {P:5.3f} {R:5.3f} {F:5.3f}\")\n",
    "\n",
    "for enc in ENCODERS:\n",
    "    report_variant(enc, variant_hits[enc])\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "#  6) Strict, burst‑aware evaluation for each encoder variant\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "for enc in ENCODERS:\n",
    "    print(f\"\\n================  STRICT EVALUATION: {enc}  ================\")\n",
    "    hot_masks = {nm: ncd_meta[enc][nm]['hot'] for nm in STN}\n",
    "    station_metrics, network_metrics, n_windows = evaluate_windowed_model(\n",
    "        hot          = hot_masks,\n",
    "        stroke_records = stroke_records,\n",
    "        quantized      = quantized,\n",
    "        station_order  = STN,\n",
    "        win            = WIN,\n",
    "        hop            = HOP,\n",
    "        burst_len      = BURST_LEN,\n",
    "        min_stn        = MIN_STN,\n",
    "        tol_win        = 0,\n",
    "        plot           = True\n",
    "    )\n",
    "    print(f\"\\n—— Station / window metrics  (n_windows = {n_windows:,}) ——\")\n",
    "    for nm, m in station_metrics.items():\n",
    "        print(f\"{nm}: TP={m['TP']:<4} FP={m['FP']:<5} FN={m['FN']:<4} \"\n",
    "              f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "    print(\"\\n—— Network / stroke metrics ——\")\n",
    "    print(network_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
