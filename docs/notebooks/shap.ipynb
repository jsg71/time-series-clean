{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Synthetic X‑ray dataset of a hollow metal sphere\n",
    "\n",
    "**What this cell is for.**  \n",
    "Synthesise a simple greyscale dataset that looks like X‑ray cross‑sections of a *hollow* metal sphere.  \n",
    "Each image contains a circular shell (intensity on the shell only), and a single **global density factor** ($y$) that scales the whole shell. This makes the problem deliberately **linear** so pixel‑wise linear models behave sensibly, while still being realistic enough for a CNN to learn.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Geometry and coordinate system\n",
    "\n",
    "Let the image be $(100\\times 100)$ pixels with centre $(c_x,c_y)=\\big(\\tfrac{H-1}{2},\\tfrac{W-1}{2}\\big)$.  \n",
    "Generate a slightly **elliptical** radius to allow mild shape variation:\n",
    "\n",
    "$$\n",
    "r_e(x,y)\\;=\\;\\sqrt{\\left(\\frac{x-c_x}{s_x}\\right)^2 + \\left(\\frac{y-c_y}{s_y}\\right)^2},\n",
    "\\qquad s_x,s_y \\approx 1.\n",
    "$$\n",
    "\n",
    "Two radii define the **hollow** interior and the **outer** boundary:\n",
    "\n",
    "- inner radius $R_0 \\approx 20$ px (interior is empty),\n",
    "- outer radius $R_1 \\approx 40$ px (outside is background).\n",
    "\n",
    "The shell mask is\n",
    "\n",
    "$$\n",
    "\\mathbf{1}_{\\text{shell}}(x,y)=\n",
    "\\begin{cases}\n",
    "1, & R_0 < r_e(x,y) \\le R_1,\\\\[2pt]\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Radial intensity profile inside the shell\n",
    "\n",
    "Within the shell create a smooth radial fall‑off using a shape parameter $\\gamma\\!\\in[0.9,1.1]$.\n",
    "\n",
    "Define the **normalised thickness coordinate**\n",
    "$$\n",
    "t(x,y)\\;=\\;\\frac{r_e(x,y)-R_0}{\\,R_1 - R_0\\,}\\in[0,1].\n",
    "$$\n",
    "\n",
    "The unshaded base intensity is\n",
    "$$\n",
    "I_0(x,y)\\;=\\;\\bigl(1-t(x,y)\\bigr)^{\\gamma}.\n",
    "$$\n",
    "\n",
    "Intuitively, $I_0$ is bright at the inner boundary and fades towards the outer boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Low‑frequency illumination (shading)\n",
    "\n",
    "Apply a gentle multiplicative illumination field to mimic sensor vignetting:\n",
    "\n",
    "$$\n",
    "S(x,y)\\;=\\;1+\\alpha \\, P_2(X,Y),\n",
    "\\qquad \\alpha\\in[0.05,0.12],\n",
    "$$\n",
    "\n",
    "where $(X,Y)$ are coordinates normalised by $R_1$ and\n",
    "$$\n",
    "P_2(X,Y)=c_0+c_1X+c_2Y+c_3XY+c_4X^2+c_5Y^2,\n",
    "$$\n",
    "with the polynomial re‑scaled to have zero mean and unit standard deviation so that $\\alpha$ directly controls the magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Image formation and label\n",
    "\n",
    "Sample a **global density factor** $y\\in[0.8,1.2]$ and form the image (before noise) as\n",
    "\n",
    "$$\n",
    "I(x,y)\\;=\\;y\\,I_0(x,y)\\,S(x,y).\n",
    "$$\n",
    "\n",
    "Then add a small constant **offset** and **Gaussian pixel noise** to simulate the sensor:\n",
    "\n",
    "$$\n",
    "I_{\\text{obs}}(x,y)\\;=\\;\\mathrm{clip}\\Big(I(x,y)+\\text{offset}+\\varepsilon(x,y),\\,0,1\\Big),\n",
    "\\qquad \\varepsilon\\sim\\mathcal{N}(0,\\sigma^2).\n",
    "$$\n",
    "\n",
    "The training **label** is $y$ with tiny measurement noise:\n",
    "$$\n",
    "y_{\\text{obs}} = y + \\eta,\\qquad \\eta\\sim\\mathcal{N}(0,\\sigma_y^2).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Why this cell matters\n",
    "\n",
    "- The mapping from pixels to $y$ is **linear** at the population level, so OLS/Ridge provide a solid baseline.  \n",
    "- Small geometric and illumination variations keep the task realistic and prevent trivial overfitting.  \n",
    "- Later cells (CNN + SHAP) use these arrays:\n",
    "\n",
    "```\n",
    "images       : (N, 100, 100) float32 in [0,1]\n",
    "y_true_all   : (N,)   noise‑free target values\n",
    "y_obs_all    : (N,)   observed targets (y_true + tiny noise)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Synthetic X‑ray dataset of a hollow metal sphere (100×100) — generation & visualisation\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "What this cell does:\n",
    "  • Generates N=1000 grayscale 100×100 images that look like X‑ray cross‑sections of a\n",
    "    *hollow* metal sphere:\n",
    "      - inner radius R0 ≈ 20 px (hollow)\n",
    "      - outer radius R1 ≈ 40 px (metal shell)\n",
    "      - intensity decays smoothly from the inner boundary to the outer boundary\n",
    "  • Adds small, realistic variations per image:\n",
    "      radii jitter, slight centre shift, mild ellipticity,\n",
    "      gentle low‑frequency “illumination” shading,\n",
    "      sensor noise, tiny label noise\n",
    "  • Defines a per‑image target y (“density factor”): it globally scales the shell intensity.\n",
    "    This makes the pixel‑wise linear model predictive, but noise/variation keep it realistic.\n",
    "  • Shows:\n",
    "      - a grid of sample images with y_true and y_obs,\n",
    "      - the label histogram (y_obs),\n",
    "      - the mean image across the dataset.\n",
    "\n",
    "\n",
    "Variables created for later cells:\n",
    "  images          : (N, 100, 100) float32 in [0,1]\n",
    "  y_true_all      : (N,) noise‑free target values\n",
    "  y_obs_all       : (N,) observed targets (y_true + tiny noise)\n",
    "  helper functions: print_metrics, plot_pred_vs_actual, show_heatmap\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility & settings\n",
    "# ----------------------------\n",
    "SEED = 2025\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "IMG_SIZE = 100\n",
    "N_IMAGES = 1000\n",
    "\n",
    "# Geometry\n",
    "R0_BASE = 20.0     # inner (hollow) radius\n",
    "R1_BASE = 40.0     # outer radius\n",
    "CENTER   = (IMG_SIZE - 1) / 2.0  # 49.5 (sub‑pixel centre for symmetry)\n",
    "\n",
    "# Variation & noise (small but meaningful)\n",
    "RADIUS_JITTER_PX      = 2.0\n",
    "CENTER_JITTER_PX      = 1.0\n",
    "ELLIPTICITY_RANGE     = (0.98, 1.02)\n",
    "PROFILE_GAMMA_RANGE   = (0.90, 1.10)   # 1.0 is linear fall‑off\n",
    "SHADING_ALPHA_RANGE   = (0.05, 0.12)   # multiplicative, low‑frequency illumination\n",
    "PIXEL_NOISE_STD       = 0.02           # additive pixel noise\n",
    "BACKGROUND_OFFSET_MAX = 0.01           # small sensor offset per image\n",
    "Y_RANGE               = (0.8, 1.2)     # density scale factor\n",
    "Y_LABEL_NOISE_STD     = 0.01           # tiny measurement noise on labels\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: metrics & plotting (re‑used in later cells)\n",
    "# ----------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def print_metrics(split_name, y_true, y_pred):\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rms = rmse(y_true, y_pred)\n",
    "    print(f\"[{split_name}] R² = {r2:.4f} | MAE = {mae:.4f} | RMSE = {rms:.4f}\")\n",
    "    return r2, mae, rms\n",
    "\n",
    "def plot_pred_vs_actual(y_actual, y_pred, title=\"Predicted vs Actual\"):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(y_actual, y_pred, s=14, alpha=0.75)\n",
    "    lo = min(float(np.min(y_actual)), float(np.min(y_pred)))\n",
    "    hi = max(float(np.max(y_actual)), float(np.max(y_pred)))\n",
    "    pad = 0.02 * (hi - lo) if hi > lo else 0.01\n",
    "    plt.plot([lo - pad, hi + pad], [lo - pad, hi + pad], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"Actual y\")\n",
    "    plt.ylabel(\"Predicted y\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_heatmap(arr2d, title, cmap=None, with_colorbar=True):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(arr2d, cmap=cmap)\n",
    "    if with_colorbar:\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Coordinates (constant fields)\n",
    "# ----------------------------\n",
    "y_coords, x_coords = np.indices((IMG_SIZE, IMG_SIZE))\n",
    "X0 = x_coords - CENTER\n",
    "Y0 = y_coords - CENTER\n",
    "XN = X0 / R1_BASE\n",
    "YN = Y0 / R1_BASE\n",
    "\n",
    "def make_low_frequency_shading(alpha, rng):\n",
    "    \"\"\"\n",
    "    Smooth multiplicative shading:\n",
    "      shade(x,y) = 1 + alpha * P2(x,y),\n",
    "    where P2 is a random quadratic in (XN, YN), normalised to zero‑mean and unit‑std.\n",
    "    \"\"\"\n",
    "    c = rng.normal(0.0, 1.0, size=6)\n",
    "    field = (c[0] + c[1]*XN + c[2]*YN + c[3]*XN*YN + c[4]*XN*XN + c[5]*YN*YN)\n",
    "    field = field - field.mean()\n",
    "    std = field.std()\n",
    "    if std > 1e-8:\n",
    "        field = field / std\n",
    "    return 1.0 + alpha * field\n",
    "\n",
    "def generate_one_image(rng):\n",
    "    \"\"\"\n",
    "    Create one hollow‑sphere image with small geometric & illumination variations.\n",
    "    Returns:\n",
    "      img    : (H,W) float32 in [0,1]\n",
    "      y_true : noise‑free density factor\n",
    "      y_obs  : observed (y_true + tiny label noise)\n",
    "    \"\"\"\n",
    "    # Jittered geometry\n",
    "    R0 = R0_BASE + rng.uniform(-RADIUS_JITTER_PX, +RADIUS_JITTER_PX)\n",
    "    R1 = R1_BASE + rng.uniform(-RADIUS_JITTER_PX, +RADIUS_JITTER_PX)\n",
    "    if R1 <= R0 + 8.0:  # ensure sensible shell thickness\n",
    "        R1 = R0 + 8.0\n",
    "\n",
    "    cx = CENTER + rng.uniform(-CENTER_JITTER_PX, +CENTER_JITTER_PX)\n",
    "    cy = CENTER + rng.uniform(-CENTER_JITTER_PX, +CENTER_JITTER_PX)\n",
    "\n",
    "    sx = rng.uniform(*ELLIPTICITY_RANGE)  # mild ellipticity\n",
    "    sy = rng.uniform(*ELLIPTICITY_RANGE)\n",
    "\n",
    "    gamma = rng.uniform(*PROFILE_GAMMA_RANGE)  # fall‑off shape\n",
    "\n",
    "    # Elliptical radius from (cx,cy)\n",
    "    dx = (x_coords - cx) / sx\n",
    "    dy = (y_coords - cy) / sy\n",
    "    re = np.sqrt(dx*dx + dy*dy)\n",
    "\n",
    "    # Ring profile: value 1 at inner boundary → 0 at outer boundary, shaped by gamma\n",
    "    img = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float64)\n",
    "    shell = (re > R0) & (re <= R1)\n",
    "    t = np.empty_like(re)\n",
    "    t[shell] = (re[shell] - R0) / (R1 - R0)  # 0 at inner boundary, 1 at outer\n",
    "    img[shell] = (1.0 - t[shell]) ** gamma\n",
    "\n",
    "    # Low‑frequency multiplicative shading\n",
    "    alpha = rng.uniform(*SHADING_ALPHA_RANGE)\n",
    "    shading = make_low_frequency_shading(alpha, rng)\n",
    "    img = img * shading\n",
    "\n",
    "    # Apply density scaling (true label)\n",
    "    y_true = float(rng.uniform(*Y_RANGE))\n",
    "    img = y_true * img\n",
    "\n",
    "    # Sensor offset + pixel noise\n",
    "    offset = rng.uniform(0.0, BACKGROUND_OFFSET_MAX)\n",
    "    img = img + offset\n",
    "    img = img + rng.normal(0.0, PIXEL_NOISE_STD, size=img.shape)\n",
    "\n",
    "    # Clip to [0,1]\n",
    "    img = np.clip(img, 0.0, 1.0).astype(np.float32)\n",
    "\n",
    "    # Observed label with tiny measurement noise\n",
    "    y_obs = y_true + float(rng.normal(0.0, Y_LABEL_NOISE_STD))\n",
    "    return img, y_true, y_obs\n",
    "\n",
    "def generate_dataset(n_images, rng):\n",
    "    images = np.empty((n_images, IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    y_true = np.empty(n_images, dtype=np.float32)\n",
    "    y_obs  = np.empty(n_images, dtype=np.float32)\n",
    "    for i in range(n_images):\n",
    "        img, yt, yo = generate_one_image(rng)\n",
    "        images[i] = img\n",
    "        y_true[i] = yt\n",
    "        y_obs[i]  = yo\n",
    "    return images, y_true, y_obs\n",
    "\n",
    "# ----------------------------\n",
    "# Generate & summarise\n",
    "# ----------------------------\n",
    "images, y_true_all, y_obs_all = generate_dataset(N_IMAGES, rng)\n",
    "\n",
    "print(\"Dataset summary:\")\n",
    "print(f\"  images.shape = {images.shape} (float32, values in [0,1])\")\n",
    "print(f\"  y_true: mean={y_true_all.mean():.4f}, std={y_true_all.std():.4f}, \"\n",
    "      f\"min={y_true_all.min():.4f}, max={y_true_all.max():.4f}\")\n",
    "print(f\"  y_obs :  mean={y_obs_all.mean():.4f},  std={y_obs_all.std():.4f},  \"\n",
    "      f\"min={y_obs_all.min():.4f},  max={y_obs_all.max():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Visual checks\n",
    "# ----------------------------\n",
    "# Grid of sample images\n",
    "def show_image_grid(images, y_true, y_obs, n_show=12, title=\"Sample simulated images\"):\n",
    "    n_show = int(min(n_show, images.shape[0]))\n",
    "    idxs = rng.choice(images.shape[0], size=n_show, replace=False)\n",
    "    cols = int(np.ceil(np.sqrt(n_show)))\n",
    "    rows = int(np.ceil(n_show / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(1.9*cols, 1.9*rows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "    for ax, i in zip(axes, idxs):\n",
    "        ax.imshow(images[i], cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "        ax.set_title(f\"y_true={y_true[i]:.3f}\\ny_obs={y_obs[i]:.3f}\", fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "    for k in range(n_show, len(axes)):  # hide any spare panels\n",
    "        axes[k].axis(\"off\")\n",
    "    plt.suptitle(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_image_grid(images, y_true_all, y_obs_all, n_show=12,\n",
    "                title=\"Synthetic hollow‑sphere X‑ray images\")\n",
    "\n",
    "# Label histogram\n",
    "plt.figure(figsize=(5.5, 4))\n",
    "plt.hist(y_obs_all, bins=20, edgecolor=\"black\", alpha=0.85)\n",
    "plt.xlabel(\"Observed y\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Label distribution (y_obs)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mean image (across the full dataset)\n",
    "mean_img = images.mean(axis=0)\n",
    "show_heatmap(mean_img, title=\"Mean image (all samples)\", cmap=\"gray\", with_colorbar=True)\n",
    "\n",
    "print(\"\\nAll arrays are now in memory: images, y_true_all, y_obs_all.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "\n",
    "# Linear and Ridge regression\n",
    "\n",
    "**Purpose of this cell**  \n",
    "Fit two classical linear baselines on the synthetic images: (i) Ordinary Least Squares (OLS) on flattened pixels and (ii) Ridge regression with feature standardisation and cross‑validated regularisation. Report metrics on Train/Validation/Test splits and visualise both predictions and coefficient maps.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Data layout and splits\n",
    "\n",
    "- Each image has shape $H\\times W=100\\times 100$ and is flattened into a vector of length $p=10{,}000$. Stacking all images yields a design matrix $X\\in\\mathbb{R}^{n\\times p}$ and a target vector $y\\in\\mathbb{R}^{n}$.\n",
    "- The cell creates fixed splits (seeded): **Train 64%**, **Validation 16%**, **Test 20%**.\n",
    "- Note the scale: typically $p \\gg n_{\\text{train}}$ (e.g. $10{,}000$ features vs about $640$ training samples). This matters for OLS.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Ordinary Least Squares (OLS)\n",
    "\n",
    "**Objective.** With coefficients $w\\in\\mathbb{R}^{p}$ and intercept $b\\in\\mathbb{R}$,\n",
    "$$\n",
    "\\min_{w,b}\\ \\frac{1}{2}\\,\\lVert Xw + b\\mathbf{1} - y\\rVert_2^2.\n",
    "$$\n",
    "\n",
    "Let $\\tilde X=[X\\ \\ \\mathbf{1}]$ and $\\tilde w=[w^\\top\\ \\ b]^\\top$. The SVD‑based solver returns the minimum‑norm solution\n",
    "$$\n",
    "\\tilde w^\\star=\\tilde X^{+}y,\n",
    "$$\n",
    "where $\\tilde X^{+}$ is the Moore–Penrose pseudoinverse.\n",
    "\n",
    "**Why training $R^2$ can be $\\approx 1$ when $p\\gg n$.**  \n",
    "When $\\operatorname{rank}(\\tilde X)=n_{\\text{train}}$, the fitted predictions on the training set satisfy\n",
    "$$\n",
    "\\hat y=\\tilde X\\tilde w^\\star=\\tilde X\\tilde X^{+}y=Hy,\n",
    "$$\n",
    "with $H$ the idempotent **hat matrix** (a projector). In the over‑parameterised regime the column space of $\\tilde X$ is large enough to interpolate $y$, producing near‑zero training error and hence $R^2_{\\text{train}}\\approx 1$. This indicates capacity, not necessarily genuine signal.\n",
    "\n",
    "**Plots and maps.** The cell shows Predicted‑vs‑Actual scatter plots and reshapes the learned $w$ to $H\\times W$ to display a coefficient heatmap.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Ridge regression (with standardisation and cross‑validation)\n",
    "\n",
    "**Why standardise?** Ridge penalises coefficient magnitudes, so each feature is scaled to comparable units:\n",
    "$$\n",
    "Z_{ij}=\\frac{X_{ij}-\\mu_j}{\\sigma_j}.\n",
    "$$\n",
    "\n",
    "**Objective and solution.**\n",
    "$$\n",
    "\\min_{w}\\ \\frac{1}{2}\\,\\lVert Zw-y\\rVert_2^2+\\lambda\\lVert w\\rVert_2^2\n",
    "\\quad\\Longrightarrow\\quad\n",
    "w_\\lambda=(Z^\\top Z+\\lambda I)^{-1}Z^\\top y.\n",
    "$$\n",
    "The intercept is fitted separately (equivalently, $Z$ and $y$ are centred).\n",
    "\n",
    "**Choosing $\\lambda$ (alpha).**  \n",
    "A 5‑fold cross‑validation over a log‑spaced grid $\\lambda\\in[10^{-6},10^3]$ is run **on the training split only**. The best $\\lambda^\\star$ by CV‑MSE is then refit on **Train+Val** and evaluated once on Test.\n",
    "\n",
    "**Mapping coefficients back to raw pixel space.**  \n",
    "Let $w^{\\text{scaled}}$ be the weights learned on $Z$. With scaler statistics $(\\mu_j,\\sigma_j)$,\n",
    "$$\n",
    "w^{\\text{raw}}_j=\\frac{w^{\\text{scaled}}_j}{\\sigma_j},\n",
    "\\qquad\n",
    "b^{\\text{raw}}=b^{\\text{scaled}}-\\sum_{j=1}^{p}\\frac{w^{\\text{scaled}}_j\\,\\mu_j}{\\sigma_j}.\n",
    "$$\n",
    "The $H\\times W$ reshaped $w^{\\text{raw}}$ gives an interpretable pixel‑space heatmap.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Metrics reported\n",
    "\n",
    "For predictions $\\hat y$ against targets $y$:\n",
    "- **Coefficient of determination**\n",
    "$$\n",
    "R^2=1-\\frac{\\sum_i (y_i-\\hat y_i)^2}{\\sum_i (y_i-\\bar y)^2},\\qquad \\bar y=\\frac{1}{n}\\sum_i y_i.\n",
    "$$\n",
    "- **Mean Absolute Error (MAE)**: $\\ \\text{MAE}=\\frac{1}{n}\\sum_i |y_i-\\hat y_i|$.\n",
    "- **Root Mean Squared Error (RMSE)**: $\\ \\text{RMSE}=\\sqrt{\\frac{1}{n}\\sum_i (y_i-\\hat y_i)^2}$.\n",
    "\n",
    "**Noise ceiling for $R^2$.** Using the noise‑free labels $y^{\\text{true}}$,\n",
    "$$\n",
    "R^2_{\\text{ceiling}} = 1 - \\frac{\\sum_i (y^{\\text{true}}_i - y^{\\text{obs}}_i)^2}{\\sum_i (y^{\\text{obs}}_i - \\bar y^{\\text{obs}})^2},\n",
    "$$\n",
    "which equals $1-\\frac{\\operatorname{Var}(\\text{noise})}{\\operatorname{Var}(y^{\\text{obs}})}$ when the observation noise is homoscedastic.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Typical outcomes and interpretation\n",
    "\n",
    "- **OLS (Train):** $R^2$ often near 1 due to interpolation when $p\\gg n$. Validation/Test provide the honest picture.\n",
    "- **Ridge:** usually lower Train $R^2$ but **better generalisation**; the $\\ell_2$ penalty damps ill‑conditioned directions.\n",
    "- **Coefficient maps:** high absolute weights concentrate on the ring, where the generative signal lives.\n",
    "- **Negative $R^2$ on Val/Test:** means the model is worse than predicting the split mean; the tuned Ridge should avoid this unless the task is intentionally adversarial.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Practices encoded in the code\n",
    "\n",
    "- Feature scaling and Ridge are in a single `Pipeline` to avoid **data leakage** inside CV.\n",
    "- Hyper‑parameter selection uses **only the training split**; Test is never touched until the end.\n",
    "- A fixed random seed makes the split reproducible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear & Ridge regression on the in‑memory dataset\n",
    "--------------------------------------------------\n",
    "\n",
    "What this cell does:\n",
    "\n",
    "1) Builds Train / Validation / Test splits: 64% / 16% / 20%.\n",
    "2) Fits an Ordinary Least Squares (OLS) linear regression on flattened pixels → y_obs.\n",
    "   • Notes why OLS can achieve R²≈1 on the training set when p ≫ n (10,000 features vs ~640 samples).\n",
    "3) Trains a tuned Ridge regression:\n",
    "   • Pipeline(StandardScaler, Ridge)\n",
    "   • Hyper‑parameter alpha selected by 5‑fold CV on the *training* split only.\n",
    "   • Report Train & Val performance for the CV‑selected model.\n",
    "   • Refit the final Ridge on Train+Val with the chosen alpha; report Test performance.\n",
    "4) Prints metrics (R², MAE, RMSE) and draws Predicted vs Actual plots.\n",
    "5) Shows coefficient heatmaps for OLS and final Ridge (mapped back to raw‑pixel space).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sanity check: make sure Cell 1 was run\n",
    "if \"images\" not in globals() or \"y_obs_all\" not in globals():\n",
    "    raise RuntimeError(\"Cell 1 has not been run. Please run Cell 1 to create images/y_obs_all in memory.\")\n",
    "\n",
    "SEED = 2025\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC_WITHIN_TRAINVAL = 0.20  # 20% of the 80% → 16% overall\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "N, H, W = images.shape\n",
    "D = H * W\n",
    "\n",
    "# Flatten images to (N, D)\n",
    "X_all = images.reshape(N, -1).astype(np.float64)\n",
    "y_all = y_obs_all.astype(np.float64)\n",
    "y_true_all_local = y_true_all.astype(np.float64)  # for noise ceiling\n",
    "\n",
    "# Split Train+Val vs Test\n",
    "X_trainval, X_test, y_trainval, y_test, y_true_trainval, y_true_test = train_test_split(\n",
    "    X_all, y_all, y_true_all_local, test_size=TEST_FRAC, random_state=SEED\n",
    ")\n",
    "# Split Train vs Val\n",
    "X_train, X_val, y_train, y_val, y_true_train, y_true_val = train_test_split(\n",
    "    X_trainval, y_trainval, y_true_trainval, test_size=VAL_FRAC_WITHIN_TRAINVAL, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Split summary:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"  Val  : {X_val.shape[0]} samples\")\n",
    "print(f\"  Test : {X_test.shape[0]} samples\")\n",
    "print(f\"  Note: p (features) = {D:,},  n_train = {X_train.shape[0]:,}.  Here, p ≫ n.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) OLS Linear Regression (may interpolate when p ≫ n)\n",
    "# ------------------------------------------------------------\n",
    "ols = LinearRegression()  # scikit‑learn uses an SVD‑based least‑squares solver\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "yhat_tr_ols = ols.predict(X_train)\n",
    "yhat_va_ols = ols.predict(X_val)\n",
    "yhat_te_ols = ols.predict(X_test)\n",
    "\n",
    "print(\"OLS performance (against observed y):\")\n",
    "print_metrics(\"Train (OLS)\", y_train, yhat_tr_ols)\n",
    "print_metrics(\"Val   (OLS)\", y_val,   yhat_va_ols)\n",
    "print_metrics(\"Test  (OLS)\", y_test,  yhat_te_ols)\n",
    "\n",
    "# Why can Train R² be ~1?\n",
    "# With p (=10,000) >> n_train (~640), an OLS model has enough degrees of freedom to fit the training\n",
    "# labels almost perfectly (interpolate), especially as y is largely a global scaling of the image.\n",
    "# The SVD solution picks one of infinitely many solutions (minimum‑norm) that achieve near‑zero\n",
    "# training error if the design matrix has rank n_train. This is normal in over‑parameterised linear models.\n",
    "\n",
    "# Noise ceiling on Test (best achievable R² against *noisy* labels)\n",
    "r2_ceiling_test = 1.0 - np.sum((y_true_test - y_test) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "print(f\"\\n[Reference] Test‑set R² noise ceiling (label noise): {r2_ceiling_test:.4f}\\n\")\n",
    "\n",
    "# Visual diagnostics — Predicted vs Actual\n",
    "plot_pred_vs_actual(y_train, yhat_tr_ols, \"Predicted vs Actual (Train, OLS)\")\n",
    "plot_pred_vs_actual(y_val,   yhat_va_ols, \"Predicted vs Actual (Validation, OLS)\")\n",
    "plot_pred_vs_actual(y_test,  yhat_te_ols, \"Predicted vs Actual (Test, OLS)\")\n",
    "\n",
    "# Coefficient heatmap (OLS)\n",
    "coef_ols = ols.coef_.reshape(H, W)\n",
    "show_heatmap(coef_ols, title=\"OLS: learned pixel weights\", cmap=None, with_colorbar=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Ridge Regression (tuned) — Pipeline(StandardScaler, Ridge)\n",
    "# ------------------------------------------------------------\n",
    "# Standardise features because Ridge penalises the magnitude of coefficients and should\n",
    "# not be affected by arbitrary feature scales.\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"ridge\",  Ridge(random_state=SEED))\n",
    "])\n",
    "\n",
    "# Hyper‑parameter grid for alpha (L2 strength)\n",
    "alphas = np.logspace(-6, 3, 25)  # 1e-6 … 1e3\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "param_grid = {\"ridge__alpha\": alphas}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True  # refit the best on the *training* split\n",
    ")\n",
    "\n",
    "print(\"Tuning Ridge (5‑fold CV on the training split)…\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = grid.best_params_[\"ridge__alpha\"]\n",
    "print(f\"Best alpha (CV on Train): {best_alpha:.6g}\")\n",
    "\n",
    "# Evaluate the CV‑selected model on Train & Val\n",
    "ridge_cv_model = grid.best_estimator_\n",
    "yhat_tr_ridge = ridge_cv_model.predict(X_train)\n",
    "yhat_va_ridge = ridge_cv_model.predict(X_val)\n",
    "\n",
    "print(\"\\nRidge (CV‑selected) performance:\")\n",
    "print_metrics(\"Train (Ridge CV)\", y_train, yhat_tr_ridge)\n",
    "print_metrics(\"Val   (Ridge CV)\", y_val,   yhat_va_ridge)\n",
    "\n",
    "# OPTIONAL: refit on Train+Val using the chosen alpha, then evaluate on Test\n",
    "final_ridge = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"ridge\",  Ridge(alpha=best_alpha, random_state=SEED))\n",
    "])\n",
    "final_ridge.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\n",
    "\n",
    "yhat_te_ridge = final_ridge.predict(X_test)\n",
    "print_metrics(\"\\nTest  (Ridge final)\", y_test, yhat_te_ridge)\n",
    "\n",
    "# Visual diagnostics — Predicted vs Actual for Ridge (final)\n",
    "plot_pred_vs_actual(y_train, yhat_tr_ridge, \"Predicted vs Actual (Train, Ridge CV model)\")\n",
    "plot_pred_vs_actual(y_val,   yhat_va_ridge, \"Predicted vs Actual (Validation, Ridge CV model)\")\n",
    "plot_pred_vs_actual(y_test,  yhat_te_ridge, \"Predicted vs Actual (Test, Ridge final model)\")\n",
    "\n",
    "# Coefficient heatmap (Ridge, mapped back to raw pixel space)\n",
    "#  ŷ = intercept + Σ w_scaled_i * (x_i - mean_i)/std_i\n",
    "#  ⇒ effective raw‑space weight: w_raw_i = w_scaled_i / std_i\n",
    "#     effective raw intercept:  b_raw = intercept - Σ w_scaled_i * mean_i / std_i\n",
    "scaler = final_ridge.named_steps[\"scaler\"]\n",
    "ridge  = final_ridge.named_steps[\"ridge\"]\n",
    "w_scaled = ridge.coef_.ravel()\n",
    "w_raw = w_scaled / (scaler.scale_ + 1e-12)\n",
    "coef_ridge_raw = w_raw.reshape(H, W)\n",
    "show_heatmap(coef_ridge_raw, title=\"Ridge (final): pixel weights (raw‑space)\", cmap=None, with_colorbar=True)\n",
    "\n",
    "# Brief takeaways\n",
    "print(\"\\nTakeaways:\")\n",
    "print(\"  • With 10,000 features and ~640 training samples (p ≫ n), OLS can nearly interpolate the training labels,\")\n",
    "print(\"    often giving R²≈1 on Train. That’s expected and is a symptom of high capacity, not necessarily true signal.\")\n",
    "print(\"  • Ridge regularisation stabilises the solution, improves conditioning, and typically gives better generalisation.\")\n",
    "print(\"  • The learned weight maps concentrate on the ring, which is exactly where the signal lies.\")\n",
    "print(\"  • ‘Noise ceiling’ gives the best possible R² on Test when labels contain measurement noise;\")\n",
    "print(\"    if your model approaches it, you are close to optimal under the present noise level.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# CNN regressor on hollow‑sphere images\n",
    "\n",
    "## **Purpose of this cell**  \n",
    "\n",
    "### Train a convolutional regressor that predicts the scalar target from each image. The network is designed to succeed on both the *easy* dataset (no star) and the *hard* dataset (with a tiny star on the shell), while remaining compatible with Deep SHAP. \n",
    "---\n",
    "\n",
    "## 1) Inputs, splits, and preprocessing\n",
    "\n",
    "**Inputs.** Each image is a single‑channel array of shape $H\\times W=100\\times100$ with values in $[0,1]$. The target is a scalar $y$ per image (observed label), with a noise‑free companion $y^{\\text{true}}$ used only for diagnostics later.\n",
    "\n",
    "**Splits.** A fixed seed produces: Train 64%, Validation 16%, Test 20% (same recipe as the linear cell).\n",
    "\n",
    "**Background subtraction.** For each image, the median of the four $k\\times k$ corner patches (default $k=10$) estimates a background offset $b$. The corrected image uses\n",
    "$$\n",
    "I'(x,y)=\\max\\{\\,I(x,y)-b,\\,0\\,\\}.\n",
    "$$\n",
    "\n",
    "**Global normalisation (train statistics).** Let $\\mu=\\operatorname{mean}(I'_{\\text{train}})$ and $\\sigma=\\operatorname{std}(I'_{\\text{train}})$. Normalise every split with\n",
    "$$\n",
    "\\tilde I(x,y)=\\operatorname{clip}\\!\\left(\\frac{I'(x,y)-\\mu}{\\sigma},\\, -8,\\ 8\\right).\n",
    "$$\n",
    "\n",
    "**CoordConv channels.** Three coordinate maps are concatenated to the image channel:\n",
    "- $x_{\\text{lin}}=(x-c_x)/C$,\n",
    "- $y_{\\text{lin}}=(y-c_y)/C$,\n",
    "- $r=\\sqrt{x_{\\text{lin}}^2+y_{\\text{lin}}^2}$ (rescaled to $[0,1]$),\n",
    "with $C=(H-1)/2$. These channels provide translation‑/rotation‑aware geometry; the shell and the star both live on a ring in $r$.\n",
    "\n",
    "**Data augmentation.** Random horizontal and vertical flips. Arrays are re‑made contiguous after flipping to avoid “negative stride” issues when converting to PyTorch tensors.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Target standardisation\n",
    "\n",
    "The network predicts a **z‑scored** target on the training split:\n",
    "$$\n",
    "z=\\frac{y-\\mu_y}{\\sigma_y},\\qquad \\hat z=f_\\theta(\\tilde I, x_{\\text{lin}}, y_{\\text{lin}}, r).\n",
    "$$\n",
    "Predictions are mapped back to label space with\n",
    "$$\n",
    "\\hat y=\\sigma_y\\,\\hat z+\\mu_y.\n",
    "$$\n",
    "This stabilises optimisation and aligns scales across experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Dataset and loaders\n",
    "\n",
    "A `Dataset` returns triples $(X, z, y)$ where $X\\in\\mathbb{R}^{4\\times H\\times W}$ contains `[image, x, y, r]`.  \n",
    "Loaders use a moderate batch size (128 on CUDA/MPS, 64 on CPU).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Network architecture (SHAP‑compatible)\n",
    "\n",
    "The model exposes `.features`, `.gap`, and `.head` with **exactly two `Linear` layers** in `.head`. All activations are non‑in‑place (`ReLU(inplace=False)`), and the forward method returns a cloned tensor to keep Deep SHAP happy.\n",
    "\n",
    "### 4.1 Fixed image stem (edge/contrast helpers)\n",
    "\n",
    "From the image channel only, two fixed maps are computed:\n",
    "- Sobel gradient magnitude $G=\\sqrt{(I_x)^2+(I_y)^2}$,\n",
    "- Local high‑pass $H=I-\\text{mean3}(I)$,\n",
    "then concatenated: `[image, x, y, r, G, H]` → **6 channels**. These maps are part of the graph (differentiable) but non‑trainable, so external inputs remain 4‑channel.\n",
    "\n",
    "### 4.2 Ring priors and “starness” branch\n",
    "\n",
    "From the radius map $r$, three Gaussian bands act as **priors** for the shell:\n",
    "$$\n",
    "\\text{ring}(r;r_0,\\sigma)=\\exp\\!\\left(-\\tfrac12\\Big(\\tfrac{r-r_0}{\\sigma}\\Big)^2\\right).\n",
    "$$\n",
    "- Mid‑shell: $r_0\\approx0.435,\\ \\sigma\\approx0.060$  \n",
    "- Inner edge: $r_0\\approx0.283,\\ \\sigma\\approx0.040$  \n",
    "- Outer edge: $r_0\\approx0.566,\\ \\sigma\\approx0.040$\n",
    "\n",
    "A lightweight starness branch runs two $3\\times3$ convolutions with ReLU on the **gated image** $I \\times \\text{ring}_{\\text{mid}}$ to produce a `star_map`. A broadcast global‑mean map $g(x,y)=\\operatorname{mean}(I)$ supplies scene‑wide brightness context. Concatenation yields **11 channels**:\n",
    "```\n",
    "[image, x, y, r, G, H, ring_mid, edge_inner, edge_outer, star_map, gmean]\n",
    "```\n",
    "\n",
    "### 4.3 Convolutional trunk\n",
    "\n",
    "A sequence of Conv → ReLU → small residual blocks, with two max‑pools:\n",
    "- 100→50 and 50→25 spatial downsamples.\n",
    "- No BatchNorm/GroupNorm (avoids instability on MPS, and keeps the graph simple for SHAP).\n",
    "\n",
    "### 4.4 Global pooling that preserves tiny cues\n",
    "\n",
    "`.gap` is a small custom module:\n",
    "- Global **mean** pooling per channel,\n",
    "- Concatenated with global **max** pooling per channel,\n",
    "so a tensor with $C$ channels becomes $2C$. Formally, for channel $k$,\n",
    "$$\n",
    "p^{\\text{avg}}_k=\\frac{1}{HW}\\sum_{i,j} A_{kij},\\qquad\n",
    "p^{\\text{max}}_k=\\max_{i,j} A_{kij},\\qquad\n",
    "p=\\big[p^{\\text{avg}},\\,p^{\\text{max}}\\big].\n",
    "$$\n",
    "The max path ensures that a small, bright star can dominate the pooled descriptor even if its area is tiny.\n",
    "\n",
    "### 4.5 Head (two `Linear` layers)\n",
    "\n",
    "```\n",
    "Flatten → Linear(2C,128) → ReLU → Dropout(0.10) → Linear(128,1)\n",
    "```\n",
    "This outputs $\\hat z$ (a scalar). Returning `x.clone()` in `forward` avoids autograd view/in‑place quirks that sometimes trip Deep SHAP.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Loss, optimisation, and schedules\n",
    "\n",
    "**Loss (MSE in z‑space).**\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}\\big(\\hat z_i - z_i\\big)^2.\n",
    "$$\n",
    "\n",
    "**Optimiser and schedule.** AdamW with weight decay $2\\times10^{-4}$, initial learning rate $6\\times10^{-4}$, cosine annealing to $10^{-5}$.\n",
    "\n",
    "**Regularisation and stability.**\n",
    "- Gradient clipping at unit norm.\n",
    "- Early stopping on validation loss (patience 20 epochs).\n",
    "- Non‑finite‑loss guard: if a batch yields non‑finite loss (rare on MPS), halve the learning rate and skip that step.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Metrics and plots\n",
    "\n",
    "After restoring the best validation checkpoint, the cell reports for Train/Val/Test:\n",
    "- $R^2=1-\\dfrac{\\sum(y-\\hat y)^2}{\\sum(y-\\bar y)^2}$,\n",
    "- MAE and RMSE in label space.  \n",
    "The “noise ceiling” estimate on Test quantifies the best possible $R^2$ given the label noise.\n",
    "\n",
    "Two diagnostic plot types are produced:\n",
    "1) Training curves (loss vs epoch).  \n",
    "2) Predicted‑vs‑Actual scatter, with the identity line for reference.\n",
    "\n",
    "A small panel shows Original image → Background‑subtracted input → textual triplet $(y^{\\text{true}}, y^{\\text{obs}}, \\hat y)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Why this CNN works on both datasets\n",
    "\n",
    "- **Easy (no star):** The target is largely a global density scale. The global‑mean map and the average‑pool path allow the model to capture this quickly. Ring priors and edge‑aware features encourage a focus on the shell rather than the background.\n",
    "- **Hard (with star):** The star is a compact, high‑contrast cue located on the shell. The starness branch creates a concentrated activation where a star‑like pattern aligns with the mid‑ring. The **max** path in ConcatPool ensures this activation can drive the prediction even with tiny area. CoordConv provides the geometry to separate inner/outer edges from mid‑ring content, improving localisation.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) SHAP compatibility and later interpretability\n",
    "\n",
    "- The class exposes `.features`, `.gap`, and `.head`, with exactly **two** `Linear` layers in the head, matching downstream SHAP assumptions.\n",
    "- All activations are non‑in‑place. The forward pass returns a clone.  \n",
    "- Deep SHAP is typically wrapped in a small compatibility class that replaces `nn.Flatten` with a functional flatten; additivity checks are disabled when needed.\n",
    "\n",
    "This allows downstream analyses to: select a good baseline, draw overlays of SHAP attributions, compute deletion curves, and run component‑aware bounding boxes that verify whether the model locks onto the star.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Practical checks and troubleshooting\n",
    "\n",
    "- If validation loss becomes `NaN` on MPS, reduce the learning rate and verify that inputs are finite after normalisation. The guard in the training loop already performs a back‑off.\n",
    "- Confirm that background subtraction and normalisation reuse **training** statistics for all splits; otherwise leakage or distribution shift can degrade generalisation.\n",
    "- When porting to a different geometry, retune the ring‑prior centres and widths; the general form\n",
    "  $$\\text{ring}(r;r_0,\\sigma)=\\exp\\!\\left(-\\tfrac12\\big((r-r_0)/\\sigma\\big)^2\\right)$$\n",
    "  remains the same.\n",
    "- To emphasise microscopic cues further, bias the pooling by increasing the weight of the max branch (e.g., concatenate `[mean, max, max]`) and adjust the first Linear layer shape accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Take‑home message\n",
    "\n",
    "The architecture deliberately separates three responsibilities:\n",
    "1) **Global brightness** (handled by average pooling and the global‑mean map),\n",
    "2) **Shell geometry** (handled by CoordConv radius and ring priors),\n",
    "3) **Tiny local anomalies** such as a star (handled by the starness branch and the max‑pool path).\n",
    "\n",
    "This separation makes the network robust, interpretable with SHAP, and effective across both easy and hard variants of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — CNN that learns global density + shell edges + a tiny star if there\n",
    "# (SHAP‑compatible, M1‑safe, robust training)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- Preconditions\n",
    "if \"images\" not in globals() or \"y_true_all\" not in globals() or \"y_obs_all\" not in globals():\n",
    "    raise RuntimeError(\"Please run Cell 1 (data generation) first.\")\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# ----- Device (M1 → MPS if available)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ----- Repro\n",
    "SEED = 2025\n",
    "np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ----- Split (same recipe as Cell 2)\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC_WITHIN_TRAINVAL = 0.20\n",
    "\n",
    "N, H, W = images.shape\n",
    "CENTER = (H - 1) / 2.0\n",
    "\n",
    "X_all = images\n",
    "y_all = y_obs_all.astype(np.float32)\n",
    "y_true_all_local = y_true_all.astype(np.float32)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test, y_true_trainval, y_true_test = train_test_split(\n",
    "    X_all, y_all, y_true_all_local, test_size=TEST_FRAC, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val, y_true_train, y_true_val = train_test_split(\n",
    "    X_trainval, y_trainval, y_true_trainval, test_size=VAL_FRAC_WITHIN_TRAINVAL, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Split summary:\")\n",
    "print(f\"  Train: {X_train.shape[0]}  | Val: {X_val.shape[0]}  | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# ----- Preprocessing (corner‑median background → train‑stats normalisation)\n",
    "ys, xs = np.indices((H, W))\n",
    "\n",
    "def estimate_background_offset(img: np.ndarray, k: int = 10) -> float:\n",
    "    patches = [img[:k, :k], img[:k, -k:], img[-k:, :k], img[-k:, -k:]]\n",
    "    return float(np.median(np.concatenate([p.ravel() for p in patches])))\n",
    "\n",
    "def subtract_background(X: np.ndarray) -> np.ndarray:\n",
    "    Xp = np.empty_like(X)\n",
    "    for i, img in enumerate(X):\n",
    "        bg = estimate_background_offset(img, k=10)\n",
    "        im = img - bg\n",
    "        im[im < 0.0] = 0.0\n",
    "        Xp[i] = im\n",
    "    return Xp\n",
    "\n",
    "X_train_s = subtract_background(X_train)\n",
    "X_val_s   = subtract_background(X_val)\n",
    "X_test_s  = subtract_background(X_test)\n",
    "\n",
    "pix_mean = float(X_train_s.mean())\n",
    "pix_std  = float(X_train_s.std() + 1e-8)\n",
    "\n",
    "def norm_clamp(Xs):\n",
    "    Xn = (Xs - pix_mean) / pix_std\n",
    "    Xn = np.clip(Xn, -8.0, 8.0).astype(np.float32)\n",
    "    return Xn\n",
    "\n",
    "X_train_n = norm_clamp(X_train_s)\n",
    "X_val_n   = norm_clamp(X_val_s)\n",
    "X_test_n  = norm_clamp(X_test_s)\n",
    "\n",
    "# CoordConv channels (exported for SHAP)\n",
    "x_lin = (xs - CENTER) / CENTER\n",
    "y_lin = (ys - CENTER) / CENTER\n",
    "r_map = np.sqrt(x_lin**2 + y_lin**2)\n",
    "r_map = r_map / (r_map.max() + 1e-12)\n",
    "\n",
    "coord_tensor = torch.from_numpy(np.stack([x_lin, y_lin, r_map], axis=0).astype(np.float32))\n",
    "coord_hflip  = torch.flip(coord_tensor, dims=[2])\n",
    "coord_vflip  = torch.flip(coord_tensor, dims=[1])\n",
    "coord_bflip  = torch.flip(coord_tensor, dims=[1, 2])\n",
    "\n",
    "# ----- Target scaling (z‑score on TRAIN)\n",
    "y_mean = float(np.mean(y_train))\n",
    "y_std  = float(np.std(y_train) + 1e-8)\n",
    "def to_z(y):   return ((y - y_mean) / y_std).astype(np.float32)\n",
    "def from_z(z): return z * y_std + y_mean\n",
    "\n",
    "z_train = to_z(y_train); z_val = to_z(y_val); z_test = to_z(y_test)\n",
    "\n",
    "# ----- Augmentation (flips; always contiguous → no negative‑stride errors)\n",
    "def random_flip_image_and_coords(im: np.ndarray):\n",
    "    hflip = (np.random.rand() < 0.5)\n",
    "    vflip = (np.random.rand() < 0.5)\n",
    "    if hflip: im = im[:, ::-1]\n",
    "    if vflip: im = im[::-1, :]\n",
    "    im = np.ascontiguousarray(im, dtype=np.float32)\n",
    "    if   hflip and  vflip: coord_use = coord_bflip\n",
    "    elif hflip and not vflip: coord_use = coord_hflip\n",
    "    elif not hflip and vflip: coord_use = coord_vflip\n",
    "    else: coord_use = coord_tensor\n",
    "    return im, coord_use\n",
    "\n",
    "class ImageRegDataset(Dataset):\n",
    "    def __init__(self, Xn, z, y, train: bool):\n",
    "        self.Xn = Xn; self.z = z; self.y = y; self.train = train\n",
    "    def __len__(self): return self.Xn.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        im = self.Xn[i]\n",
    "        if self.train:\n",
    "            im, coord_use = random_flip_image_and_coords(im)\n",
    "        else:\n",
    "            im = np.ascontiguousarray(im, dtype=np.float32)\n",
    "            coord_use = coord_tensor\n",
    "        x_img = torch.from_numpy(im).unsqueeze(0)              # (1,H,W)\n",
    "        x     = torch.cat([x_img, coord_use], dim=0)           # (4,H,W)\n",
    "        z     = torch.tensor(self.z[i:i+1], dtype=torch.float32)\n",
    "        y     = torch.tensor(self.y[i:i+1], dtype=torch.float32)\n",
    "        return x, z, y\n",
    "\n",
    "BATCH_SIZE = 128 if DEVICE in (\"cuda\", \"mps\") else 64\n",
    "PIN = (DEVICE == \"cuda\")\n",
    "train_loader = DataLoader(ImageRegDataset(X_train_n, z_train, y_train, train=True),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=PIN, num_workers=0)\n",
    "val_loader   = DataLoader(ImageRegDataset(X_val_n,   z_val,   y_val,   train=False),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=PIN, num_workers=0)\n",
    "test_loader  = DataLoader(ImageRegDataset(X_test_n,  z_test,  y_test,  train=False),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=PIN, num_workers=0)\n",
    "\n",
    "# ----- Fixed helper maps (inside the net; external input stays 4‑ch)\n",
    "class FixedImageStem(nn.Module):\n",
    "    \"\"\"Sobel grad‑mag + local contrast from image channel.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        sobel_x = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "        sobel_y = sobel_x.transpose(2,3).contiguous()\n",
    "        mean3   = torch.full((1,1,3,3), 1.0/9.0, dtype=torch.float32)\n",
    "        self.conv_sx = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        self.conv_sy = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        self.blur3   = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.conv_sx.weight.copy_(sobel_x)\n",
    "            self.conv_sy.weight.copy_(sobel_y)\n",
    "            self.blur3.weight.copy_(mean3)\n",
    "        for p in self.parameters(): p.requires_grad = False\n",
    "    def forward(self, x4):\n",
    "        img = x4[:, :1]\n",
    "        gx = self.conv_sx(img); gy = self.conv_sy(img)\n",
    "        gradmag   = torch.sqrt(torch.clamp(gx*gx + gy*gy, min=1e-12))\n",
    "        localmean = self.blur3(img)\n",
    "        highpass  = img - localmean\n",
    "        return torch.cat([x4, gradmag, highpass], dim=1)  # 4 → 6 ch\n",
    "\n",
    "# ----- Ring priors (from r channel) + starness branch\n",
    "class RingPriorsAndStar(nn.Module):\n",
    "    \"\"\"\n",
    "    Inside‑net priors:\n",
    "      • ring_mid, edge_inner, edge_outer from r channel (Gaussian bands)\n",
    "      • starness: convs on (img × ring_mid) to produce a focused 'star map'\n",
    "    Returns: concatenation of [x6, priors(3), star_map(1), global_mean_map(1)] → 6 + 3 + 1 + 1 = 11 channels\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # starness sub-net (lightweight, high gain)\n",
    "        self.s_c1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.s_a1 = nn.ReLU(inplace=False)\n",
    "        self.s_c2 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "        nn.init.kaiming_normal_(self.s_c1.weight, nonlinearity=\"relu\")\n",
    "        nn.init.kaiming_normal_(self.s_c2.weight, nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.s_c1.bias); nn.init.zeros_(self.s_c2.bias)\n",
    "\n",
    "    def forward(self, x6):\n",
    "        # x6 = [img, x, y, r, gradmag, highpass]\n",
    "        img = x6[:, :1]\n",
    "        r   = x6[:, 3:4]\n",
    "        # ring priors (values tuned to your geometry: inner~0.283, mid~0.435, outer~0.566)\n",
    "        ring_mid   = torch.exp(-0.5*((r - 0.435)/0.060)**2)\n",
    "        edge_inner = torch.exp(-0.5*((r - 0.283)/0.040)**2)\n",
    "        edge_outer = torch.exp(-0.5*((r - 0.566)/0.040)**2)\n",
    "        # starness on img × ring_mid\n",
    "        z = img * ring_mid\n",
    "        z = self.s_a1(self.s_c1(z))\n",
    "        star_map = torch.relu(self.s_c2(z)) * ring_mid  # keep it on the ring\n",
    "        # global context map (mean intensity of img)\n",
    "        gmean = img.mean(dim=(2,3), keepdim=True).expand_as(img)\n",
    "        return torch.cat([x6, ring_mid, edge_inner, edge_outer, star_map, gmean], dim=1)  # 11 ch\n",
    "\n",
    "# ----- A small, norm‑free residual block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.pad1 = nn.ReflectionPad2d(1); self.c1 = nn.Conv2d(c, c, 3)\n",
    "        self.act  = nn.ReLU(inplace=False)\n",
    "        self.pad2 = nn.ReflectionPad2d(1); self.c2 = nn.Conv2d(c, c, 3)\n",
    "        nn.init.kaiming_normal_(self.c1.weight, nonlinearity=\"relu\")\n",
    "        nn.init.kaiming_normal_(self.c2.weight, nonlinearity=\"relu\")\n",
    "        if self.c1.bias is not None: nn.init.zeros_(self.c1.bias)\n",
    "        if self.c2.bias is not None: nn.init.zeros_(self.c2.bias)\n",
    "    def forward(self, x):\n",
    "        y = self.c1(self.pad1(x)); y = self.act(y); y = self.c2(self.pad2(y))\n",
    "        return self.act(y + x)\n",
    "\n",
    "class ConcatPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat([torch.mean(x, dim=(2,3), keepdim=True),\n",
    "                          torch.amax(x, dim=(2,3), keepdim=True)], dim=1)\n",
    "\n",
    "# ----- Features extractor\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem      = FixedImageStem()          # 4 → 6\n",
    "        self.ring_star = RingPriorsAndStar()       # 6 → 11\n",
    "        self.pad0 = nn.ReflectionPad2d(1); self.c0 = nn.Conv2d(11, 64, 3); self.a0 = nn.ReLU(inplace=False)\n",
    "        self.b1 = ResBlock(64); self.pool1 = nn.MaxPool2d(2)     # 100 → 50\n",
    "        self.pad1 = nn.ReflectionPad2d(1); self.c1 = nn.Conv2d(64, 128, 3); self.a1 = nn.ReLU(inplace=False)\n",
    "        self.b2 = ResBlock(128); self.pool2 = nn.MaxPool2d(2)    # 50 → 25\n",
    "        self.pad2 = nn.ReflectionPad2d(1); self.c2 = nn.Conv2d(128, 160, 3); self.a2 = nn.ReLU(inplace=False)\n",
    "        self.b3 = ResBlock(160)\n",
    "        for m in [self.c0, self.c1, self.c2]:\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, x4):\n",
    "        x6  = self.stem(x4)\n",
    "        x11 = self.ring_star(x6)\n",
    "        y = self.a0(self.c0(self.pad0(x11))); y = self.b1(y); y = self.pool1(y)\n",
    "        y = self.a1(self.c1(self.pad1(y)));  y = self.b2(y); y = self.pool2(y)\n",
    "        y = self.a2(self.c2(self.pad2(y)));  y = self.b3(y)\n",
    "        return y\n",
    "\n",
    "# ----- Regressor (SHAP‑compatible: .features / .gap / .head ; TWO Linear)\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch=4):\n",
    "        super().__init__()\n",
    "        self.features = FeatureExtractor()\n",
    "        self.gap      = ConcatPool2d()             # preserves “max” path for a tiny star\n",
    "        self.head     = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*160, 128), nn.ReLU(inplace=False),\n",
    "            nn.Dropout(p=0.10),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.head(x)\n",
    "        return x.clone()   # SHAP‑safe\n",
    "\n",
    "model = CNNRegressor(in_ch=4).to(DEVICE)\n",
    "print(model)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ----- Training (robust but simple; M1‑safe)\n",
    "LR = 6e-4\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EPOCHS = 140\n",
    "PATIENCE = 20\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=EPOCHS, eta_min=1e-5)\n",
    "\n",
    "best_val = float(\"inf\"); best_state = None\n",
    "train_losses, val_losses = [], []; no_improve = 0\n",
    "\n",
    "def _finite(x): return torch.isfinite(x).all().item()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    for xb, zb, _yb in train_loader:\n",
    "        xb, zb = xb.to(DEVICE), zb.to(DEVICE)\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "        pred_z = model(xb)\n",
    "        loss = criterion(pred_z, zb)\n",
    "        if not _finite(loss):\n",
    "            # rare on MPS: back off LR and skip this step\n",
    "            for g in optimiser.param_groups: g[\"lr\"] = max(g[\"lr\"]*0.5, 1e-5)\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimiser.step()\n",
    "        loss_sum += float(loss.item()) * xb.size(0)\n",
    "    train_loss = loss_sum / len(train_loader.dataset); train_losses.append(train_loss)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, zb, _yb in val_loader:\n",
    "            xb, zb = xb.to(DEVICE), zb.to(DEVICE)\n",
    "            pred_z = model(xb)\n",
    "            loss_sum += float(criterion(pred_z, zb).item()) * xb.size(0)\n",
    "    val_loss = loss_sum / len(val_loader.dataset); val_losses.append(val_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Train L={train_loss:.6f} | Val L={val_loss:.6f} | LR={scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-7:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping (patience={PATIENCE}).\")\n",
    "            break\n",
    "\n",
    "# Restore best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----- Metrics in y‑space\n",
    "def metrics_from_loader(name, loader):\n",
    "    model.eval()\n",
    "    preds_y, trues_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, zb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            zhat = model(xb).cpu().numpy().squeeze(-1)\n",
    "            yhat = from_z(zhat)\n",
    "            preds_y.append(yhat.astype(np.float32))\n",
    "            trues_y.append(yb.numpy().squeeze(-1).astype(np.float32))\n",
    "    y_pred = np.concatenate(preds_y); y_true = np.concatenate(trues_y)\n",
    "    ss_res = float(((y_true - y_pred)**2).sum())\n",
    "    ss_tot = float(((y_true - y_true.mean())**2).sum() + 1e-12)\n",
    "    r2   = 1.0 - ss_res/ss_tot\n",
    "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    print(f\"[{name}] R² = {r2:.4f} | MAE = {mae:.4f} | RMSE = {rmse:.4f}\")\n",
    "    return y_true, y_pred, r2, mae, rmse\n",
    "\n",
    "print(\"\\nPerformance (against observed y):\")\n",
    "y_tr, yhat_tr, r2_tr, _, _ = metrics_from_loader(\"Train\",      train_loader)\n",
    "y_va, yhat_va, r2_va, _, _ = metrics_from_loader(\"Validation\", val_loader)\n",
    "y_te, yhat_te, r2_te, _, _ = metrics_from_loader(\"Test\",       test_loader)\n",
    "\n",
    "# Noise ceiling on Test (label noise only)\n",
    "r2_ceiling_test = 1.0 - float(((y_true_test - y_te)**2).sum()) / float(((y_te - y_te.mean())**2).sum() + 1e-12)\n",
    "print(f\"\\n[Reference] Test noise‑ceiling R² (label noise): {r2_ceiling_test:.4f}\\n\")\n",
    "\n",
    "# ----- Plots\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_losses, label=\"Train loss\"); plt.plot(val_losses, label=\"Val loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training curves (CNN — ring prior + starness)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_pred_vs_actual(y_actual, y_pred, title):\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(y_actual, y_pred, s=14, alpha=0.75)\n",
    "    lo = min(float(np.min(y_actual)), float(np.min(y_pred)))\n",
    "    hi = max(float(np.max(y_actual)), float(np.max(y_pred)))\n",
    "    pad = 0.02*(hi - lo) if hi > lo else 0.01\n",
    "    plt.plot([lo - pad, hi + pad], [lo - pad, hi + pad], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"Actual y\"); plt.ylabel(\"Predicted y\"); plt.title(title)\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_pred_vs_actual(y_tr, yhat_tr, \"Predicted vs Actual (Train, CNN)\")\n",
    "plot_pred_vs_actual(y_va, yhat_va, \"Predicted vs Actual (Validation, CNN)\")\n",
    "plot_pred_vs_actual(y_te, yhat_te, \"Predicted vs Actual (Test, CNN)\")\n",
    "\n",
    "# Small test panel (original → preprocessed → prediction)\n",
    "n_show = min(6, len(X_test))\n",
    "sel = np.random.choice(len(X_test), size=n_show, replace=False)\n",
    "fig, axes = plt.subplots(3, n_show, figsize=(2.0*n_show, 5.8))\n",
    "for j, k in enumerate(sel):\n",
    "    axes[0, j].imshow(X_test[k],   cmap=\"gray\", vmin=0, vmax=1); axes[0, j].set_title(\"Original\");      axes[0, j].axis(\"off\")\n",
    "    axes[1, j].imshow(X_test_s[k], cmap=\"gray\", vmin=0, vmax=1); axes[1, j].set_title(\"Bg‑subtracted\"); axes[1, j].axis(\"off\")\n",
    "    axes[2, j].text(0.02, 0.80, f\"y_true≈{y_true_test[k]:.3f}\", transform=axes[2, j].transAxes)\n",
    "    axes[2, j].text(0.02, 0.55, f\"y_obs ={y_test[k]:.3f}\",      transform=axes[2, j].transAxes)\n",
    "    axes[2, j].text(0.02, 0.30, f\"y_hat ={yhat_te[k]:.3f}\",     transform=axes[2, j].transAxes)\n",
    "    axes[2, j].axis(\"off\")\n",
    "axes[0,0].set_ylabel(\"Input\"); axes[1,0].set_ylabel(\"Preproc\"); axes[2,0].set_ylabel(\"Labels\")\n",
    "plt.suptitle(\"CNN predictions on held‑out test samples\", fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Takeaways:\")\n",
    "print(\"  • Ring priors tell the model *where* to look; the starness branch makes a tiny bright cue dominate via max pooling.\")\n",
    "print(\"  • A global-mean map gives an easy path to the overall scale so capacity focuses on local features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "# Deep SHAP on linear and CNN models\n",
    "\n",
    "**Purpose of this cell**  \n",
    "Explain and validate model attributions for three regressors trained on the hollow‑sphere images:\n",
    "\n",
    "- **OLS**: Ordinary Least Squares on flattened pixels.  \n",
    "- **Ridge**: Standardised pixels + L2‑regularised linear regression.  \n",
    "- **CNN**: 4‑channel input (image + $x$, $y$, $r$) with a ring‑aware, star‑sensitive architecture.\n",
    "\n",
    "The analysis produces SHAP/Deep SHAP maps, compares baselines, quantifies attribution fidelity (deletion/AOPC), checks where the explanations sit relative to the **ground‑truth shell**, and demonstrates whether the CNN **locks onto the star** globally. The notes below aim to make each step interpretable and reproducible.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Quick refresher: what SHAP values mean\n",
    "\n",
    "A SHAP value $\\phi_i(f,x)$ reports the contribution of feature $i$ to the prediction $f(x)$ **relative to a baseline** $x'$. In the classical Shapley formulation (from cooperative game theory), for a model with feature set $F$ and $M=|F|$ features,\n",
    "$$\n",
    "\\phi_i(f,x) \\;=\\; \\sum_{S \\subseteq F\\setminus\\{i\\}}\\frac{|S|!\\,\\big(M-|S|-1\\big)!}{M!}\n",
    "\\;\\Big(f_{S\\cup\\{i\\}}(x) - f_S(x)\\Big),\n",
    "$$\n",
    "where $f_S$ is the model evaluated when only the subset $S$ is “present” and all other features are clamped to the baseline. Two properties guide interpretation:\n",
    "\n",
    "- **Local accuracy (additivity).**  \n",
    "  $$ f(x) = f(x') + \\sum_{i=1}^{M}\\phi_i(f,x). $$\n",
    "\n",
    "- **Symmetry/efficiency.** Features that influence $f$ identically receive equal credit; total credit equals the prediction difference $f(x)-f(x')$.\n",
    "\n",
    "In images, “features” are often **pixels** or **channels×pixels**. The baseline $x'$ is typically a *background distribution* (e.g., a set of training images) or a single reference (e.g., the mean image).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Why three different explainers are used\n",
    "\n",
    "### 2.1 Linear models (OLS, Ridge) — exact, fast SHAP\n",
    "For linear regression $f(x)=w^\\top x + b$ with a baseline distribution $X'$ such that $\\mathbb{E}[X']=\\bar x$, the **interventional** SHAP values reduce to\n",
    "$$\n",
    "\\phi_i(f,x) = w_i\\,(x_i-\\bar x_i), \\quad \\text{and}\\quad f(x')=w^\\top\\bar x + b.\n",
    "$$\n",
    "Hence SHAP for OLS and Ridge is exact under the chosen baseline, and **LinearExplainer** is both fast and faithful.\n",
    "\n",
    "**Intuition.** Hot pixels on the ring receive large positive (or negative) $\\phi$ if the corresponding coefficient $w_i$ is large in magnitude; zero‑mean standardisation in Ridge changes the scale but not the location of these attributions.\n",
    "\n",
    "### 2.2 CNN — Deep SHAP (DeepExplainer)\n",
    "Deep SHAP for PyTorch approximates Shapley values via **DeepLIFT‑style** rules and path integrations from a baseline $x'$ to the input $x$. One useful mental model is *Integrated Gradients* (IG) with multiple baselines $\\{x'^{(b)}\\}$:\n",
    "$$\n",
    "\\phi_i^{\\text{IG}}(f,x;x') \\approx (x_i-x'_i)\\int_0^1 \n",
    "\\frac{\\partial f\\big(x'+t(x-x')\\big)}{\\partial x_i}\\,dt,\n",
    "\\qquad\n",
    "\\phi_i^{\\text{DeepSHAP}}(f,x) \\approx \\frac{1}{B}\\sum_{b=1}^B \\phi_i^{\\text{IG}}(f,x;x'^{(b)}).\n",
    "$$\n",
    "DeepExplainer applies exact rules on piecewise‑linear parts and approximations on others. Two practical adjustments are made in the code to keep it robust:\n",
    "\n",
    "1. **SHAP‑compatible wrapper** (`SHAPCompatCNN`). This removes `nn.Flatten` as a module, disables in‑place ReLUs, and copies the two linear layers explicitly, so the graph stays simple and traceable.\n",
    "2. **`check_additivity=False`.** This disables strict additivity checks, which can fail with non‑supported ops (e.g., `sqrt` in the fixed Sobel stem) or small numerical drift. Additivity can still be inspected manually by summing attributions and comparing to $f(x)-f(x')$.\n",
    "\n",
    "**Baseline choice matters.** A poor baseline can make any attribution method look inconsistent. This cell *evaluates several baselines* and selects one by fidelity metrics (see §6).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Preliminaries and splits\n",
    "\n",
    "- **Device.** CUDA if available, otherwise Apple MPS, otherwise CPU.  \n",
    "- **Warnings.** Harmless SHAP warnings about unrecognised modules (e.g., `ReflectionPad2d`, the custom `ConcatPool2d`) are suppressed to keep the output clean.  \n",
    "- **Splits.** The same 64/16/20 Train/Val/Test split as earlier cells ensures metric comparability. A fixed `rng` seed stabilises sampling for background sets and the test subset used for explanations.\n",
    "\n",
    "A small **test subset** (default 24 images) is sampled from the Test split to keep the SHAP runs snappy while remaining representative for visualisation and metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Common geometry and regional metrics\n",
    "\n",
    "Three radial regions are defined using the known sphere geometry:\n",
    "- **Hollow**: $r \\le 18$ px.  \n",
    "- **Shell**: $18 < r \\le 42$ px.  \n",
    "- **Outside**: $r > 42$ px.\n",
    "\n",
    "Given any attribution map $S\\in\\mathbb{R}^{H\\times W}$, the analysis reports:\n",
    "\n",
    "- **Region shares** (area‑biased): fraction of $\\sum|S|$ that falls in each region.  \n",
    "- **Region means** (area‑fair): mean $|S|$ per pixel in each region.  \n",
    "- **Top‑$p$ concentration**: the fraction of total $|S|$ contained in the top $p\\%$ pixels by $|S|$. High concentration indicates focus.\n",
    "\n",
    "A **radial profile** $\\bar s(r)$ is also shown by binning pixels into $r$‑bands and averaging $|S|$ within each band. Expect peaks near the inner/outer shell edges for models that key off shell structure.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good* (with star): high share/mean in the **shell**, high **top‑1% concentration** (evidence of a tight cue), and a radial profile with sharp peaks near the shell radius.  \n",
    "- *Bad*: substantial attribution outside the shell or a flat radial profile (model not localising the physics).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Linear explainers (OLS and Ridge)\n",
    "\n",
    "### 5.1 Background sets\n",
    "- **OLS.** Background = a random subset of flattened train images; this sets $\\bar x$ in the linear SHAP formula.  \n",
    "- **Ridge.** The model operates on **standardised** features $Z=(X-\\mu)/\\sigma$. SHAP is computed in $Z$‑space and reshaped back to image‐space. The map reflects $w_{\\text{ridge}}$ scaled by the feature z‑scores.\n",
    "\n",
    "### 5.2 Expected qualitative results\n",
    "- OLS: crisp ring attributions; may over‑fit noise in Train if $p\\gg n$.  \n",
    "- Ridge: ring remains but is smoother; coefficients are shrunk toward zero, so diffuse patterns appear.\n",
    "\n",
    "**Fidelity check.** *Deletion curves* (see §7) should show a noticeable drop in $R^2$ once the top 1–5% of pixels by $|$SHAP$|$ are removed or replaced by baseline values.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) CNN attributions — baselines and selection by fidelity\n",
    "\n",
    "### 6.1 Building CNN inputs for SHAP\n",
    "Inputs follow the **training recipe** exactly: corner median background subtraction, train‑statistics normalisation, and appending the three CoordConv maps $(x,y,r)$. A mismatch here is the most common source of odd attributions.\n",
    "\n",
    "### 6.2 Candidate baselines\n",
    "Four baselines are considered:\n",
    "1. **Train subset**: $m$ random training samples (recommended).  \n",
    "2. **Mean image**: single image channel = train mean; $(x,y,r)$ channels kept as deterministic maps.  \n",
    "3. **Median image**: robust to outliers.  \n",
    "4. **Blurred mean**: low‑pass filtered mean; often improves stability.\n",
    "\n",
    "### 6.3 How the “best” baseline is chosen\n",
    "Three fidelity signals are computed for each candidate:\n",
    "\n",
    "- **Shell IoU/Precision** at $k\\in\\{1\\%,5\\%\\}$ of top $|$SHAP$|$ pixels:  \n",
    "  $$\n",
    "  \\text{IoU}=\\frac{|M_k\\cap \\text{Shell}|}{|M_k\\cup \\text{Shell}|},\\qquad\n",
    "  \\text{Precision}=\\frac{|M_k\\cap \\text{Shell}|}{|M_k|}.\n",
    "  $$\n",
    "\n",
    "- **Deletion AOPC** (area over the performance curve): remove the top‑$k$% $|$SHAP$|$ pixels in the **image** channel (keeping $(x,y,r)$ intact), recompute $R^2(k)$ and integrate the drop:\n",
    "  $$\n",
    "  \\mathrm{AOPC}=\\int_{k\\in\\mathcal{K}}\\big(R^2(0)-R^2(k)\\big)\\,dk.\n",
    "  $$\n",
    "\n",
    "A simple normalised score combines IoU@1%, IoU@5% and AOPC, and the baseline with the largest score is chosen. This is a pragmatic proxy for *faithful, focused* explanations on this dataset.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good baseline*: high IoU/Precision against the shell and large AOPC (removing top‑|SHAP| harms performance quickly).  \n",
    "- *Bad baseline*: low IoU and small AOPC; attributions likely smear over background or contradict physics.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Deletion curves (OLS, Ridge, CNN)\n",
    "\n",
    "**Purpose.** Test whether high‑magnitude SHAP pixels are **causally important** for prediction.\n",
    "\n",
    "- For OLS/Ridge, the top‑$k$% features by $|$SHAP$|$ are replaced by baseline values (mean or zero in the standardised space).  \n",
    "- For the CNN, only the **image channel** is masked at those pixels (the coordinate channels remain untouched).\n",
    "\n",
    "The metric reported is the **$R^2$ against observed labels** on the explained subset as a function of $k$. The summarising scalar is **AOPC** (larger is better).\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good*: $R^2$ drops sharply by removing 1–5% of top pixels; large AOPC.  \n",
    "- *Bad*: flat $R^2$ curve; indicates unfaithful or noisy attributions.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Bounding‑box analysis on top‑|SHAP| within the shell\n",
    "\n",
    "**Goal.** Quantify how **compact** the attribution is within the shell, and whether it behaves like a local cue (e.g., the star).\n",
    "\n",
    "Procedure per image:\n",
    "1. Select the **top $p\\%$** ($p=1$% by default) pixels by $|$SHAP$|$ **within the shell**.  \n",
    "2. Extract the **largest weighted connected component** (8‑connectivity; weights = $|$SHAP$|$).  \n",
    "3. Compute metrics for the tight axis‑aligned bounding box around that component:\n",
    "\n",
    "   - **Area fraction of shell**:  \n",
    "     $$\\text{AreaFrac}=\\frac{|\\,\\text{Box}\\cap\\text{Shell}\\,|}{|\\,\\text{Shell}\\,|}.$$\n",
    "   - **|SHAP| share captured by box**:  \n",
    "     $$\\text{Share}=\\frac{\\sum_{(i,j)\\in\\text{Box}\\cap\\text{Shell}}|S_{ij}|}{\\sum_{(i,j)\\in\\text{Shell}}|S_{ij}|}.$$\n",
    "   - **Angular coverage** (degrees) of selected pixels using circular geometry.  \n",
    "   - **Radial width** normalised by shell thickness.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good (star dataset)*: **small** AreaFrac, **large** Share (most attribution captured by a small box), **low** angular coverage, and **narrow** radial width.  \n",
    "- *Bad*: large boxes capturing little attribution, or angular coverage approaching a full ring (360°).\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Global star‑capture analysis\n",
    "\n",
    "This section verifies that the model focuses on the star **globally**, not only via per‑image boxes.\n",
    "\n",
    "1. **Star proxy centre.** For each image, compute a ring‑weighted, high‑tail map and pick its maximum as a proxy star centre $(y_s,x_s)$.  \n",
    "2. **Star disc mask.** $D=\\{(i,j): (i-y_s)^2+(j-x_s)^2 \\le r^2\\}$ with $r=4$ px.  \n",
    "3. **SHAP share within disc** (normalised by shell attribution):\n",
    "   $$\n",
    "   \\text{Share}_{\\text{disc}}=\\frac{\\sum_{(i,j)\\in D\\cap\\text{Shell}}|S_{ij}|}{\\sum_{(i,j)\\in \\text{Shell}}|S_{ij}|}.\n",
    "   $$\n",
    "4. **Enrichment** relative to the area fraction of the disc:  \n",
    "   $$\n",
    "   \\text{Enrichment}=\\frac{\\text{Share}_{\\text{disc}}}{|D\\cap\\text{Shell}|/|\\text{Shell}|}.\n",
    "   $$\n",
    "5. **Angular alignment** of the largest component vs the star angle $\\theta_s$: compute a circular mean angle of the component weighted by $|S|$ and report $|\\theta_{\\text{comp}}-\\theta_s|$ in degrees.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good (CNN on star dataset)*: high **Share**, **Enrichment $\\gg 1$**, and **small** angular error.  \n",
    "- *Bad*: Share near area fraction (no enrichment) or large angular errors, similar to OLS/Ridge behaviour.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Baseline sensitivity for the CNN\n",
    "\n",
    "Deep SHAP is baseline‑conditional. The analysis compares attributions with a **train‑subset** background vs a **mean** background and reports the **Spearman rank correlation** between $|$SHAP$|$ maps per image.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good*: moderate to high rank correlation (e.g., median $\\gtrsim 0.6$), indicating stability across sensible baselines.  \n",
    "- *Bad*: near‑zero or highly variable correlations, suggesting the baseline selection dominates the explanation.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Polar superpixels / regional analysis\n",
    "\n",
    "Pixels are binned into $(r,\\theta)$ sectors (e.g., 10 radial bands × 16 angular bins). For each sector, the mean $|$SHAP$|$ is recorded. Two descriptors are shown:\n",
    "\n",
    "- **Peak ring location** (which radial band holds the largest mean attribution).  \n",
    "- **Anisotropy coefficient of variation** within that band:\n",
    "  $$\n",
    "  \\text{CV}=\\frac{\\operatorname{std}_\\theta(\\text{mean}|S|)}{\\operatorname{mean}_\\theta(\\text{mean}|S|)}.\n",
    "  $$\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good (star dataset)*: peak on the shell bands and **higher** CV (attribution concentrated into arcs aligned with the star).  \n",
    "- *Bad*: flat CV near zero (isotropic ring), typical of purely global linear cues.\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Augmentation‑invariance sanity check (horizontal flips)\n",
    "\n",
    "Inputs are flipped horizontally, explained, then unflipped back in attribution space. The **Spearman rank correlation** between original and flipped‑then‑unflipped $|$SHAP$|$ maps is reported per image.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good*: high correlation (the explanation respects the flip symmetry).  \n",
    "- *Bad*: unstable attributions under symmetry transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## 13) Integrated Gradients (IG) as triangulation\n",
    "\n",
    "To cross‑check Deep SHAP, **Integrated Gradients** is computed from a simple baseline $\\tilde x$ (image channel zeroed; coordinate channels intact):\n",
    "$$\n",
    "\\mathrm{IG}_i(x; \\tilde x)=(x_i-\\tilde x_i)\\int_0^1 \\frac{\\partial f\\big(\\tilde x + t(x-\\tilde x)\\big)}{\\partial x_i}\\,dt.\n",
    "$$\n",
    "A Spearman correlation between $|\\mathrm{IG}|$ and $|$SHAP$|$ per image is reported.\n",
    "\n",
    "**Good vs bad.**\n",
    "- *Good*: positive, often substantial correlation, indicating agreement between two attribution methods with different assumptions.  \n",
    "- *Bad*: no correlation; revisit baseline choice and preprocessing consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## 14) Interpreting typical outputs on this dataset\n",
    "\n",
    "- **Linear models (OLS/Ridge).** Expect ring‑shaped attributions. Deletion curves drop, but not catastrophically at 1–5%. Bounding boxes are large; star‑disc enrichment near 1.  \n",
    "- **CNN (hard/star dataset).** Expect small bounding boxes, large |SHAP| share captured, low angular coverage; high star‑disc enrichment and small angular error. Deletion AOPC usually largest among the three.  \n",
    "- **CNN (easy/no‑star dataset).** Attributions move to the shell edges and global scale; star‑specific metrics are not applicable or show no enrichment.\n",
    "\n",
    "Numerically, on a healthy run it is common to observe:  \n",
    "- IoU@5% vs shell: CNN $>$ Ridge $\\gtrsim$ OLS.  \n",
    "- AOPC (CNN) $>$ AOPC (Ridge) $>$ AOPC (OLS).  \n",
    "- Star‑disc enrichment: CNN $\\gg 1$, OLS/Ridge $\\approx 1$.\n",
    "\n",
    "---\n",
    "\n",
    "## 15) Practical pitfalls and remedies\n",
    "\n",
    "- **Baseline mismatch** between training and SHAP inputs → nonsensical maps. Always rebuild inputs with the *same* background subtraction and normalisation.  \n",
    "- **Non‑supported ops** (e.g., `sqrt` from Sobel magnitude) can violate strict additivity checks. Using `check_additivity=False` avoids hard failures; manual additivity checks remain possible.  \n",
    "- **Floating‑point quirks on MPS/CUDA** can surface as non‑finite losses; the training code already defends by reducing the learning rate and skipping the offending step.  \n",
    "- **Over‑aggressive masking** in deletion curves can change the data distribution drastically; restricting masking to the **image channel** (and retaining $(x,y,r)$) keeps the test closer to “remove visual evidence”.\n",
    "\n",
    "---\n",
    "\n",
    "## 16) A short glossary\n",
    "\n",
    "- **Baseline (background).** The reference input(s) $x'$ relative to which contributions are measured.  \n",
    "- **Deep SHAP.** DeepExplainer’s adaptation of Shapley ideas to deep networks, mixing DeepLIFT rules and path integrations.  \n",
    "- **AOPC.** “Area Over the (performance) Curve” after progressively deleting top‑|SHAP| pixels; larger indicates more faithful attributions.  \n",
    "- **IoU / Precision vs shell.** Overlap metrics between top‑|SHAP| pixels and the known shell region.  \n",
    "- **Largest weighted component.** The connected subset within top‑|SHAP| that has the greatest total $|$SHAP$|$; used to avoid fragmented boxes.  \n",
    "- **Enrichment.** Share of $|$SHAP$|$ inside the star disc divided by the star disc’s area fraction of the shell. Values $\\gg1$ indicate star focus.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom line.** On this synthetic problem, linear models explain the **global density** via ring‑like weights; the CNN explains **both** global scale and **local star cues**. The attribution tests here (baseline selection, deletion/AOPC, IoU/Precision, bounding boxes, global star capture, flips, and IG triangulation) provide a comprehensive, quantitative argument that the CNN’s predictions are driven by the intended visual evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep SHAP — End‑to‑end, commented, and *interpretable* analysis on our three models\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "B. Models explained:\n",
    "  • OLS (pixels → y), Ridge (StandardScaler + Ridge → y), and CNN (4‑ch input: image + x,y,r; predicts z then inverted to y).\n",
    "  • CNN is made SHAP‑safe by:\n",
    "      – a SHAPCompatCNN wrapper that removes nn.Flatten and any in‑place ReLUs,\n",
    "      – using check_additivity=False (via safe_shap_values) to tolerate unsupported ops (e.g., sqrt in fixed stems).\n",
    "\n",
    "Changes in this version:\n",
    "  • Insertion curves removed (kept Deletion/AOPC).\n",
    "  • FIX: bounding‑box analysis is now robust (no negative kth; handles degenerate masks; always 2‑D indexing).\n",
    "  • NEW: before boxing we take the **largest weighted connected component** of the top‑|SHAP| pixels within the shell.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------\n",
    "# Imports and checks\n",
    "# -----------------\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# ----- Device (M1 → MPS if available)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Silence SHAP's \"unrecognized nn.Module\" warnings for harmless modules (ConcatPool2d / ReflectionPad2d)\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"unrecognized nn\\.Module: .*\",\n",
    "    category=UserWarning,\n",
    "    module=\"shap.explainers._deep.deep_pytorch\"\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install SHAP first:  pip install shap\") from e\n",
    "\n",
    "# Optional Spearman correlation (baseline sensitivity); fall back to Pearson if unavailable\n",
    "try:\n",
    "    from scipy.stats import spearmanr\n",
    "    HAVE_SPEARMAN = True\n",
    "except Exception:\n",
    "    HAVE_SPEARMAN = False\n",
    "\n",
    "# Sanity: ensure earlier cells provided these\n",
    "required = [\"images\", \"y_true_all\", \"y_obs_all\", \"ols\", \"final_ridge\", \"model\"]\n",
    "for v in required:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Missing '{v}'. Please run the data, OLS/Ridge, and CNN training cells first.\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(DEVICE).eval()\n",
    "torch.set_grad_enabled(True)  # in case a previous cell disabled grad\n",
    "\n",
    "N, H, W = images.shape\n",
    "CENTER = (H - 1) / 2.0\n",
    "print(f\"[Setup] images: {images.shape} | device: {DEVICE}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Split indices (SAME recipe as earlier; seed=2025)\n",
    "# -----------------------------\n",
    "SEED = 2025\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC_WITHIN_TRAINVAL = 0.20  # 20% of 80% → 16% overall\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "idx_all = np.arange(N)\n",
    "\n",
    "# First split: TrainVal vs Test\n",
    "idx_trainval, idx_test = train_test_split(idx_all, test_size=TEST_FRAC, random_state=SEED)\n",
    "# Second split: Train vs Val (within TrainVal)\n",
    "idx_train, idx_val = train_test_split(idx_trainval, test_size=VAL_FRAC_WITHIN_TRAINVAL, random_state=SEED)\n",
    "\n",
    "print(f\"[Split] Train={len(idx_train)}, Val={len(idx_val)}, Test={len(idx_test)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Utility helpers\n",
    "# -----------------------------\n",
    "def area_trapezoid(y, x):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    return float(np.sum(0.5 * (y[1:] + y[:-1]) * (x[1:] - x[:-1])))\n",
    "\n",
    "def heatmap2d(arr2d, title, cmap=\"magma\", with_cbar=True):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(arr2d, cmap=cmap)\n",
    "    if with_cbar:\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.title(title); plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "def grid_overlays(imgs, shap_maps, title_prefix, ncols=4):\n",
    "    n = len(imgs)\n",
    "    ncols = min(ncols, n); nrows = int(np.ceil(n / ncols))\n",
    "    vmax = np.percentile(np.abs(np.concatenate([s.ravel() for s in shap_maps])), 99) if len(shap_maps) else 1.0\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3.2 * ncols, 3.2 * nrows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "    for i in range(n):\n",
    "        axes[i].imshow(imgs[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[i].imshow(shap_maps[i], cmap=\"RdBu_r\", alpha=0.7, vmin=-vmax, vmax=+vmax)\n",
    "        axes[i].set_title(f\"{title_prefix} — sample {i}\", fontsize=9)\n",
    "        axes[i].axis(\"off\")\n",
    "    for k in range(n, nrows * ncols): axes[k].axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Radial & angular fields + “region share” metrics\n",
    "# -----------------------------\n",
    "ys, xs = np.indices((H, W))\n",
    "R = np.sqrt((ys - CENTER) ** 2 + (xs - CENTER) ** 2)\n",
    "theta = (np.arctan2(ys - CENTER, xs - CENTER) + 2*np.pi) % (2*np.pi)  # used by polar & bbox\n",
    "\n",
    "INNER_R = 18.0\n",
    "OUTER_R = 42.0\n",
    "mask_inner = (R <= INNER_R)  # hollow\n",
    "mask_shell = (R > INNER_R) & (R <= OUTER_R)  # metal shell\n",
    "mask_outer = (R > OUTER_R)  # outside\n",
    "\n",
    "def region_shares(abs_shap_map):\n",
    "    total = abs_shap_map.sum() + 1e-12\n",
    "    return (\n",
    "        float((abs_shap_map[mask_inner]).sum() / total),\n",
    "        float((abs_shap_map[mask_shell]).sum() / total),\n",
    "        float((abs_shap_map[mask_outer]).sum() / total),\n",
    "    )\n",
    "\n",
    "def region_means(abs_shap_map):\n",
    "    mi = float(abs_shap_map[mask_inner].mean() if mask_inner.sum() else 0.0)\n",
    "    ms = float(abs_shap_map[mask_shell].mean() if mask_shell.sum() else 0.0)\n",
    "    mo = float(abs_shap_map[mask_outer].mean() if mask_outer.sum() else 0.0)\n",
    "    return mi, ms, mo\n",
    "\n",
    "def top_p_concentration(abs_shap_map, p=0.05):\n",
    "    A = np.abs(abs_shap_map).ravel()\n",
    "    k = max(1, int(p * A.size))\n",
    "    kth = A.size - k  # non‑negative kth\n",
    "    if kth < 0: kth = 0\n",
    "    thr = np.partition(A, kth)[kth]\n",
    "    return float((A[A >= thr]).sum() / (A.sum() + 1e-12))\n",
    "\n",
    "def radial_profile(abs_shap_map, nbins=60):\n",
    "    r = R.ravel(); v = abs_shap_map.ravel()\n",
    "    bins = np.linspace(0, R.max() + 1e-6, nbins + 1)\n",
    "    prof = np.zeros(nbins, dtype=float)\n",
    "    for i in range(nbins):\n",
    "        m = (r >= bins[i]) & (r < bins[i + 1])\n",
    "        prof[i] = v[m].mean() if np.any(m) else 0.0\n",
    "    centres = 0.5 * (bins[:-1] + bins[1:])\n",
    "    return centres, prof\n",
    "\n",
    "def summarise_region_and_profile(stack, model_name):\n",
    "    shares = np.array([region_shares(np.abs(s)) for s in stack])\n",
    "    inner_s, shell_s, outer_s = shares.mean(axis=0)\n",
    "\n",
    "    means = np.array([region_means(np.abs(s)) for s in stack])\n",
    "    inner_m, shell_m, outer_m = means.mean(axis=0)\n",
    "\n",
    "    conc_1 = np.mean([top_p_concentration(np.abs(s), p=0.01) for s in stack])\n",
    "    conc_5 = np.mean([top_p_concentration(np.abs(s), p=0.05) for s in stack])\n",
    "\n",
    "    print(f\"\\n{model_name} — mean |SHAP| region *share* (area‑biased):\")\n",
    "    print(f\"  Hollow (r ≤ {INNER_R:.0f}) : {inner_s:6.2%}\")\n",
    "    print(f\"  Shell  ({INNER_R:.0f}<r≤{OUTER_R:.0f}): {shell_s:6.2%}\")\n",
    "    print(f\"  Outside(r > {OUTER_R:.0f}) : {outer_s:6.2%}\")\n",
    "\n",
    "    print(f\"{model_name} — mean |SHAP| *per pixel* (area‑fair):\")\n",
    "    print(f\"  Hollow:  {inner_m:.6f}  |  Shell: {shell_m:.6f}  |  Outside: {outer_m:.6f}\")\n",
    "    print(f\"{model_name} — concentration: top‑1% captures {conc_1:6.2%}, top‑5% captures {conc_5:6.2%}\")\n",
    "\n",
    "    profs, radii = [], None\n",
    "    for s in stack:\n",
    "        rr, p = radial_profile(np.abs(s), nbins=70)\n",
    "        profs.append(p); radii = rr\n",
    "    profs = np.stack(profs, axis=0)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(radii, profs.mean(axis=0))\n",
    "    plt.xlabel(\"Radius (px)\"); plt.ylabel(\"Mean |SHAP|\")\n",
    "    plt.title(f\"Radial profile of |SHAP| — {model_name}\\n(look for peaks around the shell boundaries)\")\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "def topk_mask(abs_map, k_frac):\n",
    "    A = np.abs(abs_map).ravel()\n",
    "    k = max(1, int(k_frac * A.size))\n",
    "    kth = A.size - k\n",
    "    if kth < 0: kth = 0\n",
    "    thr = np.partition(A, kth)[kth]\n",
    "    return (np.abs(abs_map) >= thr)\n",
    "\n",
    "def iou_and_precision_vs_shell(stack, k_fracs=(0.01, 0.02, 0.05, 0.10)):\n",
    "    results = {}; shell = mask_shell\n",
    "    for kf in k_fracs:\n",
    "        ious, precs = [], []\n",
    "        for s in stack:\n",
    "            m = topk_mask(s, kf).reshape(H, W)\n",
    "            inter = np.logical_and(m, shell).sum()\n",
    "            union = np.logical_or(m, shell).sum()\n",
    "            iou = inter / (union + 1e-12)\n",
    "            prec = inter / (m.sum() + 1e-12)\n",
    "            ious.append(iou); precs.append(prec)\n",
    "        results[kf] = (float(np.mean(ious)), float(np.mean(precs)))\n",
    "    return results\n",
    "\n",
    "def print_iou_precision(name, results):\n",
    "    print(f\"\\n{name} — Ground‑truth alignment vs shell (IoU@k / Precision@k):\")\n",
    "    for kf, (iou, prec) in results.items():\n",
    "        print(f\"  k={int(kf * 100)}%  IoU={iou:.3f}  |  Precision={prec:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# A common test subset to *explain*\n",
    "# -----------------------------\n",
    "N_EXPLAIN = min(24, len(idx_test))\n",
    "test_sel_idx = rng.choice(idx_test, size=N_EXPLAIN, replace=False)\n",
    "test_imgs = images[test_sel_idx]\n",
    "y_obs_test_sel = y_obs_all[test_sel_idx]\n",
    "\n",
    "# ==========================================================\n",
    "# 1) OLS — LinearExplainer on flattened pixels\n",
    "# ==========================================================\n",
    "print(\"\\n[OLS] Expect a ring; most |SHAP| in the shell.\")\n",
    "X_flat = images.reshape(N, -1).astype(np.float64)\n",
    "\n",
    "bg_ols_idx = rng.choice(idx_train, size=min(200, len(idx_train)), replace=False)\n",
    "X_bg_ols = X_flat[bg_ols_idx]\n",
    "X_ols_test = X_flat[test_sel_idx]\n",
    "\n",
    "expl_ols = shap.LinearExplainer(ols, X_bg_ols)  # identity link\n",
    "shap_ols_flat = expl_ols.shap_values(X_ols_test)\n",
    "if isinstance(shap_ols_flat, list):  # defensive\n",
    "    shap_ols_flat = shap_ols_flat[0]\n",
    "shap_ols_imgs = shap_ols_flat.reshape(-1, H, W)\n",
    "\n",
    "grid_overlays(test_imgs[:8], shap_ols_imgs[:8], \"OLS\")\n",
    "heatmap2d(np.mean(np.abs(shap_ols_imgs), axis=0), \"Global mean |SHAP| — OLS\")\n",
    "summarise_region_and_profile(shap_ols_imgs, \"OLS\")\n",
    "print_iou_precision(\"OLS\", iou_and_precision_vs_shell(shap_ols_imgs))\n",
    "\n",
    "# ==========================================================\n",
    "# 2) Ridge — LinearExplainer on STANDARDISED features\n",
    "# ==========================================================\n",
    "print(\"\\n[Ridge] Expect more diffused credit spread; still ring‑biased.\")\n",
    "if not isinstance(final_ridge, Pipeline):\n",
    "    raise RuntimeError(\"Expected 'final_ridge' to be a Pipeline(StandardScaler, Ridge).\")\n",
    "scaler = final_ridge.named_steps[\"scaler\"]\n",
    "ridge  = final_ridge.named_steps[\"ridge\"]\n",
    "\n",
    "Z_bg   = scaler.transform(X_flat[bg_ols_idx])\n",
    "Z_test = scaler.transform(X_flat[test_sel_idx])\n",
    "\n",
    "expl_ridge = shap.LinearExplainer(ridge, Z_bg)\n",
    "shap_ridge_scaled = expl_ridge.shap_values(Z_test)\n",
    "if isinstance(shap_ridge_scaled, list):\n",
    "    shap_ridge_scaled = shap_ridge_scaled[0]\n",
    "shap_ridge_imgs = shap_ridge_scaled.reshape(-1, H, W)\n",
    "\n",
    "grid_overlays(test_imgs[:8], shap_ridge_imgs[:8], \"Ridge\")\n",
    "heatmap2d(np.mean(np.abs(shap_ridge_imgs), axis=0), \"Global mean |SHAP| — Ridge\")\n",
    "summarise_region_and_profile(shap_ridge_imgs, \"Ridge\")\n",
    "print_iou_precision(\"Ridge\", iou_and_precision_vs_shell(shap_ridge_imgs))\n",
    "\n",
    "# ==========================================================\n",
    "# 3) CNN — DeepExplainer on 4‑channel inputs (image + CoordConv x,y,r)\n",
    "#      SHAP‑compatible wrapper (no nn.Flatten; no in‑place ops); additivity off\n",
    "# ==========================================================\n",
    "print(\"\\n[CNN] Strong edges on inner/outer shell expected; baseline matters — we select it by fidelity.\")\n",
    "\n",
    "# --- preprocessing used for CNN inputs (match the training recipe) ---\n",
    "def estimate_background_offset(img: np.ndarray, k: int = 10) -> float:\n",
    "    patches = [img[:k, :k], img[:k, -k:], img[-k:, :k], img[-k:, -k:]]\n",
    "    return float(np.median(np.concatenate([p.ravel() for p in patches])))\n",
    "\n",
    "def subtract_background(X: np.ndarray) -> np.ndarray:\n",
    "    Xp = np.empty_like(X)\n",
    "    for i, img in enumerate(X):\n",
    "        bg = estimate_background_offset(img, k=10)\n",
    "        im = img - bg\n",
    "        im[im < 0.0] = 0.0\n",
    "        Xp[i] = im\n",
    "    return Xp\n",
    "\n",
    "pm_ps_found = False\n",
    "if \"pix_mean\" in globals() and \"pix_std\" in globals():\n",
    "    pm = float(pix_mean); ps = float(pix_std); pm_ps_found = True\n",
    "elif \"pixel_mean\" in globals() and \"pixel_std\" in globals():\n",
    "    pm = float(pixel_mean); ps = float(pixel_std); pm_ps_found = True\n",
    "if not pm_ps_found:\n",
    "    X_train_bgsub = subtract_background(images[idx_train])\n",
    "    pm = float(X_train_bgsub.mean()); ps = float(X_train_bgsub.std() + 1e-8)\n",
    "\n",
    "# CoordConv maps\n",
    "x_lin = (xs - CENTER) / CENTER\n",
    "y_lin = (ys - CENTER) / CENTER\n",
    "r_map = np.sqrt(x_lin ** 2 + y_lin ** 2)\n",
    "r_map = r_map / (r_map.max() + 1e-12)\n",
    "\n",
    "def build_cnn_input(img_batch: np.ndarray) -> np.ndarray:\n",
    "    Xs = subtract_background(img_batch)\n",
    "    Xn = (Xs - pm) / ps\n",
    "    n = Xn.shape[0]\n",
    "    X4 = np.zeros((n, 4, H, W), dtype=np.float32)\n",
    "    X4[:, 0] = Xn.astype(np.float32)\n",
    "    X4[:, 1] = x_lin.astype(np.float32)\n",
    "    X4[:, 2] = y_lin.astype(np.float32)\n",
    "    X4[:, 3] = r_map.astype(np.float32)\n",
    "    return X4\n",
    "\n",
    "X_cnn_test_4 = build_cnn_input(images[test_sel_idx])\n",
    "\n",
    "# --- SHAP‑compatible wrapper that removes nn.Flatten and copies Linear weights ---\n",
    "class SHAPCompatCNN(nn.Module):\n",
    "    def __init__(self, base: nn.Module):\n",
    "        super().__init__()\n",
    "        m = copy.deepcopy(base).to(DEVICE).eval()\n",
    "        for module in m.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.inplace = False\n",
    "        self.features = m.features\n",
    "        self.gap = m.gap\n",
    "        # Extract Linear layers from head (expects exactly two)\n",
    "        lin_layers = [mod for mod in m.head if isinstance(mod, nn.Linear)]\n",
    "        if len(lin_layers) != 2:\n",
    "            raise RuntimeError(\"Expected two Linear layers in model.head.\")\n",
    "        self.fc1 = nn.Linear(lin_layers[0].in_features, lin_layers[0].out_features, bias=True)\n",
    "        self.act = nn.ReLU(inplace=False)\n",
    "        drops = [mod for mod in m.head if isinstance(mod, nn.Dropout)]\n",
    "        self.drop = nn.Dropout(p=drops[0].p if drops else 0.0)\n",
    "        self.fc2 = nn.Linear(lin_layers[1].in_features, lin_layers[1].out_features, bias=True)\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.copy_(lin_layers[0].weight); self.fc1.bias.copy_(lin_layers[0].bias)\n",
    "            self.fc2.weight.copy_(lin_layers[1].weight); self.fc2.bias.copy_(lin_layers[1].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)  # functional flatten (no nn.Flatten module)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        y = self.fc2(x)\n",
    "        return y.clone()\n",
    "\n",
    "shap_model = SHAPCompatCNN(model).to(DEVICE).eval()\n",
    "\n",
    "# --- candidate baselines for CNN SHAP ---\n",
    "def build_baseline(kind=\"train_subset\", m=64):\n",
    "    \"\"\"\n",
    "    kind ∈ {\"train_subset\", \"mean\", \"median\", \"blurred_mean\"}.\n",
    "    Returns a tensor on DEVICE suitable for DeepExplainer background.\n",
    "    \"\"\"\n",
    "    if kind == \"train_subset\":\n",
    "        idx = rng.choice(idx_train, size=min(m, len(idx_train)), replace=False)\n",
    "        X_bg = build_cnn_input(images[idx])\n",
    "        return torch.from_numpy(X_bg).to(DEVICE)\n",
    "    if kind in {\"mean\", \"median\", \"blurred_mean\"}:\n",
    "        X_tr = build_cnn_input(images[idx_train])\n",
    "        if kind == \"mean\":\n",
    "            img_ch = X_tr[:, 0].mean(axis=0, keepdims=True).astype(np.float32)\n",
    "        elif kind == \"median\":\n",
    "            img_ch = np.median(X_tr[:, 0], axis=0, keepdims=True).astype(np.float32)\n",
    "        else:  # blurred_mean\n",
    "            mean_img = X_tr[:, 0].mean(axis=0)\n",
    "            k = 5; pad = k // 2\n",
    "            tmp = np.pad(mean_img, pad, mode=\"reflect\"); out = np.zeros_like(mean_img)\n",
    "            for y in range(H):\n",
    "                for x in range(W):\n",
    "                    out[y, x] = tmp[y:y + k, x:x + k].mean()\n",
    "            img_ch = out[None, :, :].astype(np.float32)\n",
    "        Xb = np.zeros((1, 4, H, W), dtype=np.float32)\n",
    "        Xb[0, 0] = img_ch[0]; Xb[0, 1] = x_lin; Xb[0, 2] = y_lin; Xb[0, 3] = r_map\n",
    "        return torch.from_numpy(Xb).to(DEVICE)\n",
    "    raise ValueError(\"Unknown baseline kind.\")\n",
    "\n",
    "# --- DeepExplainer helper that disables additivity check (critical fix) ---\n",
    "def safe_shap_values(explainer, x_tensor):\n",
    "    try:\n",
    "        return explainer.shap_values(x_tensor, check_additivity=False)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            import shap.explainers._deep.deep_utils as _du\n",
    "            _du.TOLERANCE = 1e9\n",
    "        except Exception:\n",
    "            pass\n",
    "        return explainer.shap_values(x_tensor)\n",
    "\n",
    "# --- Deep SHAP for several baselines; pick the best by fidelity ---\n",
    "BASELINES = [\"train_subset\", \"mean\", \"median\", \"blurred_mean\"]\n",
    "baseline_results = {}\n",
    "torch.set_grad_enabled(True)\n",
    "X_test_t = torch.from_numpy(X_cnn_test_4).to(DEVICE).requires_grad_(True)\n",
    "\n",
    "def cnn_predict_y_from_4(X4):\n",
    "    \"\"\"Predict y (not z) for a batch of 4‑channel inputs with the *trained* CNN clone used for SHAP.\"\"\"\n",
    "    y_train_obs = y_obs_all[idx_train]\n",
    "    y_mean = float(np.mean(y_train_obs)); y_std = float(np.std(y_train_obs) + 1e-8)\n",
    "    def from_z(z): return z * y_std + y_mean\n",
    "    shap_model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = shap_model(torch.from_numpy(X4).to(DEVICE)).cpu().numpy().squeeze(-1)\n",
    "    return from_z(z)\n",
    "\n",
    "fractions = [0.01, 0.02, 0.05, 0.10, 0.20]\n",
    "y_obs_sel = y_obs_test_sel\n",
    "\n",
    "for bkind in BASELINES:\n",
    "    bg_t = build_baseline(bkind, m=64 if bkind == \"train_subset\" else 1)\n",
    "    expl = shap.DeepExplainer(shap_model, bg_t)\n",
    "    vals = safe_shap_values(expl, X_test_t)\n",
    "    sh = vals[0] if isinstance(vals, list) else vals  # (n, 4, H, W)\n",
    "    sh_img = sh[:, 0, :, :]\n",
    "    baseline_results[bkind] = {\"all\": sh, \"img\": sh_img}\n",
    "\n",
    "    # IoU/Precision@k\n",
    "    io = iou_and_precision_vs_shell(sh_img, k_fracs=(0.01, 0.05))\n",
    "    baseline_results[bkind][\"iou_prec\"] = io\n",
    "\n",
    "    # Deletion AOPC\n",
    "    n, _, h, w = X_cnn_test_4.shape; D = h * w\n",
    "    order = np.argsort(-np.abs(sh_img.reshape(n, D)), axis=1)\n",
    "    y0 = cnn_predict_y_from_4(X_cnn_test_4); r2_base = r2_score(y_obs_sel, y0)\n",
    "    r2_list = []\n",
    "    for frac in fractions:\n",
    "        k = max(1, int(frac * D))\n",
    "        X_mod = X_cnn_test_4.copy()\n",
    "        for i in range(n):\n",
    "            idxk = order[i, :k]; rr, cc = idxk // w, idxk % w\n",
    "            X_mod[i, 0, rr, cc] = 0.0\n",
    "        y_pred_mod = cnn_predict_y_from_4(X_mod)\n",
    "        r2_list.append(r2_score(y_obs_sel, y_pred_mod))\n",
    "    drops = (r2_base - np.array(r2_list))\n",
    "    aopc = area_trapezoid(drops, np.array(fractions))\n",
    "    baseline_results[bkind][\"aopc\"] = float(aopc)\n",
    "    baseline_results[bkind][\"r2_base\"] = float(r2_base)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Pick best baseline by (IoU@5% + IoU@1%) + AOPC (simple normalised score)\n",
    "def normalise(v):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    if np.ptp(v) < 1e-12:\n",
    "        return np.ones_like(v)\n",
    "    return (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "scores = []; kinds = []\n",
    "for k, d in baseline_results.items():\n",
    "    io1 = d[\"iou_prec\"][0.01][0]; io5 = d[\"iou_prec\"][0.05][0]\n",
    "    kinds.append(k); scores.append([io1, io5, d[\"aopc\"]])\n",
    "scores = np.array(scores)\n",
    "score = normalise(scores[:, 0]) + normalise(scores[:, 1]) + normalise(scores[:, 2])\n",
    "BEST_BASELINE = kinds[int(np.argmax(score))]\n",
    "print(f\"\\n[CNN] Baseline selection by fidelity → chosen: **{BEST_BASELINE}**\")\n",
    "for k in BASELINES:\n",
    "    io1 = baseline_results[k][\"iou_prec\"][0.01][0]\n",
    "    io5 = baseline_results[k][\"iou_prec\"][0.05][0]\n",
    "    print(f\"  {k:13s}  IoU@1%={io1:.3f}  IoU@5%={io5:.3f}  AOPC={baseline_results[k]['aopc']:.4f}\")\n",
    "\n",
    "# Use the chosen baseline’s SHAP maps from now on\n",
    "shap_cnn_allch = baseline_results[BEST_BASELINE][\"all\"]\n",
    "shap_cnn_imgch = baseline_results[BEST_BASELINE][\"img\"]\n",
    "\n",
    "# Light background bootstrapping (variability hints)\n",
    "print(\"\\n[CNN] Background bootstrapping (variability over train‑subset backgrounds)\")\n",
    "B = 3\n",
    "boot_shares = []\n",
    "for b in range(B):\n",
    "    bg_t = build_baseline(\"train_subset\", m=32)\n",
    "    torch.set_grad_enabled(True)\n",
    "    e = shap.DeepExplainer(shap_model, bg_t)\n",
    "    vals = safe_shap_values(e, X_test_t)\n",
    "    torch.set_grad_enabled(False)\n",
    "    sh = vals[0] if isinstance(vals, list) else vals\n",
    "    sh_img = sh[:, 0]\n",
    "    shares = np.array([region_shares(np.abs(s)) for s in sh_img])\n",
    "    boot_shares.append(shares.mean(axis=0))\n",
    "boot_shares = np.stack(boot_shares, axis=0)\n",
    "means = boot_shares.mean(axis=0); stds = boot_shares.std(axis=0)\n",
    "print(f\"  Region share mean±sd (Hollow/Shell/Outside): \"\n",
    "      f\"{means[0]:.3f}±{stds[0]:.3f} / {means[1]:.3f}±{stds[1]:.3f} / {means[2]:.3f}±{stds[2]:.3f}\")\n",
    "\n",
    "# Local overlays & global maps for CNN (chosen baseline)\n",
    "grid_overlays(test_imgs[:8], shap_cnn_imgch[:8], f\"CNN [{BEST_BASELINE}]\")\n",
    "heatmap2d(np.mean(np.abs(shap_cnn_imgch), axis=0), f\"Global mean |SHAP| — CNN [{BEST_BASELINE}]\")\n",
    "summarise_region_and_profile(shap_cnn_imgch, f\"CNN [{BEST_BASELINE}]\")\n",
    "print_iou_precision(f\"CNN [{BEST_BASELINE}]\", iou_and_precision_vs_shell(shap_cnn_imgch))\n",
    "\n",
    "# -----------------------------\n",
    "# Deletion curves — measured as R² changes (OLS / Ridge / CNN)\n",
    "# -----------------------------\n",
    "print(\"\\n[Deletion curves: what they mean]\\n\"\n",
    "      \"• Remove top‑|SHAP| pixels and re‑score R² vs true labels.\\n\"\n",
    "      \"  A *faithful* explanation targets genuinely important pixels ⇒ R² **drops fast**. We summarise by AOPC.\\n\")\n",
    "\n",
    "def plot_deletion_r2(title, fractions, r2_base, r2_list):\n",
    "    plt.figure(figsize=(5.8, 4))\n",
    "    plt.plot([f * 100 for f in fractions], [r2_base] * len(fractions), linestyle=\"--\", label=\"Baseline R²\")\n",
    "    plt.plot([f * 100 for f in fractions], r2_list, marker=\"o\", label=\"R² after deletion\")\n",
    "    plt.xlabel(\"Top-|SHAP| pixels removed (%)\")\n",
    "    plt.ylabel(\"R² vs true labels (test subset)\")\n",
    "    plt.title(title); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    drops = (r2_base - np.array(r2_list))\n",
    "    aopc = area_trapezoid(drops, np.array(fractions))\n",
    "    print(f\"{title} — AOPC (higher → better fidelity): {aopc:.4f}\")\n",
    "    return aopc\n",
    "\n",
    "# --- OLS deletion\n",
    "y0_ols_pred = ols.predict(X_ols_test)\n",
    "r2_base_ols = r2_score(y_obs_test_sel, y0_ols_pred)\n",
    "order_ols = np.argsort(-np.abs(shap_ols_flat), axis=1)\n",
    "r2_list_ols_del = []\n",
    "mu = X_bg_ols.mean(axis=0)\n",
    "for frac in fractions:\n",
    "    k = max(1, int(frac * X_ols_test.shape[1]))\n",
    "    X_del = X_ols_test.copy()\n",
    "    for i in range(X_del.shape[0]):\n",
    "        X_del[i, order_ols[i, :k]] = mu[order_ols[i, :k]]\n",
    "    r2_list_ols_del.append(r2_score(y_obs_test_sel, ols.predict(X_del)))\n",
    "aopc_ols = plot_deletion_r2(\"Deletion curve — OLS (R² drop)\", fractions, r2_base_ols, r2_list_ols_del)\n",
    "\n",
    "# --- Ridge deletion\n",
    "y0_ridge_pred = final_ridge.predict(X_flat[test_sel_idx])\n",
    "r2_base_ridge = r2_score(y_obs_test_sel, y0_ridge_pred)\n",
    "order_ridge = np.argsort(-np.abs(shap_ridge_scaled), axis=1)\n",
    "r2_list_ridge_del = []\n",
    "Z_test = scaler.transform(X_flat[test_sel_idx])\n",
    "for frac in fractions:\n",
    "    k = max(1, int(frac * Z_test.shape[1]))\n",
    "    Z_del = Z_test.copy()\n",
    "    for i in range(Z_del.shape[0]):\n",
    "        Z_del[i, order_ridge[i, :k]] = 0.0  # 0 == scaled mean\n",
    "    y_pred_del = (Z_del @ ridge.coef_.ravel() + ridge.intercept_)\n",
    "    r2_list_ridge_del.append(r2_score(y_obs_test_sel, y_pred_del))\n",
    "aopc_ridge = plot_deletion_r2(\"Deletion curve — Ridge (R² drop)\", fractions, r2_base_ridge, r2_list_ridge_del)\n",
    "\n",
    "# --- CNN deletion (image channel only; coords intact)\n",
    "y_train_obs = y_obs_all[idx_train]\n",
    "y_mean = float(np.mean(y_train_obs)); y_std = float(np.std(y_train_obs) + 1e-8)\n",
    "def from_z(z): return z * y_std + y_mean\n",
    "\n",
    "def cnn_predict_y(_model, X4):\n",
    "    _model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = _model(torch.from_numpy(X4).to(DEVICE)).cpu().numpy().squeeze(-1)\n",
    "    return from_z(z)\n",
    "\n",
    "y0_cnn_pred = cnn_predict_y(shap_model, X_cnn_test_4)\n",
    "r2_base_cnn = r2_score(y_obs_test_sel, y0_cnn_pred)\n",
    "n, _, h, w = X_cnn_test_4.shape; D = h * w\n",
    "order_cnn = np.argsort(-np.abs(shap_cnn_imgch.reshape(n, D)), axis=1)\n",
    "\n",
    "r2_list_cnn_del = []\n",
    "for frac in fractions:\n",
    "    k = max(1, int(frac * D))\n",
    "    X_mod = X_cnn_test_4.copy()\n",
    "    for i in range(n):\n",
    "        idx = order_cnn[i, :k]; rr, cc = idx // w, idx % w\n",
    "        X_mod[i, 0, rr, cc] = 0.0\n",
    "    y_pred_mod = cnn_predict_y(shap_model, X_mod)\n",
    "    r2_list_cnn_del.append(r2_score(y_obs_test_sel, y_pred_mod))\n",
    "aopc_cnn = plot_deletion_r2(f\"Deletion curve — CNN [{BEST_BASELINE}] (R² drop)\", fractions, r2_base_cnn, r2_list_cnn_del)\n",
    "\n",
    "# -----------------------------\n",
    "# Bounding‑box analysis over top‑|SHAP| within the shell\n",
    "#   • Robust against shape issues (always flattens indices safely).\n",
    "#   • Takes the **largest weighted connected component** before boxing.\n",
    "# -----------------------------\n",
    "def circular_coverage(angles_rad):\n",
    "    \"\"\"Minimal angular span (radians) covering all given angles on the circle.\"\"\"\n",
    "    if angles_rad.size == 0:\n",
    "        return 0.0\n",
    "    a = np.sort(angles_rad)\n",
    "    diffs = np.diff(np.concatenate([a, a[:1] + 2*np.pi]))\n",
    "    max_gap = np.max(diffs)\n",
    "    return float(2*np.pi - max_gap)\n",
    "\n",
    "def _largest_component(mask2d, weights2d=None):\n",
    "    \"\"\"\n",
    "    8‑connected components on a boolean 2‑D mask.\n",
    "    Returns the mask of the component with largest total weight (or largest size if weights is None).\n",
    "    \"\"\"\n",
    "    M = np.ascontiguousarray(mask2d.astype(bool))\n",
    "    H_, W_ = M.shape\n",
    "    visited = np.zeros_like(M, dtype=bool)\n",
    "    best_weight = -1.0\n",
    "    best_comp = None\n",
    "\n",
    "    # Precompute neighbor offsets (8‑connectivity)\n",
    "    nbrs = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "\n",
    "    # Iterate over all True pixels\n",
    "    ys_, xs_ = np.where(M)\n",
    "    for y0, x0 in zip(ys_, xs_):\n",
    "        if visited[y0, x0]:\n",
    "            continue\n",
    "        # BFS/stack\n",
    "        stack = [(y0, x0)]\n",
    "        visited[y0, x0] = True\n",
    "        comp_pixels = [(y0, x0)]\n",
    "        while stack:\n",
    "            y, x = stack.pop()\n",
    "            for dy, dx in nbrs:\n",
    "                yy, xx = y + dy, x + dx\n",
    "                if 0 <= yy < H_ and 0 <= xx < W_ and (not visited[yy, xx]) and M[yy, xx]:\n",
    "                    visited[yy, xx] = True\n",
    "                    stack.append((yy, xx))\n",
    "                    comp_pixels.append((yy, xx))\n",
    "        # weight for this component\n",
    "        if weights2d is not None:\n",
    "            w = float(np.sum([weights2d[y, x] for (y, x) in comp_pixels]))\n",
    "        else:\n",
    "            w = float(len(comp_pixels))\n",
    "        if w > best_weight:\n",
    "            best_weight = w\n",
    "            best_comp = comp_pixels\n",
    "\n",
    "    comp_mask = np.zeros_like(M, dtype=bool)\n",
    "    if best_comp is not None:\n",
    "        ys_c, xs_c = zip(*best_comp)\n",
    "        comp_mask[ys_c, xs_c] = True\n",
    "    return comp_mask\n",
    "\n",
    "def compute_bbox_metrics(stack, model_name, imgs_for_overlay=None, draw_overlays=True, overlay_n=6,\n",
    "                         p_top=0.01):\n",
    "    \"\"\"\n",
    "    For each SHAP map in `stack`, within the shell:\n",
    "      • select the TOP p_top fraction of |SHAP| pixels (non-negative kth),\n",
    "      • take the **largest weighted connected component** of that mask,\n",
    "      • compute the tight axis-aligned bounding box,\n",
    "      • report area fraction of the shell covered by the box,\n",
    "               fraction of total |SHAP| in shell captured by the box,\n",
    "               angular coverage (deg) of selected pixels,\n",
    "               radial width normalised by shell thickness.\n",
    "    Returns a dict of per-sample arrays and prints robust summarised stats.\n",
    "    \"\"\"\n",
    "    area_fracs = []\n",
    "    shap_shares = []\n",
    "    ang_degs    = []\n",
    "    rad_widths  = []\n",
    "\n",
    "    shell_area = float(mask_shell.sum())\n",
    "    shell_thick = (OUTER_R - INNER_R)\n",
    "\n",
    "    overlays_done = 0\n",
    "    if imgs_for_overlay is None:\n",
    "        imgs_for_overlay = []\n",
    "\n",
    "    for i, S in enumerate(stack):\n",
    "        # ensure 2‑D map\n",
    "        S2 = np.asarray(S)\n",
    "        if S2.ndim != 2:\n",
    "            # if something off slipped in, reduce to the last two dims\n",
    "            S2 = np.asarray(S2)[-H:, -W:]\n",
    "            S2 = S2.reshape(H, W)\n",
    "        Sabs = np.abs(S2)\n",
    "\n",
    "        vals = Sabs[mask_shell].ravel()\n",
    "\n",
    "        if vals.size == 0 or not np.any(vals > 0):\n",
    "            # skip gracefully\n",
    "            continue\n",
    "\n",
    "        # TOP‑p% threshold (non‑negative kth)\n",
    "        k = max(1, int(np.ceil(p_top * vals.size)))\n",
    "        kth = vals.size - k\n",
    "        if kth < 0: kth = 0\n",
    "        thr = np.partition(vals, kth)[kth]\n",
    "\n",
    "        # binary mask of top‑p% within shell\n",
    "        M = np.zeros((H, W), dtype=bool)\n",
    "        M_shell = (Sabs >= thr) & mask_shell\n",
    "\n",
    "        # Focus on **largest weighted component** (weights = |SHAP|)\n",
    "        M_sel = _largest_component(M_shell, weights2d=Sabs)\n",
    "\n",
    "        # fallback: if component is empty (pathological), take the single max pixel in shell\n",
    "        if not np.any(M_sel):\n",
    "            shell_idx = np.column_stack(np.where(mask_shell))\n",
    "            # argmax over shell pixels\n",
    "            argmax_flat = np.argmax(Sabs[mask_shell])\n",
    "            yx = shell_idx[argmax_flat]\n",
    "            M_sel = np.zeros((H, W), dtype=bool)\n",
    "            M_sel[yx[0], yx[1]] = True\n",
    "\n",
    "        # coords of selected pixels (robust way, works for any shape)\n",
    "        coords = np.column_stack(np.where(M_sel))\n",
    "        y_min, y_max = int(coords[:, 0].min()), int(coords[:, 0].max())\n",
    "        x_min, x_max = int(coords[:, 1].min()), int(coords[:, 1].max())\n",
    "\n",
    "        # area fraction of shell covered by the BOX ∩ shell\n",
    "        box_mask = np.zeros((H, W), dtype=bool)\n",
    "        box_mask[y_min:y_max+1, x_min:x_max+1] = True\n",
    "        box_shell = np.logical_and(box_mask, mask_shell)\n",
    "        area_frac = float(box_shell.sum() / (shell_area + 1e-12))\n",
    "        area_fracs.append(area_frac)\n",
    "\n",
    "        # |SHAP| share captured by the BOX within the shell\n",
    "        shap_shell_total = float(Sabs[mask_shell].sum() + 1e-12)\n",
    "        shap_in_box = float(Sabs[box_shell].sum())\n",
    "        shap_share = shap_in_box / shap_shell_total\n",
    "        shap_shares.append(shap_share)\n",
    "\n",
    "        # angular coverage (deg) of SELECTED pixels\n",
    "        ang = theta[M_sel]\n",
    "        ang_span = circular_coverage(ang.ravel())\n",
    "        ang_deg = float(ang_span * 180.0 / np.pi)\n",
    "        ang_degs.append(ang_deg)\n",
    "\n",
    "        # radial width normalised by shell thickness (using selected pixels)\n",
    "        r_sel = R[M_sel]\n",
    "        rad_w = float((r_sel.max() - r_sel.min()) / (shell_thick + 1e-12))\n",
    "        rad_widths.append(rad_w)\n",
    "\n",
    "        # overlays (draw box + highlight selected component)\n",
    "        if draw_overlays and (overlays_done < overlay_n) and (i < len(imgs_for_overlay)):\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(3.2, 3.2))\n",
    "            ax.imshow(imgs_for_overlay[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "            rect = patches.Rectangle((x_min, y_min), x_max - x_min + 1, y_max - y_min + 1,\n",
    "                                     linewidth=1.8, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            # faint fill for the selected component\n",
    "            M_alpha = np.zeros((H, W), dtype=float)\n",
    "            M_alpha[M_sel] = 0.5\n",
    "            ax.imshow(M_alpha, cmap=\"Reds\", alpha=0.25, vmin=0, vmax=1)\n",
    "            ax.set_title(f\"{model_name} — bbox on top-{int(p_top*100)}% |SHAP| (largest component)\", fontsize=9)\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "            overlays_done += 1\n",
    "\n",
    "    # Summaries\n",
    "    area_fracs = np.array(area_fracs, dtype=float)\n",
    "    shap_shares = np.array(shap_shares, dtype=float)\n",
    "    ang_degs    = np.array(ang_degs, dtype=float)\n",
    "    rad_widths  = np.array(rad_widths, dtype=float)\n",
    "\n",
    "    def summ(name, arr, unit=\"\"):\n",
    "        if arr.size == 0:\n",
    "            print(f\"  {name}: n=0\")\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "        print(f\"  {name}: median={np.nanmedian(arr):.3f}{unit}  \"\n",
    "              f\"IQR=({np.nanpercentile(arr,25):.3f}{unit}, {np.nanpercentile(arr,75):.3f}{unit})\")\n",
    "        return (float(np.nanmedian(arr)),\n",
    "                float(np.nanpercentile(arr,25)),\n",
    "                float(np.nanpercentile(arr,75)))\n",
    "\n",
    "    print(f\"\\n[{model_name}] Bounding‑box metrics on top‑|SHAP| within the shell (largest component; p_top={p_top*100:.1f}%):\")\n",
    "    s1 = summ(\"Area fraction of shell (BOX∩shell / shell)\", area_fracs)\n",
    "    s2 = summ(\"|SHAP| share captured (BOX∩shell / shell)\",   shap_shares)\n",
    "    s3 = summ(\"Angular coverage (deg) of selected pixels\",     ang_degs, unit=\"°\")\n",
    "    s4 = summ(\"Radial width / shell thickness\",                rad_widths)\n",
    "\n",
    "    return {\n",
    "        \"area_frac\": area_fracs, \"shap_share\": shap_shares,\n",
    "        \"angle_deg\": ang_degs, \"rad_width\": rad_widths,\n",
    "        \"summary\": {\"area_frac\": s1, \"shap_share\": s2, \"angle_deg\": s3, \"rad_width\": s4}\n",
    "    }\n",
    "\n",
    "# --- Run bounding‑box metrics for each model (top 1% by default)\n",
    "P_TOP = 0.01\n",
    "bbox_stats_ols   = compute_bbox_metrics(shap_ols_imgs,   \"OLS\",   imgs_for_overlay=test_imgs[:8], draw_overlays=False, p_top=P_TOP)\n",
    "bbox_stats_ridge = compute_bbox_metrics(shap_ridge_imgs, \"Ridge\", imgs_for_overlay=test_imgs[:8], draw_overlays=False, p_top=P_TOP)\n",
    "bbox_stats_cnn   = compute_bbox_metrics(shap_cnn_imgch,  f\"CNN [{BEST_BASELINE}]\",\n",
    "                                        imgs_for_overlay=test_imgs[:8], draw_overlays=True, p_top=P_TOP)\n",
    "\n",
    "print(\"\\n[Reading the bounding‑box summaries]\")\n",
    "print(\"• A small **Area fraction** with a large **|SHAP| share** and **low angular coverage** means the model relies on a tight, local cue.\\n\"\n",
    "      \"• For the *hard/star* dataset, the CNN should have the **smallest boxes** and **tightest angles** among the three, \"\n",
    "      \"while OLS/Ridge remain diffuse.\\n\"\n",
    "      \"• For the *simple* dataset (no star), the CNN’s boxes will typically hug thin arcs on the inner/outer edges; \"\n",
    "      \"angles will be modest, not full‑ring, and radial width < 0.5.\\n\")\n",
    "\n",
    "# ================================\n",
    "# Global star-capture analysis (robust 2-D handling + enrichment)\n",
    "# ================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n[Global star-capture analysis]\")\n",
    "\n",
    "# --- Helpers to guarantee 2-D (H,W) arrays and clean values\n",
    "def _ensure_hw(x):\n",
    "    a = np.asarray(x)\n",
    "    a = np.squeeze(a)\n",
    "    if a.ndim != 2:\n",
    "        # Force-shape to (H, W) if a weird singleton remains\n",
    "        a = a.reshape(H, W)\n",
    "    # Clean NaN/Inf for safety\n",
    "    a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return a\n",
    "\n",
    "# Reuse ring geometry already defined: R, theta, mask_shell, INNER_R, OUTER_R\n",
    "R_mid   = 0.5 * (INNER_R + OUTER_R)\n",
    "R_sigma = 0.18 * (OUTER_R - INNER_R)\n",
    "RING_W  = np.exp(-0.5 * ((R - R_mid) / (R_sigma + 1e-8))**2) * mask_shell\n",
    "shell_area = float(mask_shell.sum())\n",
    "shell_thick = (OUTER_R - INNER_R)\n",
    "\n",
    "def _box_blur3(a):\n",
    "    a = _ensure_hw(a)\n",
    "    tmp = np.pad(a, 1, mode=\"reflect\")\n",
    "    out = np.zeros_like(a, dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            out[y, x] = tmp[y:y+3, x:x+3].mean()\n",
    "    return out\n",
    "\n",
    "def find_star_proxy_center(img):\n",
    "    # Ring-weighted, high-tail emphasis\n",
    "    z = _ensure_hw(img) - float(np.median(img))\n",
    "    z[z < 0.0] = 0.0\n",
    "    m = z * RING_W\n",
    "    m = _box_blur3(m)\n",
    "    yi, xi = np.unravel_index(np.argmax(m), m.shape)\n",
    "    return int(yi), int(xi)\n",
    "\n",
    "def disc_mask(yc, xc, radius=4):\n",
    "    yy, xx = np.ogrid[:H, :W]\n",
    "    return ((yy - yc)**2 + (xx - xc)**2) <= (radius*radius)\n",
    "\n",
    "def circular_coverage(angles_rad):\n",
    "    if angles_rad.size == 0:\n",
    "        return 0.0\n",
    "    a = np.sort(angles_rad)\n",
    "    diffs = np.diff(np.concatenate([a, a[:1] + 2*np.pi]))\n",
    "    max_gap = np.max(diffs)\n",
    "    return float(2*np.pi - max_gap)\n",
    "\n",
    "def circ_mean_angle(angles, weights=None):\n",
    "    if angles.size == 0:\n",
    "        return np.nan\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(angles, dtype=float)\n",
    "    c = np.sum(weights * np.cos(angles))\n",
    "    s = np.sum(weights * np.sin(angles))\n",
    "    return float(np.arctan2(s, c))\n",
    "\n",
    "def circ_abs_diff_deg(a, b):\n",
    "    d = np.abs((a - b + np.pi) % (2*np.pi) - np.pi)\n",
    "    return float(d * 180.0 / np.pi)\n",
    "\n",
    "# ---- robust 8-connected component on 2-D boolean masks\n",
    "def _largest_component(mask2d, weights2d=None):\n",
    "    M = _ensure_hw(mask2d).astype(bool)\n",
    "    Wts = _ensure_hw(weights2d) if weights2d is not None else None\n",
    "    H_, W_ = M.shape\n",
    "    visited = np.zeros_like(M, dtype=bool)\n",
    "    best_weight = -1.0\n",
    "    best_comp = None\n",
    "    nbrs = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "    ys_, xs_ = np.where(M)\n",
    "    for y0, x0 in zip(ys_, xs_):\n",
    "        if visited[y0, x0]:\n",
    "            continue\n",
    "        stack = [(y0, x0)]\n",
    "        visited[y0, x0] = True\n",
    "        comp_pixels = [(y0, x0)]\n",
    "        while stack:\n",
    "            y, x = stack.pop()\n",
    "            for dy, dx in nbrs:\n",
    "                yy, xx = y + dy, x + dx\n",
    "                if 0 <= yy < H_ and 0 <= xx < W_ and (not visited[yy, xx]) and M[yy, xx]:\n",
    "                    visited[yy, xx] = True\n",
    "                    stack.append((yy, xx))\n",
    "                    comp_pixels.append((yy, xx))\n",
    "        if Wts is not None:\n",
    "            w = float(np.sum([Wts[y, x] for (y, x) in comp_pixels]))\n",
    "        else:\n",
    "            w = float(len(comp_pixels))\n",
    "        if w > best_weight:\n",
    "            best_weight = w\n",
    "            best_comp = comp_pixels\n",
    "    comp_mask = np.zeros_like(M, dtype=bool)\n",
    "    if best_comp:\n",
    "        ys_c, xs_c = zip(*best_comp)\n",
    "        comp_mask[ys_c, xs_c] = True\n",
    "    return comp_mask\n",
    "\n",
    "def largest_component_mask(abs_map, top_frac=0.01):\n",
    "    S = _ensure_hw(abs_map)\n",
    "    vals = S[mask_shell].ravel()\n",
    "    if vals.size == 0:\n",
    "        return np.zeros((H, W), dtype=bool)\n",
    "    vals = np.nan_to_num(vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    k = max(1, int(np.ceil(top_frac * vals.size)))\n",
    "    kth = max(0, vals.size - k)           # non-negative kth\n",
    "    thr = np.partition(vals, kth)[kth]\n",
    "    M_shell = (S >= thr) & mask_shell\n",
    "    return _largest_component(M_shell, weights2d=S)\n",
    "\n",
    "def global_star_capture(shap_stack, model_name, imgs_subset, disc_radius=4):\n",
    "    shares, enrichments, ang_errs = [], [], []\n",
    "    for i in range(len(shap_stack)):\n",
    "        img = _ensure_hw(imgs_subset[i])\n",
    "        S   = np.abs(_ensure_hw(shap_stack[i]))\n",
    "\n",
    "        # (1) star disc\n",
    "        ys_, xs_ = find_star_proxy_center(img)\n",
    "        D = disc_mask(ys_, xs_, radius=disc_radius)\n",
    "        disc_shell = np.logical_and(D, mask_shell)\n",
    "\n",
    "        # (2) |SHAP| share inside disc (normalised by |SHAP| on shell)\n",
    "        denom = float(S[mask_shell].sum() + 1e-12)\n",
    "        numer = float(S[disc_shell].sum())\n",
    "        share = numer / denom\n",
    "        shares.append(share)\n",
    "\n",
    "        # (3) enrichment vs area fraction of disc in the shell\n",
    "        area_frac_disc = float(disc_shell.sum() / (shell_area + 1e-12))\n",
    "        enrichments.append(share / (area_frac_disc + 1e-12))\n",
    "\n",
    "        # (4) angular alignment vs largest |SHAP| component\n",
    "        M_sel = largest_component_mask(S, top_frac=0.01)\n",
    "        if np.any(M_sel):\n",
    "            th_star = float(theta[ys_, xs_])\n",
    "            ww = S[M_sel]\n",
    "            th_comp = circ_mean_angle(theta[M_sel], weights=ww / (ww.sum() + 1e-12))\n",
    "            ang_errs.append(circ_abs_diff_deg(th_star, th_comp))\n",
    "        else:\n",
    "            ang_errs.append(np.nan)\n",
    "\n",
    "    shares = np.array(shares, dtype=float)\n",
    "    enrichments = np.array(enrichments, dtype=float)\n",
    "    ang_errs = np.array(ang_errs, dtype=float)\n",
    "\n",
    "    def summarise(name, arr, unit=\"\"):\n",
    "        nfin = int(np.sum(np.isfinite(arr)))\n",
    "        print(f\"  {model_name} — {name}: \"\n",
    "              f\"median={np.nanmedian(arr):.3f}{unit} | \"\n",
    "              f\"IQR=({np.nanpercentile(arr,25):.3f}{unit}, {np.nanpercentile(arr,75):.3f}{unit}) | n={nfin}\")\n",
    "\n",
    "    print(f\"\\n[{model_name}] Star-disc capture (radius={disc_radius}px), enrichment, and angular alignment\")\n",
    "    summarise(\"SHAP share inside star disc\", shares)\n",
    "    summarise(\"enrichment (share / area)\",   enrichments)\n",
    "    summarise(\"abs angular error vs star (deg)\", ang_errs, unit=\"°\")\n",
    "    return shares, enrichments, ang_errs\n",
    "\n",
    "# Run on the same subset used for SHAP (test_imgs / shap_*_imgs)\n",
    "shares_ols,   enrich_ols,   ang_ols   = global_star_capture(shap_ols_imgs,   \"OLS\",   test_imgs, disc_radius=4)\n",
    "shares_ridge, enrich_ridge, ang_ridge = global_star_capture(shap_ridge_imgs, \"Ridge\", test_imgs, disc_radius=4)\n",
    "shares_cnn,   enrich_cnn,   ang_cnn   = global_star_capture(shap_cnn_imgch,  f\"CNN [{BEST_BASELINE}]\", test_imgs, disc_radius=4)\n",
    "\n",
    "# Optional: global comparison plots\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([shares_ols, shares_ridge, shares_cnn], labels=[\"OLS\",\"Ridge\",\"CNN\"], showmeans=True)\n",
    "plt.ylabel(\"Star-disc |SHAP| share (fraction of |SHAP| in shell)\")\n",
    "plt.title(\"Global star capture across explained test subset\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([enrich_ols, enrich_ridge, enrich_cnn], labels=[\"OLS\",\"Ridge\",\"CNN\"], showmeans=True)\n",
    "plt.ylabel(\"Enrichment (|SHAP| share / area fraction of disc)\")\n",
    "plt.title(\"Global enrichment of |SHAP| at the star location\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CNN baseline sensitivity (train subset vs mean baseline)\n",
    "# -----------------------------\n",
    "torch.set_grad_enabled(True)\n",
    "bg_train_t = build_baseline(\"train_subset\", m=64)\n",
    "bg_mean_t  = build_baseline(\"mean\", m=1)\n",
    "e_train = shap.DeepExplainer(shap_model, bg_train_t)\n",
    "e_mean  = shap.DeepExplainer(shap_model, bg_mean_t)\n",
    "vals_a = safe_shap_values(e_train, X_test_t)\n",
    "vals_b = safe_shap_values(e_mean, X_test_t)\n",
    "torch.set_grad_enabled(False)\n",
    "A = (vals_a[0] if isinstance(vals_a, list) else vals_a)[:, 0]\n",
    "B = (vals_b[0] if isinstance(vals_b, list) else vals_b)[:, 0]\n",
    "if HAVE_SPEARMAN:\n",
    "    corrs = []\n",
    "    for i in range(A.shape[0]):\n",
    "        a = np.abs(A[i]).ravel(); b = np.abs(B[i]).ravel()\n",
    "        if (np.std(a) < 1e-12) or (np.std(b) < 1e-12):\n",
    "            corrs.append(np.nan)\n",
    "        else:\n",
    "            corrs.append(spearmanr(a, b, nan_policy=\"omit\").correlation)\n",
    "    corrs = np.array(corrs)\n",
    "    print(\"\\n[Baseline sensitivity — CNN]\")\n",
    "    print(f\"  Spearman rank correlation of |SHAP| (train‑baseline vs mean‑baseline): \"\n",
    "          f\"median={np.nanmedian(corrs):.3f}, IQR=({np.nanpercentile(corrs, 25):.3f}, {np.nanpercentile(corrs, 75):.3f})\")\n",
    "else:\n",
    "    print(\"\\n[Baseline sensitivity — CNN]\\n  SciPy not available; install SciPy to report Spearman correlation.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Polar superpixels / regional analysis\n",
    "# ==========================================================\n",
    "print(\"\\n[Polar superpixels / regional analysis]\")\n",
    "NB_RADIAL = 10; NB_THETA = 16\n",
    "rad_edges  = np.linspace(0, R.max() + 1e-6, NB_RADIAL + 1)\n",
    "theta_edges= np.linspace(0, 2 * np.pi, NB_THETA + 1)\n",
    "rad_bin = np.digitize(R, rad_edges) - 1\n",
    "theta_bin = np.digitize(theta, theta_edges) - 1\n",
    "rad_bin[rad_bin == NB_RADIAL] = NB_RADIAL - 1\n",
    "theta_bin[theta_bin == NB_THETA] = NB_THETA - 1\n",
    "\n",
    "def polar_bin_aggregate(abs_shap_map):\n",
    "    out = np.zeros((NB_RADIAL, NB_THETA), dtype=np.float64)\n",
    "    counts = np.zeros_like(out)\n",
    "    for r in range(NB_RADIAL):\n",
    "        for t in range(NB_THETA):\n",
    "            m = (rad_bin == r) & (theta_bin == t)\n",
    "            if np.any(m):\n",
    "                out[r, t] = abs_shap_map[m].mean()\n",
    "                counts[r, t] = m.sum()\n",
    "    return out, counts\n",
    "\n",
    "def polar_summary(stack, model_name):\n",
    "    mats = []\n",
    "    for s in stack:\n",
    "        M, _ = polar_bin_aggregate(np.abs(s))\n",
    "        mats.append(M)\n",
    "    Mmean = np.mean(mats, axis=0)\n",
    "    ring_strength = Mmean.mean(axis=1)\n",
    "    r_idx = int(np.argmax(ring_strength))\n",
    "    r_lo, r_hi = rad_edges[r_idx], rad_edges[r_idx + 1]\n",
    "    row = Mmean[r_idx, :]\n",
    "    cv = float(np.std(row) / (np.mean(row) + 1e-12))\n",
    "\n",
    "    plt.figure(figsize=(6, 3.8))\n",
    "    plt.imshow(Mmean, aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.yticks(np.arange(NB_RADIAL), [f\"{rad_edges[i]:.0f}-{rad_edges[i + 1]:.0f}\" for i in range(NB_RADIAL)])\n",
    "    plt.xticks(np.arange(0, NB_THETA, 4), [f\"{int(360 * t / NB_THETA)}°\" for t in range(0, NB_THETA, 4)])\n",
    "    plt.xlabel(\"Angle (θ)\"); plt.ylabel(\"Radius band (px)\")\n",
    "    plt.title(f\"{model_name} — polar mean |SHAP|\\n(peak ring ~ {r_lo:.0f}–{r_hi:.0f}px; anisotropy CV={cv:.3f})\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(f\"{model_name} — peak ring: ~{r_lo:.0f}–{r_hi:.0f} px, anisotropy (CV across angle) = {cv:.3f}\")\n",
    "    return (r_idx, (r_lo, r_hi), cv, Mmean)\n",
    "\n",
    "r_ols, band_ols, cv_ols, M_ols   = polar_summary(shap_ols_imgs,   \"OLS\")\n",
    "r_rid, band_rid, cv_rid, M_rid   = polar_summary(shap_ridge_imgs, \"Ridge\")\n",
    "r_cnn, band_cnn, cv_cnn, M_cnn   = polar_summary(shap_cnn_imgch,  f\"CNN [{BEST_BASELINE}]\")\n",
    "\n",
    "print(\"\\n[Interpretation — polar summaries]\\n\"\n",
    "      \"• Peak ring should sit on the shell radius; CNN often shows sharper peaks on inner/outer edges.\\n\"\n",
    "      \"• Anisotropy (CV) near zero ⇒ isotropic ring; higher CV ⇒ focus on arcs (useful for local defects).\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# CNN regional ablation + effect sizes (optional sanity)\n",
    "# -----------------------------\n",
    "print(\"\\n[CNN regional ablation + effect sizes]\")\n",
    "K_REGIONS = 12\n",
    "flat_scores = M_cnn.ravel()\n",
    "top_idx = np.argsort(-flat_scores)[:K_REGIONS]\n",
    "top_regions = [(i // NB_THETA, i % NB_THETA) for i in top_idx]\n",
    "\n",
    "mask_top = np.zeros((H, W), dtype=bool)\n",
    "for (rr, tt) in top_regions:\n",
    "    mask_top |= ((rad_bin == rr) & (theta_bin == tt))\n",
    "\n",
    "X_cnn_test_4_ablate = X_cnn_test_4.copy()\n",
    "X_cnn_test_4_ablate[:, 0, :, :] = np.where(mask_top[None, :, :], 0.0, X_cnn_test_4_ablate[:, 0, :, :])\n",
    "\n",
    "y_pred_cnn_base = y0_cnn_pred\n",
    "y_pred_cnn_abl  = cnn_predict_y(shap_model, X_cnn_test_4_ablate)\n",
    "\n",
    "r2_cnn_base = r2_score(y_obs_test_sel, y_pred_cnn_base)\n",
    "r2_cnn_abl  = r2_score(y_obs_test_sel, y_pred_cnn_abl)\n",
    "print(f\"  CNN R² (baseline on this subset): {r2_cnn_base:.4f}\")\n",
    "print(f\"  CNN R² after ablating top‑{K_REGIONS} polar regions: {r2_cnn_abl:.4f}\")\n",
    "print(f\"  R² drop: {r2_cnn_base - r2_cnn_abl:.4f} (larger ⇒ those regions matter)\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Augmentation‑invariance sanity check (flips)\n",
    "# -----------------------------\n",
    "print(\"\\n[Augmentation‑invariance sanity check — CNN SHAP under flips]\")\n",
    "def flip_h(arr):  # horizontal flip\n",
    "    return arr[..., :, ::-1]\n",
    "\n",
    "Xt_flip_np = flip_h(X_cnn_test_4.copy()).copy()\n",
    "Xt_flip = torch.from_numpy(Xt_flip_np).to(DEVICE).requires_grad_(True)\n",
    "torch.set_grad_enabled(True)\n",
    "e_best = shap.DeepExplainer(shap_model, build_baseline(BEST_BASELINE, m=(64 if BEST_BASELINE == \"train_subset\" else 1)))\n",
    "vals_flip = safe_shap_values(e_best, Xt_flip)\n",
    "torch.set_grad_enabled(False)\n",
    "sh_flip = vals_flip[0] if isinstance(vals_flip, list) else vals_flip\n",
    "sh_flip_img_unflipped = flip_h(sh_flip[:, 0]).copy()\n",
    "\n",
    "if HAVE_SPEARMAN:\n",
    "    corrs = []\n",
    "    for i in range(N_EXPLAIN):\n",
    "        a = np.abs(shap_cnn_imgch[i]).ravel()\n",
    "        b = np.abs(sh_flip_img_unflipped[i]).ravel()\n",
    "        if np.std(a) < 1e-12 or np.std(b) < 1e-12:\n",
    "            corrs.append(np.nan)\n",
    "        else:\n",
    "            corrs.append(spearmanr(a, b, nan_policy=\"omit\").correlation)\n",
    "    corrs = np.array(corrs)\n",
    "    print(f\"  Flip‑invariance — Spearman(|SHAP|, original vs unflipped‑flip): \"\n",
    "          f\"median={np.nanmedian(corrs):.3f}, IQR=({np.nanpercentile(corrs, 25):.3f}, {np.nanpercentile(corrs, 75):.3f})\")\n",
    "else:\n",
    "    print(\"  (Install SciPy to compute Spearman rank correlations.)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Integrated Gradients (IG) for triangulation (no external libs)\n",
    "# -----------------------------\n",
    "print(\"\\n[Integrated Gradients (IG) — optional triangulation]\")\n",
    "def integrated_gradients(model_torch, X_np, baseline_np, steps=32):\n",
    "    was_enabled = torch.is_grad_enabled()\n",
    "    try:\n",
    "        torch.set_grad_enabled(True)\n",
    "        model_torch.eval()\n",
    "        X = torch.from_numpy(np.ascontiguousarray(X_np)).to(DEVICE)\n",
    "        B = torch.from_numpy(np.ascontiguousarray(baseline_np)).to(DEVICE)\n",
    "        attr = torch.zeros_like(X)\n",
    "        for s in range(1, steps + 1):\n",
    "            t = s / steps\n",
    "            Xi = B + t * (X - B)\n",
    "            Xi.requires_grad_(True)\n",
    "            out = model_torch(Xi)\n",
    "            grads = torch.autograd.grad(out.sum(), Xi, retain_graph=False, create_graph=False)[0]\n",
    "            attr += grads\n",
    "        attr = (X - B) * attr / steps\n",
    "        return attr.detach().cpu().numpy()\n",
    "    finally:\n",
    "        torch.set_grad_enabled(was_enabled)\n",
    "\n",
    "X_base_ig = X_cnn_test_4.copy(); X_base_ig[:, 0] = 0.0\n",
    "attr_ig = integrated_gradients(shap_model, X_cnn_test_4, X_base_ig, steps=32)[:, 0]\n",
    "\n",
    "if HAVE_SPEARMAN:\n",
    "    igcorrs = []\n",
    "    for i in range(N_EXPLAIN):\n",
    "        a = np.abs(attr_ig[i]).ravel()\n",
    "        b = np.abs(shap_cnn_imgch[i]).ravel()\n",
    "        if np.std(a) < 1e-12 or np.std(b) < 1e-12:\n",
    "            igcorrs.append(np.nan)\n",
    "        else:\n",
    "            igcorrs.append(spearmanr(a, b, nan_policy=\"omit\").correlation)\n",
    "    igcorrs = np.array(igcorrs)\n",
    "    print(f\"  IG vs Deep SHAP — Spearman(|attr|): median={np.nanmedian(igcorrs):.3f} \"\n",
    "          f\"IQR=({np.nanpercentile(igcorrs, 25):.3f}, {np.nanpercentile(igcorrs, 75):.3f})\")\n",
    "else:\n",
    "    print(\"  (Install SciPy to compute Spearman rank correlations.)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final, conclusions\n",
    "# -----------------------------\n",
    "print(\"\\n=== Conclusions ===\")\n",
    "print(\"• OLS: clean ring; Ridge: more diffused ring; CNN: crisp double band on inner/outer edges \"\n",
    "      \"with local arcs where defects (or the star) dominate.\")\n",
    "print(\"• Deletion curves (AOPC) quantify faithfulness; higher AOPC → better.\")\n",
    "print(\"• Bounding‑box metrics (largest component) — CNN concentrates |SHAP| into **smaller**, more **angularly tight** regions, \"\n",
    "      \"capturing a **larger share** of |SHAP| with less area, especially on the hard dataset.\")\n",
    "print(\"• Baseline matters; this cell auto‑selects one by IoU@k + AOPC. IG triangulation is included as a sanity check.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "\n",
    "# Interpreting the SHAP results on the **simple** hollow‑sphere dataset (no star)\n",
    "\n",
    "This note interprets the SHAP outputs produced for three models trained on the *simple* dataset:\n",
    "\n",
    "- **OLS**: a plain linear regressor, $\\,\\hat y = Xw + b$.\n",
    "- **Ridge**: a regularised linear regressor, $\\,\\hat y = Xw + b$ with an $L_2$ penalty on $w$.\n",
    "- **CNN**: a small convolutional network that receives a 4‑channel input (image + $x, y, r$). It predicts a $z$‑scored target and is mapped back to $y$ at evaluation time.\n",
    "\n",
    "The dataset contains greyscale images of a hollow metal shell. The label $y$ mainly scales the **whole shell** (global density factor), with small variations from illumination and defects. There is *no* bright local star in this dataset; the only genuine signal is concentrated on the **ring** (inner/outer edges and shell interior).\n",
    "\n",
    "The SHAP framework decomposes a prediction as\n",
    "$$\n",
    "f(x) - \\mathbb{E}[f(X)] \\;=\\; \\sum_{j=1}^D \\phi_j(x),\n",
    "$$\n",
    "where $\\phi_j(x)$ is the contribution (Shapley value) of feature $j$ for input $x$. For linear models with a mean baseline, this simplifies to\n",
    "$$\n",
    "\\phi_j(x) = w_j\\,(x_j - \\mathbb{E}[X_j]).\n",
    "$$\n",
    "\n",
    "Below, each figure or metric is explained, followed by what “good” and “bad” patterns look like in this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Overlay panels: per‑image SHAP on top of the input\n",
    "\n",
    "**What the panels show.** Each small image overlays a SHAP heatmap on the greyscale input; blue/orange indicate negative/positive contributions to $\\hat y$. A strong, thin band on the **shell** is expected because $y$ scales shell intensity.\n",
    "\n",
    "**Observed patterns.**\n",
    "\n",
    "- **OLS:** the overlays show tight bands hugging the inner and outer radii. This is the textbook pattern for a linear model when the label is a global scale of the ring: pixels on the ring move $y$ the most.\n",
    "- **Ridge:** the overlays look more diffuse. Attribution spills into the hollow centre and the outside background. This indicates the model is partly using nuisance correlations (illumination, offset) rather than focusing solely on the ring.\n",
    "- **CNN:** the overlays show crisp arcs on the ring, often emphasising the inner and outer edges. This indicates that the network has learned to anchor on geometric structure (edges) rather than global brightness alone.\n",
    "\n",
    "**How to read colours.** If the inner edge is dark and the outer edge is bright, OLS often assigns opposite signs on the two edges because increasing one pixel while holding the mean baseline changes the predicted scale with opposite effect at different sides of the gradient. The *absolute* value matters most when assessing “where” the model looks.\n",
    "\n",
    "**Pitfalls.** Visual overlays are suggestive but not quantitative; when features are collinear (adjacent pixels on a ring), contributions are spread and can alternate sign. Use the absolute‑value summaries below to avoid being misled by colour.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Global mean $|\\text{SHAP}|$ maps\n",
    "\n",
    "**What the map shows.** Averaging $|\\phi|$ across the explained test subset answers: *“On average, where in the image does the model place credit?”*\n",
    "\n",
    "**Good vs bad.**\n",
    "\n",
    "- A **good** map is an annulus: most energy lies on the shell; the hollow interior and the outside are dark.\n",
    "- A **bad** map shows energy in the background or the hollow centre.\n",
    "\n",
    "**Observed.**\n",
    "- **OLS:** a clean annulus.\n",
    "- **Ridge:** substantial energy in the hollow and outside, implying spurious reliance on global nuisance variation.\n",
    "- **CNN:** a clean annulus, often with a slightly stronger inner rim, consistent with edge‑based features inside the network.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Radial profiles of $|\\text{SHAP}|$\n",
    "\n",
    "**Definition.** Pixels are binned by radius $r$ from the centre; the plot shows the mean $|\\phi|$ per radius bin:\n",
    "$$\n",
    "\\text{profile}(r) \\;=\\; \\mathbb{E}[\\,|\\phi(x,y)| \\mid \\text{radius}(x,y)=r\\,].\n",
    "$$\n",
    "\n",
    "**Expected shape.** Two peaks at the inner and outer shell boundaries (around $\\sim20$ px and $\\sim40$ px) because $y$ depends on shell intensity.\n",
    "\n",
    "**Observed.**\n",
    "- **OLS:** a sharp peak near the inner boundary and a shoulder towards the outer boundary — a very clear “ring” signature.\n",
    "- **Ridge:** no clear double‑peak; the curve is noisy and relatively flat, reflecting diffuse credit.\n",
    "- **CNN:** a strong inner peak with a secondary outer peak — the desired pattern for an edge‑aware model.\n",
    "\n",
    "**Pitfalls.** If the shell mask used for binning is too wide or mis‑centred, peaks blur. Always check geometry constants match the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Region‑based metrics (area‑biased vs area‑fair)\n",
    "\n",
    "Two complementary summaries are reported.\n",
    "\n",
    "- **Region share (area‑biased):** fraction of total $|\\phi|$ mass inside each region (hollow / shell / outside).\n",
    "- **Mean per pixel (area‑fair):** average $|\\phi|$ per pixel *within* each region, comparable despite different areas.\n",
    "\n",
    "**Numbers (provided).**\n",
    "\n",
    "### OLS\n",
    "- **Share:** Hollow **3.33%**, Shell **91.49%**, Outside **5.18%**.\n",
    "- **Per pixel:** Hollow **0.000068**, Shell **0.000424**, Outside **0.000024**.\n",
    "- **Interpretation:** Almost all attribution sits on the shell (good). Per‑pixel intensity in the shell is ≈ **18×** higher than outside ($0.000424/0.000024$), which is a strong localisation score.\n",
    "\n",
    "### Ridge\n",
    "- **Share:** Hollow **11.26%**, Shell **36.84%**, Outside **51.90%**.\n",
    "- **Per pixel:** Hollow **0.000091**, Shell **0.000068**, Outside **0.000096**.\n",
    "- **Interpretation:** More than half the mass is **outside**. Per‑pixel intensity is actually larger outside than on the shell (≈ **1.4×**), signalling reliance on background. This is a red flag for interpretability.\n",
    "\n",
    "### CNN (baseline = `train_subset`)\n",
    "- **Share:** Hollow **4.86%**, Shell **93.63%**, Outside **1.50%**.\n",
    "- **Per pixel:** Hollow **0.000065**, Shell **0.000292**, Outside **0.000005**.\n",
    "- **Interpretation:** Strong focus on the shell (excellent). Per‑pixel intensity in the shell is ≈ **58×** larger than outside, the crispest localisation of the three.\n",
    "\n",
    "**Pitfalls.** “Share” is influenced by region area: the shell covers many pixels, so shares are naturally large. The per‑pixel metric controls for this and is often the better fairness check.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Concentration of importance (sparsity of $|\\text{SHAP}|$)\n",
    "\n",
    "**Definition.** The **concentration** at top‑$p\\%$ is\n",
    "$$\n",
    "\\text{Conc}_{p\\%} \\;=\\;\n",
    "\\frac{\\sum_{j \\in \\text{top }p\\%\\text{ by }|\\phi|} |\\phi_j|}\n",
    "{\\sum_{j} |\\phi_j|}.\n",
    "$$\n",
    "\n",
    "**Numbers (provided).**\n",
    "- **OLS:** top‑1% → **29.33%**, top‑5% → **57.68%** (highly concentrated; a small fraction of pixels carries most of the credit).\n",
    "- **Ridge:** top‑1% → **7.79%**, top‑5% → **23.32%** (diffuse and noisy credit).\n",
    "- **CNN:** top‑1% → **20.13%**, top‑5% → **51.98%** (concentrated but spread along thin arcs).\n",
    "\n",
    "**Interpretation.** Higher concentration indicates sharper, more local attributions. Extremely high values can also appear if the model uses a single pixel artefact; always corroborate with the region metrics and overlays.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Alignment with ground truth shell (IoU and Precision at top‑$k$)\n",
    "\n",
    "**Definition.** For the top‑$k\\%$ $|\\phi|$ mask $M$ and the shell mask $S$,\n",
    "$$\n",
    "\\text{IoU} \\;=\\; \\frac{|M \\cap S|}{|M \\cup S|}, \\qquad\n",
    "\\text{Precision} \\;=\\; \\frac{|M \\cap S|}{|M|}.\n",
    "$$\n",
    "\n",
    "**Numbers (provided).**\n",
    "\n",
    "- **OLS:** Precision at 1–10% ≈ **0.98–0.99**; IoU rises from **0.021 → 0.217**.\n",
    "- **Ridge:** Precision **0.10 → 0.30**, IoU **0.002 → 0.058**.\n",
    "- **CNN:** Precision **0.985–0.989**, IoU **0.022 → 0.217**.\n",
    "\n",
    "**Interpretation.**\n",
    "- Precision near 1.0 means the **top‐ranked** pixels almost all fall on the shell (good). The small IoU is expected because the top‑$k$ mask covers a tiny fraction of the shell area; union is large, intersection is small. Use precision as the main signal here.\n",
    "- Ridge’s low precision confirms the diffuse overlays: many top‑ranked pixels land **off** the ring.\n",
    "\n",
    "**Pitfalls.** IoU is not scale‑free: for very sparse masks, a model can be perfectly aligned yet score a small IoU. Report both precision and IoU to avoid misinterpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Deletion curves and AOPC (faithfulness check)\n",
    "\n",
    "**Procedure.** Iteratively zero out the top‑$p\\%$ $|\\phi|$ pixels in the **image channel** (keeping coordinate channels intact for the CNN) and re‑score on the same explained subset. The plot shows $R^2$ after deletion. The **AOPC** score is the area under the $R^2$ drop curve:\n",
    "$$\n",
    "\\text{AOPC} \\;=\\; \\int_0^{p_{\\max}} \\big(R^2_{\\text{baseline}} - R^2_{\\text{after deletion at }p}\\big)\\, dp.\n",
    "$$\n",
    "Higher AOPC indicates that removing SHAP‑ranked pixels impairs the model the most ⇒ **more faithful** attribution.\n",
    "\n",
    "**Numbers (provided).**\n",
    "- **CNN:** AOPC ≈ **0.1586** for the chosen baseline (`train_subset`).\n",
    "- **OLS:** (from plot) AOPC ≈ **0.13** — noticeable drop, consistent with focused ring attributions.\n",
    "- **Ridge:** AOPC ≈ **0.036** — relatively small; the model is less harmed by removing its top‑ranked pixels, consistent with diffuse/noisy SHAP.\n",
    "\n",
    "**Baseline sensitivity.** For the CNN, alternative backgrounds give very similar fidelity: mean baseline AOPC ≈ **0.1623**, median ≈ **0.1582**, blurred mean ≈ **0.1550**. This closeness is a healthy sign; wildly different AOPCs across baselines suggest unstable explanations.\n",
    "\n",
    "**Pitfalls.**\n",
    "- Deletion creates **distribution shift** (images with blacked‑out pixels are not from the data distribution). This is a proxy test, not a proof.\n",
    "- If the model uses global averages or coordinate channels, deleting image pixels may not fully disrupt its computation; interpret in that context.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Baseline bootstrapping (robustness across CNN backgrounds)\n",
    "\n",
    "Using several random train‑subset backgrounds for Deep SHAP gives **stable** region shares for the CNN:\n",
    "- Hollow/Shell/Outside ≈ **0.050±0.003 / 0.935±0.003 / 0.016±0.000**.\n",
    "\n",
    "Small standard deviations indicate low sensitivity to which training images define the background distribution. This reduces the risk that results are an artefact of a single background choice.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Putting it together — model‑by‑model\n",
    "\n",
    "**OLS (linear, no regularisation)**\n",
    "- Visual and quantitative evidence agree: attributions concentrate on the **shell** (share ≈ 91.5%; precision ≈ 0.98).\n",
    "- High concentration of $|\\phi|$ (top‑5% captures ≈ 57.7%) and a clear two‑peak radial profile.\n",
    "- Good deletion behaviour (AOPC ≈ 0.13).  \n",
    "**Take‑home:** OLS is a good explainer on this dataset because the label is essentially a linear global scale of the ring.\n",
    "\n",
    "**Ridge (linear + $L_2$)**\n",
    "- Attribution mass drifts off the ring (outside share ≈ 51.9%), precision low (≤ 0.30).\n",
    "- Radial profile lacks sharp peaks; concentration is low.  \n",
    "**Take‑home:** The regulariser and standardisation damp useful contrasts and spread credit to nuisance signals, so explanations are less reliable here.\n",
    "\n",
    "**CNN (edge‑aware, with coordinates)**\n",
    "- Strong ring localisation (shell share ≈ 93.6%, precision ≈ 0.99) and crisp radial peaks.\n",
    "- High concentration and the strongest AOPC among the three. Baseline bootstrapping shows stability.  \n",
    "**Take‑home:** The CNN discovers physically meaningful cues (edges, shell arcs). Despite extra capacity, attribution remains local and faithful on this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Common traps and how to avoid them\n",
    "\n",
    "- **Confusing sign with importance.** Use $|\\phi|$ to assess *where* the model looks; the sign depends on the baseline and on how intensities map to $y$.\n",
    "- **Over‑interpreting IoU.** Small IoU at very small $k$ is normal for thin structures; focus on precision and per‑pixel means.\n",
    "- **Ignoring baselines.** Deep SHAP depends on a background distribution. Check several baselines and report stability (as done above).\n",
    "- **Assuming deletion is causal.** Deletion scores are helpful but not causal proofs; combine with multiple diagnostics.\n",
    "- **Geometry mismatch.** If the inner/outer radii used to define the shell masks are off, every radial/IoU metric degrades. Confirm geometry constants before comparing runs.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) What “good” looks like on this dataset\n",
    "\n",
    "- Overlay panels: thin arcs/bands exactly on the shell.\n",
    "- Global mean $|\\text{SHAP}|$: clean annulus.\n",
    "- Radial profile: two distinct peaks near the known inner/outer radii.\n",
    "- Region metrics: shell share $\\gtrsim 90\\%$, shell per‑pixel $|\\phi|$ $\\gg$ outside.\n",
    "- Precision@k: $\\approx 0.98$–$0.99$ for $k \\in \\{1,2,5,10\\}\\%$.\n",
    "- Concentration: top‑1% $\\gtrsim 20\\%$, top‑5% $\\gtrsim 50\\%$.\n",
    "- Deletion (AOPC): large and smooth $R^2$ drop.\n",
    "\n",
    "Anything markedly different (e.g., energy in the background, flat radial profile, low precision) indicates reliance on nuisance signals or a mismatch between model and data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# SHAP interpretations — **simple (no‑star) dataset** - part 2\n",
    "\n",
    "This note interprets the SHAP outputs for OLS, Ridge, and the CNN when trained and explained on the **simple hollow‑sphere** dataset (no star). The aim is to read each visual and metric in context and to separate *global* behaviour from *local* behaviour.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Reading the figures at a glance\n",
    "\n",
    "- **Overlays (grey image + red/blue SHAP):** red ≈ positive contribution to $\\hat y$, blue ≈ negative; the magnitude is the opacity.\n",
    "- **Global mean $|{\\rm SHAP}|$:** average absolute attribution per pixel over the explained subset; reveals *where* the model tends to look.\n",
    "- **Radial profile:** mean $|{\\rm SHAP}|$ as a function of radius; inner and outer shell edges should appear as peaks.\n",
    "- **Region shares vs shell:** split attributions across *hollow* (inside), *shell*, and *outside*.\n",
    "- **IoU@k / Precision@k:** alignment of the model’s **top‑$k$%** $|{\\rm SHAP}|$ pixels with the ground‑truth **shell mask**.\n",
    "- **Deletion curve (AOPC):** remove the top‑$|{\\rm SHAP}|$ pixels and re‑score $R^2$; faithful explanations produce faster drops.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Definitions (used throughout)\n",
    "\n",
    "Let $S\\in\\mathbb{R}^{H\\times W}$ be a SHAP map (image channel), and let $\\Omega\\subset\\{1,\\dots,H\\}\\times\\{1,\\dots,W\\}$ be a region (hollow, shell, outside).\n",
    "\n",
    "**Region share (area‑biased):**\n",
    "$$\n",
    "\\mathrm{share}_\\Omega(S)\n",
    "=\\frac{\\sum_{(i,j)\\in\\Omega}|S_{ij}|}{\\sum_{(i,j)}|S_{ij}|}.\n",
    "$$\n",
    "\n",
    "**Region mean (area‑fair):**\n",
    "$$\n",
    "\\mathrm{mean}_\\Omega(S)=\\frac{1}{|\\Omega|}\\sum_{(i,j)\\in\\Omega}|S_{ij}|.\n",
    "$$\n",
    "\n",
    "**Top‑$p$ concentration:**\n",
    "$$\n",
    "\\mathrm{conc}_p(S)=\\frac{\\sum_{m\\in\\mathcal{T}_p}|S_m|}{\\sum_m |S_m|},\n",
    "$$\n",
    "where $\\mathcal{T}_p$ contains the top $p$ fraction of pixels by $|S|$.\n",
    "\n",
    "**Alignment vs shell (IoU / Precision at $k\\%$):**\n",
    "$$\n",
    "\\mathrm{IoU}_k=\\frac{|M_k\\cap \\text{Shell}|}{|M_k\\cup \\text{Shell}|},\\qquad\n",
    "\\mathrm{Prec}_k=\\frac{|M_k\\cap \\text{Shell}|}{|M_k|},\n",
    "$$\n",
    "where $M_k$ is the binary mask of the top‑$k\\%$ $|S|$.\n",
    "\n",
    "**Deletion AOPC** (area under performance drop curve):\n",
    "$$\n",
    "\\mathrm{AOPC}\n",
    "=\\int_0^{f_{\\max}}\\big(R^2_{\\text{base}}-R^2(f)\\big)\\,\\mathrm{d}f\n",
    "\\;\\;\\approx\\;\\;\\sum_\\ell \\tfrac12\\,(d_{\\ell}+d_{\\ell+1})\\,(f_{\\ell+1}-f_\\ell),\n",
    "$$\n",
    "with $d_\\ell=R^2_{\\text{base}}-R^2(f_\\ell)$. Larger is better (faster degradation when removing truly important pixels).\n",
    "\n",
    "**Bounding‑box selection (top‑$p$% in shell):**\n",
    "$$\n",
    "M\n",
    "=\\operatorname{LargestComponent}\\Big(\\big\\{|S|\\ge t_p(S\\,|\\,\\text{shell})\\big\\}\\cap\\text{Shell}\\Big).\n",
    "$$\n",
    "\n",
    "**Star‑disc share & enrichment** (used later as a “local‑cue” probe, even without a real star):\n",
    "$$\n",
    "\\text{share}=\\frac{\\sum_{D\\cap\\text{Shell}}|S|}{\\sum_{\\text{Shell}}|S|},\\qquad\n",
    "\\text{enrich}=\\frac{\\text{share}}{\\frac{|D\\cap\\text{Shell}|}{|\\text{Shell}|}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3) OLS — interpretation\n",
    "\n",
    "**What is expected:** a clear ring since the label is a global scale of ring intensity. OLS is linear in pixels and will put weight mainly on the shell boundaries.\n",
    "\n",
    "**Numbers**\n",
    "- **Region share:** Hollow **3.33%**, Shell **91.49%**, Outside **5.18%**.\n",
    "- **Per‑pixel mean:** Hollow **6.8e‑5**, Shell **4.24e‑4**, Outside **2.4e‑5**.\n",
    "- **Concentration:** top‑1% **29.33%**, top‑5% **57.68%**.\n",
    "- **IoU / Precision:** at 1%: IoU **0.021**, Precision **0.967**; at 10%: IoU **0.217**, Precision **0.984**.\n",
    "- **Deletion AOPC:** **0.1317** (good for a linear model).\n",
    "\n",
    "**Reading the plots**\n",
    "- **Overlays / global mean:** a tight band on the ring; small bleed outside due to slight mis‑registration and noise.\n",
    "- **Radial profile:** a sharp peak near the inner edge and a secondary shoulder approaching the outer edge — edges carry the strongest linear signal.\n",
    "\n",
    "**Local vs global**\n",
    "- **Local:** the top‑1% mask forms thin arcs on the ring.\n",
    "- **Global:** the average map is annular, consistent with a pixelwise linear explanation of a radial shell.\n",
    "\n",
    "**Pitfalls**\n",
    "- High **precision** but **low IoU** at small $k$ is normal: a thin ring occupies a small fraction of the image; even a perfect ring mask has low IoU at tiny $k$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Ridge — interpretation\n",
    "\n",
    "**What is expected:** similar ring signal but more **diffuse** because the $\\ell_2$ penalty shrinks and spreads coefficients.\n",
    "\n",
    "**Numbers**\n",
    "- **Region share:** Hollow **11.26%**, Shell **36.84%**, Outside **51.90%**.\n",
    "- **Per‑pixel mean:** Hollow **9.1e‑5**, Shell **6.8e‑5**, Outside **9.6e‑5**.\n",
    "- **Concentration:** top‑1% **7.79%**, top‑5% **23.32%**.\n",
    "- **IoU / Precision:** at 1%: IoU **0.002**, Precision **0.102**; at 10%: IoU **0.058**, Precision **0.302**.\n",
    "- **Deletion AOPC:** **0.0356** (weak fidelity).\n",
    "\n",
    "**Reading the plots**\n",
    "- **Overlays:** scattered red/blue speckle across the frame; still some ring structure but attribution “escapes” into the background.\n",
    "- **Radial profile:** flat with only faint peaks → credit is not concentrated on the physical edges.\n",
    "\n",
    "**Interpretation**\n",
    "- Ridge stabilises training but the explanation becomes **low‑contrast**: the model assigns small weights broadly, so no small set of pixels dominates performance. This is consistent with the low AOPC.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) CNN — interpretation (baseline selected by fidelity)\n",
    "\n",
    "**What is expected:** $|{\\rm SHAP}|$ concentrates along **both** inner and outer edges; attributions appear as short **arcs** because small misalignments and augmentation place edge evidence locally.\n",
    "\n",
    "**Numbers**\n",
    "- **Baseline chosen:** `train_subset` (near‑ties with mean/median).\n",
    "- **Bootstrapping:** region share mean±sd = Hollow **0.050±0.003**, Shell **0.935±0.003**, Outside **0.016±0.000**.\n",
    "- **Region share:** Hollow **4.86%**, Shell **93.63%**, Outside **1.50%**.\n",
    "- **Per‑pixel mean:** Hollow **6.5e‑5**, Shell **2.92e‑4**, Outside **5e‑6**.\n",
    "- **Concentration:** top‑1% **20.13%**, top‑5% **51.98%**.\n",
    "- **IoU / Precision:** at 1%: IoU **0.022**, Precision **0.985**; at 10%: IoU **0.217**, Precision **0.984**.\n",
    "- **Deletion AOPC:** **0.1586** (best of the three).\n",
    "\n",
    "**Reading the plots**\n",
    "- **Overlays / global mean:** crisp double band at inner/outer edges; local arcs rather than full rings — the network exploits *local* edge evidence plus global context (provided by the CoordConv channels).  \n",
    "- **Radial profile:** two clear peaks at the expected radii, sharper than OLS/Ridge.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Bounding‑box metrics (top‑1% $|{\\rm SHAP}|$ *within the shell*)\n",
    "\n",
    "The box is fitted to the **largest connected component** of the top‑1% $|{\\rm SHAP}|$ in the shell. Reported metrics:\n",
    "\n",
    "- **Area fraction:** $|\\text{BOX}\\cap\\text{Shell}|/|\\text{Shell}|$.\n",
    "- **$|{\\rm SHAP}|$ share captured:** $\\sum_{\\text{BOX}\\cap\\text{Shell}}|S| / \\sum_{\\text{Shell}}|S|$.\n",
    "- **Angular coverage:** minimal angle span covering the selected pixels (degrees).\n",
    "- **Radial width / shell thickness.**\n",
    "\n",
    "**Medians (IQR):**\n",
    "- **OLS:** area **0.002** (0.002–0.003), share **0.025** (0.020–0.033), angle **9.8°** (6.9°–14.1°), radial **0.057** (0.042–0.066).\n",
    "- **Ridge:** area **0.003** (0.002–0.004), share **0.013** (0.007–0.016), angle **13.9°** (7.3°–17.1°), radial **0.041** (0.023–0.053).\n",
    "- **CNN:** area **0.005** (0.003–0.007), share **0.036** (0.022–0.053), angle **11.2°** (8.6°–16.6°), radial **0.093** (0.073–0.127).\n",
    "\n",
    "**Interpretation:** CNN captures **more** of the shell’s total evidence with a slightly larger box (because it often spans both edges), while Ridge captures **less** despite comparable area.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) “Global star‑capture” on a dataset **without** a star\n",
    "\n",
    "The procedure still finds a **proxy**: a small disc on the shell at the **brightest ring‑weighted blob**.  \n",
    "Metrics remain valid but should be read as *“does the model favour any local arc on the ring?”*\n",
    "\n",
    "- **$|{\\rm SHAP}|$ in the disc (share):** OLS **0.005**, Ridge **0.008**, CNN **0.012** (medians).\n",
    "- **Enrichment (share / area):** OLS **0.458**, Ridge **0.697**, CNN **1.132**.\n",
    "- **Angular error** (disc centre vs largest component): broad IQRs and large medians (e.g., OLS **51°**, CNN **76°**) → no consistent local hotspot, as expected when there is **no true star**.\n",
    "\n",
    "**Good sign:** enrichment close to **1** with large angular variability — indicates no persistent spurious cue.  \n",
    "**Bad sign:** consistently **high** enrichment with **small** angular error — would suggest an artefact acting like a fixed defect.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Polar superpixels (regional summary)\n",
    "\n",
    "Binning the image into $(r,\\theta)$ cells gives an angular profile at the **peak ring**.\n",
    "\n",
    "- **OLS:** peak ring ≈ inner edge; **CV ≈ 0.142** (mild anisotropy).\n",
    "- **Ridge:** peak ring faint; **CV ≈ 0.105** (nearly isotropic but weak signal).\n",
    "- **CNN:** peak ring between inner/outer edges; **CV ≈ 0.342** (deliberate focus on *arcs*).\n",
    "\n",
    "Here, $\\mathrm{CV}=\\mathrm{std}(m_\\theta)/\\mathrm{mean}(m_\\theta)$ for the angular means $m_\\theta$.  \n",
    "Small CV ⇒ isotropic ring; larger CV ⇒ the model locks onto *segments* (useful if local defects matter).\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Sanity checks\n",
    "\n",
    "- **Flip‑invariance (CNN):** Spearman correlation of $|{\\rm SHAP}|$ before/after a horizontal flip (then unflipped) — median **0.718**.  \n",
    "  Strong but not perfect: augmentation and local pooling create small asymmetries.\n",
    "- **Integrated Gradients vs Deep SHAP:** Spearman between $|{\\rm IG}|$ and $|{\\rm SHAP}|$ — median **0.485**.  \n",
    "  Different estimators agree moderately; disagreement tends to appear off‑ring.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Takeaways for the **simple** dataset\n",
    "\n",
    "- **OLS** provides a tidy, physically plausible annulus; it explains a sizeable portion of performance with a compact set (AOPC **0.1317**).\n",
    "- **Ridge** spreads credit broadly and is less faithful (AOPC **0.0356**).\n",
    "- **CNN** concentrates on **edge arcs** and is the most faithful by deletion (AOPC **0.1586**).  \n",
    "  Local boxes capture more $|{\\rm SHAP}|$ with modest angular spans — exactly what an edge‑seeking CNN should do on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "# Synthetic X‑ray dataset — “Death‑Star” contrast (energy‑neutral star)\n",
    "\n",
    "**Purpose.** Construct a dataset where the **star** is an unmistakable **local** driver of the target while a **small global anchor** still exists. The star is injected so that, on average **within the shell**, global brightness **does not increase** (energy‑neutral construction). Linear models that rely on per‑image mean or broad ring structure therefore struggle, whereas a CNN can localise the star at **any** angle on the ring and gain a strong predictive advantage.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Image formation and label (high‑level)\n",
    "\n",
    "Let the base hollow‑sphere density be a function on pixels $(i,j)$:\n",
    "- inner radius $R_0 \\approx 20$, outer radius $R_1 \\approx 40$;\n",
    "- the shell profile fades from the inner to the outer boundary with a shape exponent $\\gamma\\!\\in\\![0.95,1.05]$;\n",
    "- gentle low‑frequency **shading** $S(i,j)$ is applied **multiplicatively in the density domain**.\n",
    "\n",
    "A **smooth sensor response** converts density $x$ to brightness\n",
    "$$\n",
    "g(x) \\;=\\; 1 - e^{-x/\\tau}, \\qquad \\tau>0,\n",
    "$$\n",
    "so images stay in $[0,1)$ without hard clipping and with star highlights preserved.\n",
    "\n",
    "The observed label is\n",
    "$$\n",
    "y \\;\\approx\\; c_0\\,y_{\\text{density}}\n",
    "\\;+\\; \\beta_{\\text{edge}}\\,E\n",
    "\\;-\\; \\beta_{\\text{crack}}\\,C\n",
    "\\;-\\; \\beta_{\\text{void}}\\,V\n",
    "\\;-\\; \\beta_{\\text{aniso}}\\,A\n",
    "\\;-\\; \\beta_{\\text{int}}\\,(\\!C\\!\\cdot\\!A\\!)\n",
    "\\;+\\; \\gamma_{\\star}\\,y_{\\star}\n",
    "\\;+\\; \\text{label noise},\n",
    "$$\n",
    "where\n",
    "- $y_{\\text{density}}$ is a **small** global scaling ($\\approx\\!1\\pm2\\%$) to keep ordinary least squares (OLS) and Ridge meaningful but not dominant;\n",
    "- $E,C,V,A$ encode shell edges, cracks, voids and anisotropy from the **pre‑sensor** density field (all at **small** weight);\n",
    "- $y_{\\star}$ is a **post‑sensor**, area‑normalised **star brightness** (defined below) that dominates label variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Energy‑neutral star (why linear models struggle)\n",
    "\n",
    "A five‑arm star is placed **on the shell** at a random angle every time. In the **density domain**, let\n",
    "$$\n",
    "\\text{add}(i,j) \\;=\\; \\text{STAR\\_DENSITY\\_ADD} \\times \\text{star\\_map}(i,j),\n",
    "$$\n",
    "and let $\\mathbb{1}_{\\text{shell}}(i,j)$ be the shell mask. Define the **shell mean** of the addition\n",
    "$$\n",
    "\\mu_{\\text{add}} \\;=\\; \\frac{1}{|\\text{shell}|}\\sum_{(i,j)\\in\\text{shell}} \\text{add}(i,j).\n",
    "$$\n",
    "Apply an energy‑neutral **compensation**\n",
    "$$\n",
    "\\text{comp} \\;=\\; \\text{STAR\\_COMPENSATE}\\times \\mu_{\\text{add}},\n",
    "$$\n",
    "and form two compensated density fields\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{dens0c}(i,j)   &= \\max\\big(\\text{dens0}(i,j) - \\text{comp}\\,\\mathbb{1}_{\\text{shell}}(i,j),\\,0\\big),\\\\[2pt]\n",
    "\\text{dens\\_star}(i,j) &= \\max\\big(\\text{dens0}(i,j) + \\text{add}(i,j) - \\text{comp}\\,\\mathbb{1}_{\\text{shell}}(i,j),\\,0\\big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "- The **mean density on the shell** is preserved (up to shading and the subsequent non‑linear sensor), so the **per‑image mean brightness** carries **little** information about the star.\n",
    "- Because the star appears at a **random angle**, any single **fixed‑pixel** linear weight map cannot consistently align with it across images. A CNN, however, can learn **local** star detectors that match anywhere on the ring.\n",
    "\n",
    "Finally, brightness “without star” and “with star” are\n",
    "$$\n",
    "I_0 = g\\big(\\text{dens0c}\\times S\\big), \\qquad I_{\\star} = g\\big(\\text{dens\\_star}\\times S\\big).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Star term in the label (what the model is rewarded to see)\n",
    "\n",
    "Define the **post‑sensor excess brightness** due to the star\n",
    "$$\n",
    "\\Delta I \\;=\\; I_{\\star} - I_0 \\;\\;\\ge 0,\n",
    "$$\n",
    "and its **area‑normalised** mean over the shell\n",
    "$$\n",
    "y_{\\star} \\;=\\; \\frac{1}{|\\text{shell}|}\\sum_{(i,j)\\in\\text{shell}} \\Delta I(i,j).\n",
    "$$\n",
    "\n",
    "The noise‑free label is\n",
    "$$\n",
    "\\boxed{\\;\n",
    "y_{\\text{true}}\n",
    "= c_0\\,y_{\\text{density}}\n",
    "+ \\beta_{\\text{edge}}\\,E\n",
    "- \\beta_{\\text{crack}}\\,C\n",
    "- \\beta_{\\text{void}}\\,V\n",
    "- \\beta_{\\text{aniso}}\\,A\n",
    "- \\beta_{\\text{int}}\\,(C\\!\\cdot\\!A)\n",
    "+ \\gamma_{\\star}\\,y_{\\star}\n",
    "\\;}\n",
    "$$\n",
    "and the observed label is $y_{\\text{obs}}=y_{\\text{true}}+\\varepsilon$, with small Gaussian noise. This construction ensures that the label **rewards exactly what the sensor shows**; a CNN that highlights the star in the image will be rewarded by $y_{\\star}$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Why the chosen sensor helps (smooth, no hard clipping)\n",
    "\n",
    "The response $g(x)=1-e^{-x/\\tau}$ is **monotone** and **concave**, producing bright highlights that **do not flatten** at 1.0. This avoids:\n",
    "- gradient dead‑zones (typical of hard clipping);\n",
    "- spuriously high counts at pixel value 1.0;\n",
    "- “cheating” via global exposure changes.\n",
    "\n",
    "**Diagnostic expectation:** “Pixel mass near 1.0” $\\approx 0\\%$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) What each diagnostic now means\n",
    "\n",
    "Let $m$ be the per‑image mean brightness and let $s$ be the stored star‑brightness proxy ($y_{\\star}$).\n",
    "\n",
    "- **Corr$(y_{\\text{obs}}, m)$** — should be **modest** (target range 0.25–0.45).  \n",
    "  A larger value suggests the star is unintentionally changing the global mean (reduce $\\text{Y\\_DENSITY\\_RANGE}$ or increase $\\text{STAR\\_COMPENSATE}$).\n",
    "\n",
    "- **Corr$(y_{\\text{obs}}, s)$** — should be **high** (aim $\\ge 0.65$).  \n",
    "  Confirms that the label is actually driven by the **post‑sensor** star signal.\n",
    "\n",
    "- **Incremental $R^2$** from adding the star proxy to a mean‑only model:  \n",
    "  Fit $y\\approx b_0+b_1 m$ (model 1) and $y\\approx b_0+b_1 m + b_2 s$ (model 2). Report\n",
    "  $$\n",
    "  \\Delta R^2 \\;=\\; R^2_{\\text{model 2}} - R^2_{\\text{model 1}} \\;\\;\\text{(aim }\\ge 0.30\\text{)}.\n",
    "  $$\n",
    "  This quantifies how much **extra** variance the star explains beyond global brightness.\n",
    "\n",
    "- **Mean image** — shows the shell but **no star imprint** (the star angle is random and cancels in the average).\n",
    "\n",
    "- **Label histogram** — typically unimodal with wider spread than the simple/no‑star dataset due to the star term.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Why OLS/Ridge underperform and CNN excels\n",
    "\n",
    "- **OLS/Ridge:** features are **fixed pixel locations**. A star at arbitrary angles cannot be represented by a **single** static weight pattern; any average weight map dilutes the star across angles. Energy‑neutral construction further removes the “mean brightness” shortcut.\n",
    "- **CNN:** learns **local** filters and, via pooling and ring priors, is **equivariant** to the star’s angular placement. The star branch and concat pooling create a path where small, intense features survive global averaging and influence the output.\n",
    "\n",
    "**Practical expectation:** Ridge $R^2$ on Test stays modest (often $\\ll 0.5$), whereas the CNN achieves a substantially higher $R^2$ by focusing on the star.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Visual guide: what “good” vs “bad” looks like\n",
    "\n",
    "- **Good (as designed):**\n",
    "  - Pixel mass near 1.0 $\\approx 0\\%$.\n",
    "  - Corr$(y_{\\text{obs}}, m)$ in 0.25–0.45.\n",
    "  - Corr$(y_{\\text{obs}}, s)\\ge 0.65$.\n",
    "  - $\\Delta R^2 \\ge 0.30$.\n",
    "  - Mean image shows **no star**.\n",
    "  - Sample images: a crisp, bright star on the shell; background and ring remain realistic.\n",
    "\n",
    "- **Bad (needs tuning):**\n",
    "  - Pixel mass near 1.0 $\\gg 0\\%$ → decrease brightness or increase $\\tau$ (sensor less aggressive).\n",
    "  - Corr$(y_{\\text{obs}}, m)$ too high → increase $\\text{STAR\\_COMPENSATE}$ or narrow $\\text{Y\\_DENSITY\\_RANGE}$; reduce $\\text{SHADING\\_ALPHA\\_RANGE}$ if global illumination dominates.\n",
    "  - Corr$(y_{\\text{obs}}, s)$ too low → increase $\\text{STAR\\_DENSITY\\_ADD}$ or $\\gamma_{\\star}$; consider slightly sharper arms/core.\n",
    "  - Mean image displays a faint star ghost → check that the star angle is uniformly random; ensure compensation is on the **shell only**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Implementation notes tied to the code\n",
    "\n",
    "1) **Base shell.**  \n",
    "   The ring image $B(i,j)$ is built from the elliptical radius $r$ using a normalised depth $t=(r-R_0)/(R_1-R_0)$ and a profile $B=(1-t)^{\\gamma}$ on the shell, zero elsewhere.\n",
    "\n",
    "2) **Defects.**  \n",
    "   Anisotropy, cracks and voids adjust the **density** before the sensor. Their weights in the label are **small** so the star dominates without being the only signal.\n",
    "\n",
    "3) **Star map.**  \n",
    "   A 5‑arm pattern with tight Gaussian core/arms is placed at $(r_{\\star},\\theta_{\\star})\\approx$ mid‑shell $\\pm$ jitter. Multiplying by $\\text{STAR\\_DENSITY\\_ADD}$ yields a strong local addition in the density domain.\n",
    "\n",
    "4) **Energy‑neutral compensation.**  \n",
    "   Subtract a constant $\\text{comp}$ **only inside the shell** so the shell‑mean density remains unchanged. This keeps the **global mean** anchor small.\n",
    "\n",
    "5) **Shading in density domain.**  \n",
    "   Multiplicative shading before $g(\\cdot)$ preserves highlight structure and avoids post‑sensor renormalisation artefacts.\n",
    "\n",
    "6) **Post‑sensor star term.**  \n",
    "   $y_{\\star}$ is computed from $I_{\\star}-I_0$ **after** the sensor. The label therefore encourages the network to match what the image actually shows.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Parameter intuition (how each affects difficulty)\n",
    "\n",
    "- **STAR\\_DENSITY\\_ADD**: increases local brightness; too small → star is subtle; too large → risk of sensor saturation (watch the pixel‑mass diagnostic).\n",
    "- **STAR\\_COMPENSATE** ($0\\ldots 1$): increases energy neutrality; higher values reduce correlation with global mean.\n",
    "- **TAU\\_SENSOR**: controls concavity; larger $\\tau$ makes $g$ more linear (flatter highlights), smaller $\\tau$ increases pop (brighter star) but approaches saturation.\n",
    "- **Y\\_DENSITY\\_RANGE**: narrows/widens the global anchor; keep **tight** to force reliance on the star.\n",
    "- **SHADING\\_ALPHA\\_RANGE**: too high re‑introduces a global confound via illumination; keep gentle.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Compact formula summary\n",
    "\n",
    "- Sensor: $g(x)=1-e^{-x/\\tau}$.\n",
    "- Compensation: $\\text{comp}=\\alpha\\,\\frac{1}{|\\text{shell}|}\\sum_{(i,j)\\in\\text{shell}}\\text{add}(i,j)$.\n",
    "- Compensated densities:\n",
    "  $$\n",
    "  \\text{dens0c}=\\max(\\text{dens0}-\\text{comp}\\,\\mathbb{1}_{\\text{shell}},0),\\quad\n",
    "  \\text{dens\\_star}=\\max(\\text{dens0}+\\text{add}-\\text{comp}\\,\\mathbb{1}_{\\text{shell}},0).\n",
    "  $$\n",
    "- Brightness: $I_0=g(\\text{dens0c}\\cdot S)$, $I_{\\star}=g(\\text{dens\\_star}\\cdot S)$.\n",
    "- Star term: $y_{\\star}=\\frac{1}{|\\text{shell}|}\\sum_{\\text{shell}}\\big(I_{\\star}-I_0\\big)$.\n",
    "- Label: as boxed in §3.\n",
    "- Diagnostics: $\\text{Corr}(y,m)$, $\\text{Corr}(y,s)$, $\\Delta R^2$.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) What the plots should convey\n",
    "\n",
    "- **Sample grid:** the star is obvious and always on the shell; angle varies.\n",
    "- **Histogram:** broader spread than the simple/no‑star set.\n",
    "- **Mean image:** no star imprint, crisp ring.\n",
    "- **Printed diagnostics:** low pixel mass at 1, moderate Corr$(y,m)$, high Corr$(y,s)$, healthy $\\Delta R^2$.\n",
    "\n",
    "**Bottom line.** The dataset is deliberately hostile to global, pixel‑fixed linear models and favourable to **local, geometry‑aware** learners. Success looks like **poor Ridge** but **strong CNN**, especially once explanations (SHAP/IG) are examined in later cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Synthetic X‑ray dataset of a hollow metal sphere (100×100) — generation & visualisation\n",
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "Goal (finalised for \"Death‑Star\" contrast)\n",
    "------------------------------------------\n",
    "Make the **star** an unmistakable *local* driver of `y_true` that a CNN detects easily,\n",
    "while keeping a **small global linear anchor** (y_density) so OLS/Ridge behave sensibly\n",
    ". Inject the star in a way that **does not inflate global brightness**\n",
    "(energy‑neutral within the shell), so pixel‑wise linear models cannot exploit the mean,\n",
    "whereas a CNN can localise the star anywhere on the ring.\n",
    "\n",
    "Outputs:\n",
    "  images      : (N, 100, 100) float32 in [0,1)\n",
    "  y_true_all  : (N,) noise‑free target\n",
    "  y_obs_all   : (N,) observed target (y_true + small label noise)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility & settings\n",
    "# ----------------------------\n",
    "SEED = 2025\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "IMG_SIZE = 100\n",
    "N_IMAGES = 1000\n",
    "\n",
    "# Geometry (nominal)\n",
    "R0_BASE = 20.0  # inner (hollow) radius\n",
    "R1_BASE = 40.0  # outer radius\n",
    "CENTER = (IMG_SIZE - 1) / 2.0\n",
    "\n",
    "# Illumination & noise (applied in density domain → smooth post‑sensor highlights)\n",
    "SHADING_ALPHA_RANGE = (0.015, 0.035)  # gentle illumination; low global confound\n",
    "PIXEL_NOISE_STD = 0.008  # tiny, after sensor response\n",
    "BACKGROUND_OFFSET_MAX = 0.004\n",
    "\n",
    "# Sensor response: smooth, monotone, no hard clipping\n",
    "TAU_SENSOR = 1.7  # smaller → brighter; tuned so star “pops” but stays < 1.0\n",
    "\n",
    "# Target ranges / noise\n",
    "Y_DENSITY_RANGE = (0.98, 1.02)  # tight global anchor (keeps linear models \"ok\" but not great)\n",
    "Y_LABEL_NOISE_STD = 0.007\n",
    "Y_TRUE_CLIP = (0.80, 1.80)\n",
    "\n",
    "# Variability in shape (mild)\n",
    "RADIUS_JITTER_PX = 2.0\n",
    "CENTER_JITTER_PX = 1.0\n",
    "ELLIPTICITY_RANGE = (0.985, 1.015)\n",
    "PROFILE_GAMMA_RANGE = (0.95, 1.05)\n",
    "\n",
    "# Star parameters — **very strong local cue**\n",
    "STAR_ARMS = 5\n",
    "STAR_CORE_SIGMA_PX = (0.8, 1.2)  # tight core\n",
    "STAR_ARM_SIGMA_PX = (1.6, 2.2)  # thin arms\n",
    "STAR_SHARPNESS = 9.0  # angular sharpness\n",
    "STAR_RADIUS_JITTER = 1.0  # around mid‑shell\n",
    "STAR_AMP_RANGE = (1.0, 1.8)  # per‑image amplitude variability\n",
    "STAR_DENSITY_ADD = 2.2  # **additive in density** (pre‑sensor); strong local addition\n",
    "STAR_COMPENSATE = 1.00  # 0=none, 1=fully subtract star’s mean density on the shell\n",
    "\n",
    "# Nuisance weights (smaller than star so it stands out)\n",
    "EDGE_WT = 0.03\n",
    "CRACK_WT = 0.04\n",
    "VOID_WT = 0.035\n",
    "ANISO_WT = 0.03\n",
    "INTERACT_WT = 0.015\n",
    "STAR_Y_GAIN = 48.0  # label gain on *post‑sensor* star brightness (strong)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: metrics & plotting\n",
    "# ----------------------------\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "def print_metrics(split_name, y_true, y_pred):\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rms = rmse(y_true, y_pred)\n",
    "    print(f\"[{split_name}] R² = {r2:.4f} | MAE = {mae:.4f} | RMSE = {rms:.4f}\")\n",
    "    return r2, mae, rms\n",
    "\n",
    "\n",
    "def plot_pred_vs_actual(y_actual, y_pred, title=\"Predicted vs Actual\"):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.scatter(y_actual, y_pred, s=14, alpha=0.75)\n",
    "    lo = min(float(np.min(y_actual)), float(np.min(y_pred)))\n",
    "    hi = max(float(np.max(y_actual)), float(np.max(y_pred)))\n",
    "    pad = 0.02 * (hi - lo) if hi > lo else 0.01\n",
    "    plt.plot([lo - pad, hi + pad], [lo - pad, hi + pad], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"Actual y\");\n",
    "    plt.ylabel(\"Predicted y\");\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3);\n",
    "    plt.tight_layout();\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_heatmap(arr2d, title, cmap=None, with_colorbar=True):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(arr2d, cmap=cmap)\n",
    "    if with_colorbar:\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.title(title);\n",
    "    plt.axis(\"off\");\n",
    "    plt.tight_layout();\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Coordinates (constant fields)\n",
    "# ----------------------------\n",
    "y_coords, x_coords = np.indices((IMG_SIZE, IMG_SIZE))\n",
    "X0 = x_coords - CENTER\n",
    "Y0 = y_coords - CENTER\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Smooth illumination (mean=1), applied in *density* domain\n",
    "# ----------------------------\n",
    "def make_low_frequency_shading(alpha, rng):\n",
    "    XN = X0 / R1_BASE\n",
    "    YN = Y0 / R1_BASE\n",
    "    c = rng.normal(0.0, 1.0, size=6)\n",
    "    field = (c[0] + c[1] * XN + c[2] * YN + c[3] * XN * YN + c[4] * XN * XN + c[5] * YN * YN)\n",
    "    field = field - field.mean()\n",
    "    std = field.std() + 1e-12\n",
    "    field = field / std\n",
    "    return 1.0 + alpha * field  # multiplicative factor around 1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Geometry helpers\n",
    "# ----------------------------\n",
    "def radial_distance(x, y, cx, cy, sx=1.0, sy=1.0):\n",
    "    dx = (x - cx) / sx\n",
    "    dy = (y - cy) / sy\n",
    "    return np.sqrt(dx * dx + dy * dy)\n",
    "\n",
    "\n",
    "def angle_field(x, y, cx, cy):\n",
    "    return (np.arctan2(y - cy, x - cx) + 2 * np.pi) % (2 * np.pi)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Defects (moderate)\n",
    "# ----------------------------\n",
    "def make_anisotropy_map(re, theta, R0, R1, amp, k_freq, phase):\n",
    "    r_mid = 0.5 * (R0 + R1)\n",
    "    sigma_r = 0.30 * (R1 - R0)\n",
    "    radial_weight = np.exp(-0.5 * ((re - r_mid) / (sigma_r + 1e-8)) ** 2)\n",
    "    return amp * np.sin(k_freq * theta + phase) * radial_weight\n",
    "\n",
    "\n",
    "def add_radial_cracks(dx, dy, re, R0, R1, rng):\n",
    "    C = np.zeros_like(re, dtype=np.float64)\n",
    "    n_cr = int(np.clip(rng.poisson(lam=0.8), 0, 3))\n",
    "    for _ in range(n_cr):\n",
    "        theta0 = rng.uniform(0, 2 * np.pi)\n",
    "        depth = rng.uniform(0.12, 0.22)\n",
    "        sigma = rng.uniform(0.8, 1.4)\n",
    "        s, c = np.sin(theta0), np.cos(theta0)\n",
    "        dist_perp = np.abs(-s * dx + c * dy)\n",
    "        band = (re > (R0 + 0.8)) & (re < (R1 - 0.8))\n",
    "        C += depth * np.exp(-0.5 * (dist_perp / (sigma + 1e-8)) ** 2) * band\n",
    "    return np.clip(C, 0.0, 0.60)\n",
    "\n",
    "\n",
    "def add_voids(dx, dy, re, R0, R1, rng):\n",
    "    V = np.zeros_like(re, dtype=np.float64)\n",
    "    n_v = int(np.clip(rng.poisson(lam=0.6), 0, 3))\n",
    "    for _ in range(n_v):\n",
    "        theta_c = rng.uniform(0, 2 * np.pi)\n",
    "        r_c = rng.uniform(R0 + 3.0, R1 - 3.0)\n",
    "        cx = r_c * np.cos(theta_c);\n",
    "        cy = r_c * np.sin(theta_c)\n",
    "        depth = rng.uniform(0.18, 0.28)\n",
    "        rad = rng.uniform(1.4, 3.0)\n",
    "        dist2 = (dx - cx) ** 2 + (dy - cy) ** 2\n",
    "        V += depth * np.exp(-0.5 * dist2 / ((rad + 1e-8) ** 2))\n",
    "    return np.clip(V, 0.0, 0.65)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Star generator (5‑arm) on the shell\n",
    "# ----------------------------\n",
    "def make_star_on_shell(dx0, dy0, re, R0, R1, rng):\n",
    "    r_mid = 0.5 * (R0 + R1)\n",
    "    theta_star = rng.uniform(0, 2 * np.pi)\n",
    "    r_star = r_mid + rng.uniform(-STAR_RADIUS_JITTER, +STAR_RADIUS_JITTER)\n",
    "\n",
    "    sx = r_star * np.cos(theta_star)\n",
    "    sy = r_star * np.sin(theta_star)\n",
    "\n",
    "    U = dx0 - sx\n",
    "    V = dy0 - sy\n",
    "    rho = np.sqrt(U * U + V * V)\n",
    "    phi = (np.arctan2(V, U) + 2 * np.pi) % (2 * np.pi)\n",
    "\n",
    "    core_sig = rng.uniform(*STAR_CORE_SIGMA_PX)\n",
    "    arm_sig = rng.uniform(*STAR_ARM_SIGMA_PX)\n",
    "    amp = rng.uniform(*STAR_AMP_RANGE)\n",
    "\n",
    "    core = np.exp(-0.5 * (rho / (core_sig + 1e-8)) ** 2)\n",
    "    arms = np.exp(-0.5 * (rho / (arm_sig + 1e-8)) ** 2) * np.clip(0.5 * (1.0 + np.cos(STAR_ARMS * phi)), 0.0,\n",
    "                                                                  1.0) ** STAR_SHARPNESS\n",
    "    star = np.clip(core + 0.9 * arms, 0.0, 1.0)\n",
    "\n",
    "    band = (re > (R0 + 1.0)) & (re < (R1 - 1.0))\n",
    "    return amp * star * band  # amplitude‑scaled star map in [0, amp]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Sensor response — smooth, no hard clipping\n",
    "# ----------------------------\n",
    "def sensor_response(density_field, tau=TAU_SENSOR):\n",
    "    # g(x) = 1 - exp(-x/tau)  ∈ [0,1) strictly; monotone; concave.\n",
    "    return 1.0 - np.exp(-np.maximum(density_field, 0.0) / (tau + 1e-12))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Generate one image + targets\n",
    "# ----------------------------\n",
    "def generate_one_image(rng):\n",
    "    # --- Geometry & base fields\n",
    "    R0 = R0_BASE + rng.uniform(-RADIUS_JITTER_PX, +RADIUS_JITTER_PX)\n",
    "    R1 = R1_BASE + rng.uniform(-RADIUS_JITTER_PX, +RADIUS_JITTER_PX)\n",
    "    if R1 <= R0 + 8.0:\n",
    "        R1 = R0 + 8.0\n",
    "\n",
    "    cx = CENTER + rng.uniform(-CENTER_JITTER_PX, +CENTER_JITTER_PX)\n",
    "    cy = CENTER + rng.uniform(-CENTER_JITTER_PX, +CENTER_JITTER_PX)\n",
    "    sx = rng.uniform(*ELLIPTICITY_RANGE)\n",
    "    sy = rng.uniform(*ELLIPTICITY_RANGE)\n",
    "    gamma = rng.uniform(*PROFILE_GAMMA_RANGE)\n",
    "\n",
    "    re = radial_distance(x_coords, y_coords, cx, cy, sx=sx, sy=sy)\n",
    "\n",
    "    base = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.float64)\n",
    "    shell = (re > R0) & (re <= R1)\n",
    "    t = np.empty_like(re)\n",
    "    t[shell] = (re[shell] - R0) / (R1 - R0)\n",
    "    base[shell] = (1.0 - t[shell]) ** gamma\n",
    "\n",
    "    dx0 = (x_coords - cx)\n",
    "    dy0 = (y_coords - cy)\n",
    "    theta = angle_field(x_coords, y_coords, cx, cy)\n",
    "\n",
    "    # --- Defects (moderate) in the base density\n",
    "    A_amp = rng.uniform(-0.05, 0.05)\n",
    "    A_kfreq = int(rng.integers(1, 4))\n",
    "    A_phase = rng.uniform(0.0, 2 * np.pi)\n",
    "    aniso = make_anisotropy_map(re, theta, R0, R1, A_amp, A_kfreq, A_phase)\n",
    "\n",
    "    C_map = add_radial_cracks(dx0, dy0, re, R0, R1, rng)\n",
    "    V_map = add_voids(dx0, dy0, re, R0, R1, rng)\n",
    "\n",
    "    # --- Linear anchor (global density)\n",
    "    y_density = float(rng.uniform(*Y_DENSITY_RANGE))\n",
    "\n",
    "    # Density **before** star (structural)\n",
    "    dens0 = y_density * base * (1.0 + aniso)\n",
    "    dens0 = dens0 * (1.0 - C_map) * (1.0 - V_map)\n",
    "    dens0 = np.maximum(dens0, 0.0)\n",
    "\n",
    "    # --- Star (always present), **additive in density domain**\n",
    "    star_map = make_star_on_shell(dx0, dy0, re, R0, R1, rng)  # amplitude‑scaled shape\n",
    "    add = STAR_DENSITY_ADD * star_map  # strong local addition\n",
    "\n",
    "    # Energy‑neutral adjustment inside the shell: subtract mean(add) on the shell\n",
    "    shell_mask = shell.astype(np.float64)\n",
    "    shell_area = float(shell_mask.sum())\n",
    "    mu_add_shell = float((add * shell_mask).sum() / (shell_area + 1e-12))\n",
    "    comp = STAR_COMPENSATE * mu_add_shell\n",
    "\n",
    "    dens0c = np.maximum(dens0 - comp * shell_mask, 0.0)\n",
    "    dens_star = np.maximum(dens0 + add - comp * shell_mask, 0.0)\n",
    "\n",
    "    # --- Illumination shading in **density domain**\n",
    "    alpha = rng.uniform(*SHADING_ALPHA_RANGE)\n",
    "    shading = make_low_frequency_shading(alpha, rng)  # mean ~1\n",
    "    dens0_s = dens0c * shading\n",
    "    dens_star_s = dens_star * shading\n",
    "\n",
    "    # --- Sensor response (smooth; no hard clipping)\n",
    "    img_base = sensor_response(dens0_s, tau=TAU_SENSOR)  # “without star” brightness\n",
    "    img_star = sensor_response(dens_star_s, tau=TAU_SENSOR)  # “with star” brightness\n",
    "\n",
    "    # Final observed image (with star), tiny offset + noise\n",
    "    offset = rng.uniform(0.0, BACKGROUND_OFFSET_MAX)\n",
    "    img = img_star + offset + rng.normal(0.0, PIXEL_NOISE_STD, size=img_star.shape)\n",
    "    img = np.clip(img, 0.0, 1.0 - 1e-6).astype(np.float32)  # virtually no mass at 1.0\n",
    "\n",
    "    # --- Label from what the sensor “sees”\n",
    "    # Star strength (positive): **post‑sensor** excess brightness within the shell\n",
    "    star_excess = img_star - img_base\n",
    "    star_brightness = float(star_excess[shell.astype(bool)].sum() / (shell_area + 1e-8))  # area‑normalised\n",
    "\n",
    "    # Edge strength / defects (computed from *pre‑sensor* structural field; small weights)\n",
    "    gx, gy = np.gradient(dens0)  # density edges reflect inner/outer shell sharpness\n",
    "    grad_mag = np.sqrt(gx * gx + gy * gy)\n",
    "    edge_band = (np.abs(re - R0) <= 1.5) | (np.abs(re - R1) <= 1.5)\n",
    "    edge_strength_raw = float(grad_mag[edge_band].mean() if np.any(edge_band) else 0.0)\n",
    "    EDGE_SCALE = 0.06\n",
    "    edge_term = np.clip(edge_strength_raw / (EDGE_SCALE + 1e-8), 0.0, 1.5)\n",
    "\n",
    "    crack_raw = float((C_map * base)[shell.astype(bool)].sum() / (shell_area + 1e-8))\n",
    "    void_raw = float((V_map * base)[shell.astype(bool)].sum() / (shell_area + 1e-8))\n",
    "    CRACK_SCALE = 24.0\n",
    "    VOID_SCALE = 20.0\n",
    "    crack_term = np.clip(CRACK_SCALE * crack_raw, 0.0, 1.5)\n",
    "    void_term = np.clip(VOID_SCALE * void_raw, 0.0, 1.5)\n",
    "    aniso_term = np.clip(abs(A_amp) / 0.10, 0.0, 1.5)\n",
    "    interact_term = crack_term * aniso_term\n",
    "\n",
    "    # Compose y_true\n",
    "    y_true = (1.00 * y_density\n",
    "              + EDGE_WT * edge_term\n",
    "              - CRACK_WT * crack_term\n",
    "              - VOID_WT * void_term\n",
    "              - ANISO_WT * aniso_term\n",
    "              - INTERACT_WT * interact_term\n",
    "              + STAR_Y_GAIN * star_brightness)\n",
    "\n",
    "    y_true = float(np.clip(y_true, *Y_TRUE_CLIP))\n",
    "    y_obs = y_true + float(rng.normal(0.0, Y_LABEL_NOISE_STD))\n",
    "    return img, y_true, y_obs, star_brightness\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Generate & summarise\n",
    "# ----------------------------\n",
    "def generate_dataset(n_images, rng):\n",
    "    images = np.empty((n_images, IMG_SIZE, IMG_SIZE), dtype=np.float32)\n",
    "    y_true = np.empty(n_images, dtype=np.float32)\n",
    "    y_obs = np.empty(n_images, dtype=np.float32)\n",
    "    star_b = np.empty(n_images, dtype=np.float32)  # diagnostic: post‑sensor star brightness (our proxy)\n",
    "    for i in range(n_images):\n",
    "        img, yt, yo, sb = generate_one_image(rng)\n",
    "        images[i] = img;\n",
    "        y_true[i] = yt;\n",
    "        y_obs[i] = yo;\n",
    "        star_b[i] = sb\n",
    "    return images, y_true, y_obs, star_b\n",
    "\n",
    "\n",
    "images, y_true_all, y_obs_all, _star_brightness_all = generate_dataset(N_IMAGES, rng)\n",
    "\n",
    "print(\"Dataset summary:\")\n",
    "print(f\"  images.shape = {images.shape} (float32, values in [0,1))\")\n",
    "print(f\"  y_true: mean={y_true_all.mean():.4f}, std={y_true_all.std():.4f}, \"\n",
    "      f\"min={y_true_all.min():.4f}, max={y_true_all.max():.4f}\")\n",
    "print(f\"  y_obs :  mean={y_obs_all.mean():.4f},  std={y_obs_all.std():.4f},  \"\n",
    "      f\"min={y_obs_all.min():.4f},  max={y_obs_all.max():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Diagnostics (fast, no downstream dependencies)\n",
    "# ----------------------------\n",
    "# 1) Pixel mass near 1.0 (smooth sensor → should be ~0)\n",
    "clip_rate = float(np.mean(images >= (1.0 - 1e-6))) * 100.0\n",
    "\n",
    "# 2) Correlations with global mean and star proxy\n",
    "per_img_mean = images.reshape(len(images), -1).mean(axis=1)\n",
    "\n",
    "\n",
    "def _corr(a, b):\n",
    "    a = a - a.mean();\n",
    "    b = b - b.mean()\n",
    "    denom = (np.sqrt((a * a).sum()) * np.sqrt((b * b).sum()) + 1e-12)\n",
    "    return float((a * b).sum() / denom)\n",
    "\n",
    "\n",
    "corr_mean = _corr(y_obs_all, per_img_mean)  # global anchor\n",
    "corr_star = _corr(y_obs_all, _star_brightness_all)  # star drives label\n",
    "\n",
    "# 3) Incremental R² when adding star proxy to mean‑only linear model\n",
    "X1 = np.c_[np.ones_like(per_img_mean), per_img_mean]\n",
    "X2 = np.c_[np.ones_like(per_img_mean), per_img_mean, _star_brightness_all]\n",
    "b1 = np.linalg.lstsq(X1, y_obs_all, rcond=None)[0]\n",
    "b2 = np.linalg.lstsq(X2, y_obs_all, rcond=None)[0]\n",
    "ss = lambda y, yhat: float(((y - yhat) ** 2).sum())\n",
    "ss_tot = float(((y_obs_all - y_obs_all.mean()) ** 2).sum() + 1e-12)\n",
    "r2_1 = 1.0 - ss(y_obs_all, X1 @ b1) / ss_tot\n",
    "r2_2 = 1.0 - ss(y_obs_all, X2 @ b2) / ss_tot\n",
    "delta_r2_star = r2_2 - r2_1\n",
    "\n",
    "print(f\"[Diag] Pixel mass near 1.0 = {clip_rate:.2f}%  (smooth sensor → expect ~0.00%)\")\n",
    "print(f\"[Diag] Corr(y_obs, per‑image mean brightness) = {corr_mean:.3f}  (global anchor; aim 0.25–0.45)\")\n",
    "print(f\"[Diag] Corr(y_obs, star‑brightness proxy)     = {corr_star:.3f}  (star should dominate; aim ≥ 0.65)\")\n",
    "print(f\"[Diag] ΔR² from adding star proxy to mean‑only model = {delta_r2_star:.3f}  (aim ≥ 0.30)\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Visual checks\n",
    "# ----------------------------\n",
    "def show_image_grid(images, y_true, y_obs, n_show=12, title=\"Sample simulated images\"):\n",
    "    n_show = int(min(n_show, images.shape[0]))\n",
    "    idxs = rng.choice(images.shape[0], size=n_show, replace=False)\n",
    "    cols = int(np.ceil(np.sqrt(n_show)))\n",
    "    rows = int(np.ceil(n_show / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(1.9 * cols, 1.9 * rows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "    for ax, i in zip(axes, idxs):\n",
    "        ax.imshow(images[i], cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "        ax.set_title(f\"y_true={y_true[i]:.3f}\\ny_obs={y_obs[i]:.3f}\", fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "    for k in range(n_show, len(axes)):\n",
    "        axes[k].axis(\"off\")\n",
    "    plt.suptitle(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_image_grid(images, y_true_all, y_obs_all, n_show=12,\n",
    "                title=\"Synthetic hollow‑sphere X‑ray images (energy‑neutral star; smooth sensor)\")\n",
    "\n",
    "# Label histogram\n",
    "plt.figure(figsize=(5.5, 4))\n",
    "plt.hist(y_obs_all, bins=22, edgecolor=\"black\", alpha=0.85)\n",
    "plt.xlabel(\"Observed y\");\n",
    "plt.ylabel(\"Count\");\n",
    "plt.title(\"Label distribution (y_obs)\")\n",
    "plt.tight_layout();\n",
    "plt.show()\n",
    "\n",
    "# Mean image (across the full dataset)\n",
    "mean_img = images.mean(axis=0)\n",
    "show_heatmap(mean_img, title=\"Mean image (all samples)\", cmap=\"gray\", with_colorbar=True)\n",
    "\n",
    "print(\"\\nAll arrays are now in memory: images, y_true_all, y_obs_all.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copy of cell 2...\n",
    "\n",
    "Linear & Ridge regression on the in‑memory dataset\n",
    "--------------------------------------------------\n",
    "\n",
    "What this cell does:\n",
    "\n",
    "1) Builds Train / Validation / Test splits: 64% / 16% / 20%.\n",
    "2) Fits an Ordinary Least Squares (OLS) linear regression on flattened pixels → y_obs.\n",
    "   • Notes why OLS can achieve R²≈1 on the training set when p ≫ n (10,000 features vs ~640 samples).\n",
    "3) Trains a tuned Ridge regression:\n",
    "   • Pipeline(StandardScaler, Ridge)\n",
    "   • Hyper‑parameter alpha selected by 5‑fold CV on the *training* split only.\n",
    "   • Report Train & Val performance for the CV‑selected model.\n",
    "   • Refit the final Ridge on Train+Val with the chosen alpha; report Test performance.\n",
    "4) Prints metrics (R², MAE, RMSE) and draws Predicted vs Actual plots.\n",
    "5) Shows coefficient heatmaps for OLS and final Ridge (mapped back to raw‑pixel space).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sanity check: make sure Cell 1 was run\n",
    "if \"images\" not in globals() or \"y_obs_all\" not in globals():\n",
    "    raise RuntimeError(\"Cell 1 has not been run. Please run Cell 1 to create images/y_obs_all in memory.\")\n",
    "\n",
    "SEED = 2025\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC_WITHIN_TRAINVAL = 0.20  # 20% of the 80% → 16% overall\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "N, H, W = images.shape\n",
    "D = H * W\n",
    "\n",
    "# Flatten images to (N, D)\n",
    "X_all = images.reshape(N, -1).astype(np.float64)\n",
    "y_all = y_obs_all.astype(np.float64)\n",
    "y_true_all_local = y_true_all.astype(np.float64)  # for noise ceiling\n",
    "\n",
    "# Split Train+Val vs Test\n",
    "X_trainval, X_test, y_trainval, y_test, y_true_trainval, y_true_test = train_test_split(\n",
    "    X_all, y_all, y_true_all_local, test_size=TEST_FRAC, random_state=SEED\n",
    ")\n",
    "# Split Train vs Val\n",
    "X_train, X_val, y_train, y_val, y_true_train, y_true_val = train_test_split(\n",
    "    X_trainval, y_trainval, y_true_trainval, test_size=VAL_FRAC_WITHIN_TRAINVAL, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Split summary:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"  Val  : {X_val.shape[0]} samples\")\n",
    "print(f\"  Test : {X_test.shape[0]} samples\")\n",
    "print(f\"  Note: p (features) = {D:,},  n_train = {X_train.shape[0]:,}.  Here, p ≫ n.\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) OLS Linear Regression (may interpolate when p ≫ n)\n",
    "# ------------------------------------------------------------\n",
    "ols = LinearRegression()  # scikit‑learn uses an SVD‑based least‑squares solver\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "yhat_tr_ols = ols.predict(X_train)\n",
    "yhat_va_ols = ols.predict(X_val)\n",
    "yhat_te_ols = ols.predict(X_test)\n",
    "\n",
    "print(\"OLS performance (against observed y):\")\n",
    "print_metrics(\"Train (OLS)\", y_train, yhat_tr_ols)\n",
    "print_metrics(\"Val   (OLS)\", y_val,   yhat_va_ols)\n",
    "print_metrics(\"Test  (OLS)\", y_test,  yhat_te_ols)\n",
    "\n",
    "# Why can Train R² be ~1?\n",
    "# With p (=10,000) >> n_train (~640), an OLS model has enough degrees of freedom to fit the training\n",
    "# labels almost perfectly (interpolate), especially as y is largely a global scaling of the image.\n",
    "# The SVD solution picks one of infinitely many solutions (minimum‑norm) that achieve near‑zero\n",
    "# training error if the design matrix has rank n_train. This is normal in over‑parameterised linear models.\n",
    "\n",
    "# Noise ceiling on Test (best achievable R² against *noisy* labels)\n",
    "r2_ceiling_test = 1.0 - np.sum((y_true_test - y_test) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "print(f\"\\n[Reference] Test‑set R² noise ceiling (label noise): {r2_ceiling_test:.4f}\\n\")\n",
    "\n",
    "# Visual diagnostics — Predicted vs Actual\n",
    "plot_pred_vs_actual(y_train, yhat_tr_ols, \"Predicted vs Actual (Train, OLS)\")\n",
    "plot_pred_vs_actual(y_val,   yhat_va_ols, \"Predicted vs Actual (Validation, OLS)\")\n",
    "plot_pred_vs_actual(y_test,  yhat_te_ols, \"Predicted vs Actual (Test, OLS)\")\n",
    "\n",
    "# Coefficient heatmap (OLS)\n",
    "coef_ols = ols.coef_.reshape(H, W)\n",
    "show_heatmap(coef_ols, title=\"OLS: learned pixel weights\", cmap=None, with_colorbar=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Ridge Regression (tuned) — Pipeline(StandardScaler, Ridge)\n",
    "# ------------------------------------------------------------\n",
    "# Standardise features because Ridge penalises the magnitude of coefficients and should\n",
    "# not be affected by arbitrary feature scales.\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"ridge\",  Ridge(random_state=SEED))\n",
    "])\n",
    "\n",
    "# Hyper‑parameter grid for alpha (L2 strength)\n",
    "alphas = np.logspace(-6, 3, 25)  # 1e-6 … 1e3\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "param_grid = {\"ridge__alpha\": alphas}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    refit=True  # refit the best on the *training* split\n",
    ")\n",
    "\n",
    "print(\"Tuning Ridge (5‑fold CV on the training split)…\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = grid.best_params_[\"ridge__alpha\"]\n",
    "print(f\"Best alpha (CV on Train): {best_alpha:.6g}\")\n",
    "\n",
    "# Evaluate the CV‑selected model on Train & Val\n",
    "ridge_cv_model = grid.best_estimator_\n",
    "yhat_tr_ridge = ridge_cv_model.predict(X_train)\n",
    "yhat_va_ridge = ridge_cv_model.predict(X_val)\n",
    "\n",
    "print(\"\\nRidge (CV‑selected) performance:\")\n",
    "print_metrics(\"Train (Ridge CV)\", y_train, yhat_tr_ridge)\n",
    "print_metrics(\"Val   (Ridge CV)\", y_val,   yhat_va_ridge)\n",
    "\n",
    "# OPTIONAL: refit on Train+Val using the chosen alpha, then evaluate on Test\n",
    "final_ridge = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"ridge\",  Ridge(alpha=best_alpha, random_state=SEED))\n",
    "])\n",
    "final_ridge.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\n",
    "\n",
    "yhat_te_ridge = final_ridge.predict(X_test)\n",
    "print_metrics(\"\\nTest  (Ridge final)\", y_test, yhat_te_ridge)\n",
    "\n",
    "# Visual diagnostics — Predicted vs Actual for Ridge (final)\n",
    "plot_pred_vs_actual(y_train, yhat_tr_ridge, \"Predicted vs Actual (Train, Ridge CV model)\")\n",
    "plot_pred_vs_actual(y_val,   yhat_va_ridge, \"Predicted vs Actual (Validation, Ridge CV model)\")\n",
    "plot_pred_vs_actual(y_test,  yhat_te_ridge, \"Predicted vs Actual (Test, Ridge final model)\")\n",
    "\n",
    "# Coefficient heatmap (Ridge, mapped back to raw pixel space)\n",
    "#  ŷ = intercept + Σ w_scaled_i * (x_i - mean_i)/std_i\n",
    "#  ⇒ effective raw‑space weight: w_raw_i = w_scaled_i / std_i\n",
    "#     effective raw intercept:  b_raw = intercept - Σ w_scaled_i * mean_i / std_i\n",
    "scaler = final_ridge.named_steps[\"scaler\"]\n",
    "ridge  = final_ridge.named_steps[\"ridge\"]\n",
    "w_scaled = ridge.coef_.ravel()\n",
    "w_raw = w_scaled / (scaler.scale_ + 1e-12)\n",
    "coef_ridge_raw = w_raw.reshape(H, W)\n",
    "show_heatmap(coef_ridge_raw, title=\"Ridge (final): pixel weights (raw‑space)\", cmap=None, with_colorbar=True)\n",
    "\n",
    "# Brief takeaways\n",
    "print(\"\\nTakeaways:\")\n",
    "print(\"  • With 10,000 features and ~640 training samples (p ≫ n), OLS can nearly interpolate the training labels,\")\n",
    "print(\"    often giving R²≈1 on Train. That’s expected and is a symptom of high capacity, not necessarily true signal.\")\n",
    "print(\"  • Ridge regularisation stabilises the solution, improves conditioning, and typically gives better generalisation.\")\n",
    "print(\"  • The learned weight maps concentrate on the ring, which is exactly where the signal lies.\")\n",
    "print(\"  • ‘Noise ceiling’ gives the best possible R² on Test when labels contain measurement noise;\")\n",
    "print(\"    if your model approaches it, you are close to optimal under the present noise level.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — CNN that learns global density + shell edges + a tiny star\n",
    "# (SHAP‑compatible, M1‑safe, robust training)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----- Preconditions\n",
    "if \"images\" not in globals() or \"y_true_all\" not in globals() or \"y_obs_all\" not in globals():\n",
    "    raise RuntimeError(\"Please run Cell 1 (data generation) first.\")\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# ----- Device (M1 → MPS if available)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ----- Repro\n",
    "SEED = 2025\n",
    "np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if DEVICE == \"cuda\": torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ----- Split (same recipe as Cell 2)\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC_WITHIN_TRAINVAL = 0.20\n",
    "\n",
    "N, H, W = images.shape\n",
    "CENTER = (H - 1) / 2.0\n",
    "\n",
    "X_all = images\n",
    "y_all = y_obs_all.astype(np.float32)\n",
    "y_true_all_local = y_true_all.astype(np.float32)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test, y_true_trainval, y_true_test = train_test_split(\n",
    "    X_all, y_all, y_true_all_local, test_size=TEST_FRAC, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val, y_true_train, y_true_val = train_test_split(\n",
    "    X_trainval, y_trainval, y_true_trainval, test_size=VAL_FRAC_WITHIN_TRAINVAL, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Split summary:\")\n",
    "print(f\"  Train: {X_train.shape[0]}  | Val: {X_val.shape[0]}  | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# ----- Preprocessing (corner‑median background → train‑stats normalisation)\n",
    "ys, xs = np.indices((H, W))\n",
    "\n",
    "def estimate_background_offset(img: np.ndarray, k: int = 10) -> float:\n",
    "    patches = [img[:k, :k], img[:k, -k:], img[-k:, :k], img[-k:, -k:]]\n",
    "    return float(np.median(np.concatenate([p.ravel() for p in patches])))\n",
    "\n",
    "def subtract_background(X: np.ndarray) -> np.ndarray:\n",
    "    Xp = np.empty_like(X)\n",
    "    for i, img in enumerate(X):\n",
    "        bg = estimate_background_offset(img, k=10)\n",
    "        im = img - bg\n",
    "        im[im < 0.0] = 0.0\n",
    "        Xp[i] = im\n",
    "    return Xp\n",
    "\n",
    "X_train_s = subtract_background(X_train)\n",
    "X_val_s   = subtract_background(X_val)\n",
    "X_test_s  = subtract_background(X_test)\n",
    "\n",
    "pix_mean = float(X_train_s.mean())\n",
    "pix_std  = float(X_train_s.std() + 1e-8)\n",
    "\n",
    "def norm_clamp(Xs):\n",
    "    Xn = (Xs - pix_mean) / pix_std\n",
    "    Xn = np.clip(Xn, -8.0, 8.0).astype(np.float32)\n",
    "    return Xn\n",
    "\n",
    "X_train_n = norm_clamp(X_train_s)\n",
    "X_val_n   = norm_clamp(X_val_s)\n",
    "X_test_n  = norm_clamp(X_test_s)\n",
    "\n",
    "# CoordConv channels (exported for SHAP)\n",
    "x_lin = (xs - CENTER) / CENTER\n",
    "y_lin = (ys - CENTER) / CENTER\n",
    "r_map = np.sqrt(x_lin**2 + y_lin**2)\n",
    "r_map = r_map / (r_map.max() + 1e-12)\n",
    "\n",
    "coord_tensor = torch.from_numpy(np.stack([x_lin, y_lin, r_map], axis=0).astype(np.float32))\n",
    "coord_hflip  = torch.flip(coord_tensor, dims=[2])\n",
    "coord_vflip  = torch.flip(coord_tensor, dims=[1])\n",
    "coord_bflip  = torch.flip(coord_tensor, dims=[1, 2])\n",
    "\n",
    "# ----- Target scaling (z‑score on TRAIN)\n",
    "y_mean = float(np.mean(y_train))\n",
    "y_std  = float(np.std(y_train) + 1e-8)\n",
    "def to_z(y):   return ((y - y_mean) / y_std).astype(np.float32)\n",
    "def from_z(z): return z * y_std + y_mean\n",
    "\n",
    "z_train = to_z(y_train); z_val = to_z(y_val); z_test = to_z(y_test)\n",
    "\n",
    "# ----- Augmentation (flips; always contiguous → no negative‑stride errors)\n",
    "def random_flip_image_and_coords(im: np.ndarray):\n",
    "    hflip = (np.random.rand() < 0.5)\n",
    "    vflip = (np.random.rand() < 0.5)\n",
    "    if hflip: im = im[:, ::-1]\n",
    "    if vflip: im = im[::-1, :]\n",
    "    im = np.ascontiguousarray(im, dtype=np.float32)\n",
    "    if   hflip and  vflip: coord_use = coord_bflip\n",
    "    elif hflip and not vflip: coord_use = coord_hflip\n",
    "    elif not hflip and vflip: coord_use = coord_vflip\n",
    "    else: coord_use = coord_tensor\n",
    "    return im, coord_use\n",
    "\n",
    "class ImageRegDataset(Dataset):\n",
    "    def __init__(self, Xn, z, y, train: bool):\n",
    "        self.Xn = Xn; self.z = z; self.y = y; self.train = train\n",
    "    def __len__(self): return self.Xn.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        im = self.Xn[i]\n",
    "        if self.train:\n",
    "            im, coord_use = random_flip_image_and_coords(im)\n",
    "        else:\n",
    "            im = np.ascontiguousarray(im, dtype=np.float32)\n",
    "            coord_use = coord_tensor\n",
    "        x_img = torch.from_numpy(im).unsqueeze(0)              # (1,H,W)\n",
    "        x     = torch.cat([x_img, coord_use], dim=0)           # (4,H,W)\n",
    "        z     = torch.tensor(self.z[i:i+1], dtype=torch.float32)\n",
    "        y     = torch.tensor(self.y[i:i+1], dtype=torch.float32)\n",
    "        return x, z, y\n",
    "\n",
    "BATCH_SIZE = 128 if DEVICE in (\"cuda\", \"mps\") else 64\n",
    "PIN = (DEVICE == \"cuda\")\n",
    "train_loader = DataLoader(ImageRegDataset(X_train_n, z_train, y_train, train=True),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=PIN, num_workers=0)\n",
    "val_loader   = DataLoader(ImageRegDataset(X_val_n,   z_val,   y_val,   train=False),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=PIN, num_workers=0)\n",
    "test_loader  = DataLoader(ImageRegDataset(X_test_n,  z_test,  y_test,  train=False),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=PIN, num_workers=0)\n",
    "\n",
    "# ----- Fixed helper maps (inside the net; external input stays 4‑ch)\n",
    "class FixedImageStem(nn.Module):\n",
    "    \"\"\"Sobel grad‑mag + local contrast from image channel.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        sobel_x = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32).view(1,1,3,3)\n",
    "        sobel_y = sobel_x.transpose(2,3).contiguous()\n",
    "        mean3   = torch.full((1,1,3,3), 1.0/9.0, dtype=torch.float32)\n",
    "        self.conv_sx = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        self.conv_sy = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        self.blur3   = nn.Conv2d(1,1,3,padding=1,bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.conv_sx.weight.copy_(sobel_x)\n",
    "            self.conv_sy.weight.copy_(sobel_y)\n",
    "            self.blur3.weight.copy_(mean3)\n",
    "        for p in self.parameters(): p.requires_grad = False\n",
    "    def forward(self, x4):\n",
    "        img = x4[:, :1]\n",
    "        gx = self.conv_sx(img); gy = self.conv_sy(img)\n",
    "        gradmag   = torch.sqrt(torch.clamp(gx*gx + gy*gy, min=1e-12))\n",
    "        localmean = self.blur3(img)\n",
    "        highpass  = img - localmean\n",
    "        return torch.cat([x4, gradmag, highpass], dim=1)  # 4 → 6 ch\n",
    "\n",
    "# ----- Ring priors (from r channel) + starness branch\n",
    "class RingPriorsAndStar(nn.Module):\n",
    "    \"\"\"\n",
    "    Inside‑net priors:\n",
    "      • ring_mid, edge_inner, edge_outer from r channel (Gaussian bands)\n",
    "      • starness: convs on (img × ring_mid) to produce a focused 'star map'\n",
    "    Returns: concatenation of [x6, priors(3), star_map(1), global_mean_map(1)] → 6 + 3 + 1 + 1 = 11 channels\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # starness sub-net (lightweight, high gain)\n",
    "        self.s_c1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.s_a1 = nn.ReLU(inplace=False)\n",
    "        self.s_c2 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "        nn.init.kaiming_normal_(self.s_c1.weight, nonlinearity=\"relu\")\n",
    "        nn.init.kaiming_normal_(self.s_c2.weight, nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.s_c1.bias); nn.init.zeros_(self.s_c2.bias)\n",
    "\n",
    "    def forward(self, x6):\n",
    "        # x6 = [img, x, y, r, gradmag, highpass]\n",
    "        img = x6[:, :1]\n",
    "        r   = x6[:, 3:4]\n",
    "        # ring priors (values tuned to your geometry: inner~0.283, mid~0.435, outer~0.566)\n",
    "        ring_mid   = torch.exp(-0.5*((r - 0.435)/0.060)**2)\n",
    "        edge_inner = torch.exp(-0.5*((r - 0.283)/0.040)**2)\n",
    "        edge_outer = torch.exp(-0.5*((r - 0.566)/0.040)**2)\n",
    "        # starness on img × ring_mid\n",
    "        z = img * ring_mid\n",
    "        z = self.s_a1(self.s_c1(z))\n",
    "        star_map = torch.relu(self.s_c2(z)) * ring_mid  # keep it on the ring\n",
    "        # global context map (mean intensity of img)\n",
    "        gmean = img.mean(dim=(2,3), keepdim=True).expand_as(img)\n",
    "        return torch.cat([x6, ring_mid, edge_inner, edge_outer, star_map, gmean], dim=1)  # 11 ch\n",
    "\n",
    "# ----- A small, norm‑free residual block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.pad1 = nn.ReflectionPad2d(1); self.c1 = nn.Conv2d(c, c, 3)\n",
    "        self.act  = nn.ReLU(inplace=False)\n",
    "        self.pad2 = nn.ReflectionPad2d(1); self.c2 = nn.Conv2d(c, c, 3)\n",
    "        nn.init.kaiming_normal_(self.c1.weight, nonlinearity=\"relu\")\n",
    "        nn.init.kaiming_normal_(self.c2.weight, nonlinearity=\"relu\")\n",
    "        if self.c1.bias is not None: nn.init.zeros_(self.c1.bias)\n",
    "        if self.c2.bias is not None: nn.init.zeros_(self.c2.bias)\n",
    "    def forward(self, x):\n",
    "        y = self.c1(self.pad1(x)); y = self.act(y); y = self.c2(self.pad2(y))\n",
    "        return self.act(y + x)\n",
    "\n",
    "class ConcatPool2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat([torch.mean(x, dim=(2,3), keepdim=True),\n",
    "                          torch.amax(x, dim=(2,3), keepdim=True)], dim=1)\n",
    "\n",
    "# ----- Features extractor\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem      = FixedImageStem()          # 4 → 6\n",
    "        self.ring_star = RingPriorsAndStar()       # 6 → 11\n",
    "        self.pad0 = nn.ReflectionPad2d(1); self.c0 = nn.Conv2d(11, 64, 3); self.a0 = nn.ReLU(inplace=False)\n",
    "        self.b1 = ResBlock(64); self.pool1 = nn.MaxPool2d(2)     # 100 → 50\n",
    "        self.pad1 = nn.ReflectionPad2d(1); self.c1 = nn.Conv2d(64, 128, 3); self.a1 = nn.ReLU(inplace=False)\n",
    "        self.b2 = ResBlock(128); self.pool2 = nn.MaxPool2d(2)    # 50 → 25\n",
    "        self.pad2 = nn.ReflectionPad2d(1); self.c2 = nn.Conv2d(128, 160, 3); self.a2 = nn.ReLU(inplace=False)\n",
    "        self.b3 = ResBlock(160)\n",
    "        for m in [self.c0, self.c1, self.c2]:\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, x4):\n",
    "        x6  = self.stem(x4)\n",
    "        x11 = self.ring_star(x6)\n",
    "        y = self.a0(self.c0(self.pad0(x11))); y = self.b1(y); y = self.pool1(y)\n",
    "        y = self.a1(self.c1(self.pad1(y)));  y = self.b2(y); y = self.pool2(y)\n",
    "        y = self.a2(self.c2(self.pad2(y)));  y = self.b3(y)\n",
    "        return y\n",
    "\n",
    "# ----- Regressor (SHAP‑compatible: .features / .gap / .head ; TWO Linear)\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_ch=4):\n",
    "        super().__init__()\n",
    "        self.features = FeatureExtractor()\n",
    "        self.gap      = ConcatPool2d()             # preserves “max” path for a tiny star\n",
    "        self.head     = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2*160, 128), nn.ReLU(inplace=False),\n",
    "            nn.Dropout(p=0.10),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.head(x)\n",
    "        return x.clone()   # SHAP‑safe\n",
    "\n",
    "model = CNNRegressor(in_ch=4).to(DEVICE)\n",
    "print(model)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ----- Training (robust but simple; M1‑safe)\n",
    "LR = 6e-4\n",
    "WEIGHT_DECAY = 2e-4\n",
    "EPOCHS = 140\n",
    "PATIENCE = 20\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=EPOCHS, eta_min=1e-5)\n",
    "\n",
    "best_val = float(\"inf\"); best_state = None\n",
    "train_losses, val_losses = [], []; no_improve = 0\n",
    "\n",
    "def _finite(x): return torch.isfinite(x).all().item()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    for xb, zb, _yb in train_loader:\n",
    "        xb, zb = xb.to(DEVICE), zb.to(DEVICE)\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "        pred_z = model(xb)\n",
    "        loss = criterion(pred_z, zb)\n",
    "        if not _finite(loss):\n",
    "            # rare on MPS: back off LR and skip this step\n",
    "            for g in optimiser.param_groups: g[\"lr\"] = max(g[\"lr\"]*0.5, 1e-5)\n",
    "            continue\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimiser.step()\n",
    "        loss_sum += float(loss.item()) * xb.size(0)\n",
    "    train_loss = loss_sum / len(train_loader.dataset); train_losses.append(train_loss)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, zb, _yb in val_loader:\n",
    "            xb, zb = xb.to(DEVICE), zb.to(DEVICE)\n",
    "            pred_z = model(xb)\n",
    "            loss_sum += float(criterion(pred_z, zb).item()) * xb.size(0)\n",
    "    val_loss = loss_sum / len(val_loader.dataset); val_losses.append(val_loss)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Train L={train_loss:.6f} | Val L={val_loss:.6f} | LR={scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-7:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping (patience={PATIENCE}).\")\n",
    "            break\n",
    "\n",
    "# Restore best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ----- Metrics in y‑space\n",
    "def metrics_from_loader(name, loader):\n",
    "    model.eval()\n",
    "    preds_y, trues_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, zb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            zhat = model(xb).cpu().numpy().squeeze(-1)\n",
    "            yhat = from_z(zhat)\n",
    "            preds_y.append(yhat.astype(np.float32))\n",
    "            trues_y.append(yb.numpy().squeeze(-1).astype(np.float32))\n",
    "    y_pred = np.concatenate(preds_y); y_true = np.concatenate(trues_y)\n",
    "    ss_res = float(((y_true - y_pred)**2).sum())\n",
    "    ss_tot = float(((y_true - y_true.mean())**2).sum() + 1e-12)\n",
    "    r2   = 1.0 - ss_res/ss_tot\n",
    "    mae  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    print(f\"[{name}] R² = {r2:.4f} | MAE = {mae:.4f} | RMSE = {rmse:.4f}\")\n",
    "    return y_true, y_pred, r2, mae, rmse\n",
    "\n",
    "print(\"\\nPerformance (against observed y):\")\n",
    "y_tr, yhat_tr, r2_tr, _, _ = metrics_from_loader(\"Train\",      train_loader)\n",
    "y_va, yhat_va, r2_va, _, _ = metrics_from_loader(\"Validation\", val_loader)\n",
    "y_te, yhat_te, r2_te, _, _ = metrics_from_loader(\"Test\",       test_loader)\n",
    "\n",
    "# Noise ceiling on Test (label noise only)\n",
    "r2_ceiling_test = 1.0 - float(((y_true_test - y_te)**2).sum()) / float(((y_te - y_te.mean())**2).sum() + 1e-12)\n",
    "print(f\"\\n[Reference] Test noise‑ceiling R² (label noise): {r2_ceiling_test:.4f}\\n\")\n",
    "\n",
    "# ----- Plots\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_losses, label=\"Train loss\"); plt.plot(val_losses, label=\"Val loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training curves (CNN — ring prior + starness)\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_pred_vs_actual(y_actual, y_pred, title):\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.scatter(y_actual, y_pred, s=14, alpha=0.75)\n",
    "    lo = min(float(np.min(y_actual)), float(np.min(y_pred)))\n",
    "    hi = max(float(np.max(y_actual)), float(np.max(y_pred)))\n",
    "    pad = 0.02*(hi - lo) if hi > lo else 0.01\n",
    "    plt.plot([lo - pad, hi + pad], [lo - pad, hi + pad], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"Actual y\"); plt.ylabel(\"Predicted y\"); plt.title(title)\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_pred_vs_actual(y_tr, yhat_tr, \"Predicted vs Actual (Train, CNN)\")\n",
    "plot_pred_vs_actual(y_va, yhat_va, \"Predicted vs Actual (Validation, CNN)\")\n",
    "plot_pred_vs_actual(y_te, yhat_te, \"Predicted vs Actual (Test, CNN)\")\n",
    "\n",
    "# Small test panel (original → preprocessed → prediction)\n",
    "n_show = min(6, len(X_test))\n",
    "sel = np.random.choice(len(X_test), size=n_show, replace=False)\n",
    "fig, axes = plt.subplots(3, n_show, figsize=(2.0*n_show, 5.8))\n",
    "for j, k in enumerate(sel):\n",
    "    axes[0, j].imshow(X_test[k],   cmap=\"gray\", vmin=0, vmax=1); axes[0, j].set_title(\"Original\");      axes[0, j].axis(\"off\")\n",
    "    axes[1, j].imshow(X_test_s[k], cmap=\"gray\", vmin=0, vmax=1); axes[1, j].set_title(\"Bg‑subtracted\"); axes[1, j].axis(\"off\")\n",
    "    axes[2, j].text(0.02, 0.80, f\"y_true≈{y_true_test[k]:.3f}\", transform=axes[2, j].transAxes)\n",
    "    axes[2, j].text(0.02, 0.55, f\"y_obs ={y_test[k]:.3f}\",      transform=axes[2, j].transAxes)\n",
    "    axes[2, j].text(0.02, 0.30, f\"y_hat ={yhat_te[k]:.3f}\",     transform=axes[2, j].transAxes)\n",
    "    axes[2, j].axis(\"off\")\n",
    "axes[0,0].set_ylabel(\"Input\"); axes[1,0].set_ylabel(\"Preproc\"); axes[2,0].set_ylabel(\"Labels\")\n",
    "plt.suptitle(\"CNN predictions on held‑out test samples\", fontsize=12)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Takeaways:\")\n",
    "print(\"  • Ring priors tell the model *where* to look; the starness branch makes a tiny bright cue dominate via max pooling.\")\n",
    "print(\"  • A global-mean map gives an easy path to the overall scale so capacity focuses on local features.\")\n",
    "print(\"  • Architecture and training remain SHAP-safe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep SHAP — End‑to‑end, commented, and *interpretable* analysis on our three models\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "B. Models explained:\n",
    "  • OLS (pixels → y), Ridge (StandardScaler + Ridge → y), and CNN (4‑ch input: image + x,y,r; predicts z then inverted to y).\n",
    "  • CNN is made SHAP‑safe by:\n",
    "      – a SHAPCompatCNN wrapper that removes nn.Flatten and any in‑place ReLUs,\n",
    "      – using check_additivity=False (via safe_shap_values) to tolerate unsupported ops (e.g., sqrt in fixed stems).\n",
    "\n",
    "Changes in this version:\n",
    "  • Insertion curves removed (kept Deletion/AOPC).\n",
    "  • FIX: bounding‑box analysis is now robust (no negative kth; handles degenerate masks; always 2‑D indexing).\n",
    "  • NEW: before boxing we take the **largest weighted connected component** of the top‑|SHAP| pixels within the shell.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------\n",
    "# Imports and checks\n",
    "# -----------------\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# ----- Device (M1 → MPS if available)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Silence SHAP's \"unrecognized nn.Module\" warnings for harmless modules (ConcatPool2d / ReflectionPad2d)\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"unrecognized nn\\.Module: .*\",\n",
    "    category=UserWarning,\n",
    "    module=\"shap.explainers._deep.deep_pytorch\"\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install SHAP first:  pip install shap\") from e\n",
    "\n",
    "# Optional Spearman correlation (baseline sensitivity); fall back to Pearson if unavailable\n",
    "try:\n",
    "    from scipy.stats import spearmanr\n",
    "    HAVE_SPEARMAN = True\n",
    "except Exception:\n",
    "    HAVE_SPEARMAN = False\n",
    "\n",
    "# Sanity: ensure earlier cells provided these\n",
    "required = [\"images\", \"y_true_all\", \"y_obs_all\", \"ols\", \"final_ridge\", \"model\"]\n",
    "for v in required:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"Missing '{v}'. Please run the data, OLS/Ridge, and CNN training cells first.\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(DEVICE).eval()\n",
    "torch.set_grad_enabled(True)  # in case a previous cell disabled grad\n",
    "\n",
    "N, H, W = images.shape\n",
    "CENTER = (H - 1) / 2.0\n",
    "print(f\"[Setup] images: {images.shape} | device: {DEVICE}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Split indices (SAME recipe as earlier; seed=2025)\n",
    "# -----------------------------\n",
    "SEED = 2025\n",
    "TEST_FRAC = 0.20\n",
    "VAL_FRAC_WITHIN_TRAINVAL = 0.20  # 20% of 80% → 16% overall\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "idx_all = np.arange(N)\n",
    "\n",
    "# First split: TrainVal vs Test\n",
    "idx_trainval, idx_test = train_test_split(idx_all, test_size=TEST_FRAC, random_state=SEED)\n",
    "# Second split: Train vs Val (within TrainVal)\n",
    "idx_train, idx_val = train_test_split(idx_trainval, test_size=VAL_FRAC_WITHIN_TRAINVAL, random_state=SEED)\n",
    "\n",
    "print(f\"[Split] Train={len(idx_train)}, Val={len(idx_val)}, Test={len(idx_test)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Utility helpers\n",
    "# -----------------------------\n",
    "def area_trapezoid(y, x):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if hasattr(np, \"trapezoid\"):\n",
    "        return float(np.trapezoid(y, x))\n",
    "    return float(np.sum(0.5 * (y[1:] + y[:-1]) * (x[1:] - x[:-1])))\n",
    "\n",
    "def heatmap2d(arr2d, title, cmap=\"magma\", with_cbar=True):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(arr2d, cmap=cmap)\n",
    "    if with_cbar:\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.title(title); plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "def grid_overlays(imgs, shap_maps, title_prefix, ncols=4):\n",
    "    n = len(imgs)\n",
    "    ncols = min(ncols, n); nrows = int(np.ceil(n / ncols))\n",
    "    vmax = np.percentile(np.abs(np.concatenate([s.ravel() for s in shap_maps])), 99) if len(shap_maps) else 1.0\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(3.2 * ncols, 3.2 * nrows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "    for i in range(n):\n",
    "        axes[i].imshow(imgs[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[i].imshow(shap_maps[i], cmap=\"RdBu_r\", alpha=0.7, vmin=-vmax, vmax=+vmax)\n",
    "        axes[i].set_title(f\"{title_prefix} — sample {i}\", fontsize=9)\n",
    "        axes[i].axis(\"off\")\n",
    "    for k in range(n, nrows * ncols): axes[k].axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Radial & angular fields + “region share” metrics\n",
    "# -----------------------------\n",
    "ys, xs = np.indices((H, W))\n",
    "R = np.sqrt((ys - CENTER) ** 2 + (xs - CENTER) ** 2)\n",
    "theta = (np.arctan2(ys - CENTER, xs - CENTER) + 2*np.pi) % (2*np.pi)  # used by polar & bbox\n",
    "\n",
    "INNER_R = 18.0\n",
    "OUTER_R = 42.0\n",
    "mask_inner = (R <= INNER_R)  # hollow\n",
    "mask_shell = (R > INNER_R) & (R <= OUTER_R)  # metal shell\n",
    "mask_outer = (R > OUTER_R)  # outside\n",
    "\n",
    "def region_shares(abs_shap_map):\n",
    "    total = abs_shap_map.sum() + 1e-12\n",
    "    return (\n",
    "        float((abs_shap_map[mask_inner]).sum() / total),\n",
    "        float((abs_shap_map[mask_shell]).sum() / total),\n",
    "        float((abs_shap_map[mask_outer]).sum() / total),\n",
    "    )\n",
    "\n",
    "def region_means(abs_shap_map):\n",
    "    mi = float(abs_shap_map[mask_inner].mean() if mask_inner.sum() else 0.0)\n",
    "    ms = float(abs_shap_map[mask_shell].mean() if mask_shell.sum() else 0.0)\n",
    "    mo = float(abs_shap_map[mask_outer].mean() if mask_outer.sum() else 0.0)\n",
    "    return mi, ms, mo\n",
    "\n",
    "def top_p_concentration(abs_shap_map, p=0.05):\n",
    "    A = np.abs(abs_shap_map).ravel()\n",
    "    k = max(1, int(p * A.size))\n",
    "    kth = A.size - k  # non‑negative kth\n",
    "    if kth < 0: kth = 0\n",
    "    thr = np.partition(A, kth)[kth]\n",
    "    return float((A[A >= thr]).sum() / (A.sum() + 1e-12))\n",
    "\n",
    "def radial_profile(abs_shap_map, nbins=60):\n",
    "    r = R.ravel(); v = abs_shap_map.ravel()\n",
    "    bins = np.linspace(0, R.max() + 1e-6, nbins + 1)\n",
    "    prof = np.zeros(nbins, dtype=float)\n",
    "    for i in range(nbins):\n",
    "        m = (r >= bins[i]) & (r < bins[i + 1])\n",
    "        prof[i] = v[m].mean() if np.any(m) else 0.0\n",
    "    centres = 0.5 * (bins[:-1] + bins[1:])\n",
    "    return centres, prof\n",
    "\n",
    "def summarise_region_and_profile(stack, model_name):\n",
    "    shares = np.array([region_shares(np.abs(s)) for s in stack])\n",
    "    inner_s, shell_s, outer_s = shares.mean(axis=0)\n",
    "\n",
    "    means = np.array([region_means(np.abs(s)) for s in stack])\n",
    "    inner_m, shell_m, outer_m = means.mean(axis=0)\n",
    "\n",
    "    conc_1 = np.mean([top_p_concentration(np.abs(s), p=0.01) for s in stack])\n",
    "    conc_5 = np.mean([top_p_concentration(np.abs(s), p=0.05) for s in stack])\n",
    "\n",
    "    print(f\"\\n{model_name} — mean |SHAP| region *share* (area‑biased):\")\n",
    "    print(f\"  Hollow (r ≤ {INNER_R:.0f}) : {inner_s:6.2%}\")\n",
    "    print(f\"  Shell  ({INNER_R:.0f}<r≤{OUTER_R:.0f}): {shell_s:6.2%}\")\n",
    "    print(f\"  Outside(r > {OUTER_R:.0f}) : {outer_s:6.2%}\")\n",
    "\n",
    "    print(f\"{model_name} — mean |SHAP| *per pixel* (area‑fair):\")\n",
    "    print(f\"  Hollow:  {inner_m:.6f}  |  Shell: {shell_m:.6f}  |  Outside: {outer_m:.6f}\")\n",
    "    print(f\"{model_name} — concentration: top‑1% captures {conc_1:6.2%}, top‑5% captures {conc_5:6.2%}\")\n",
    "\n",
    "    profs, radii = [], None\n",
    "    for s in stack:\n",
    "        rr, p = radial_profile(np.abs(s), nbins=70)\n",
    "        profs.append(p); radii = rr\n",
    "    profs = np.stack(profs, axis=0)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(radii, profs.mean(axis=0))\n",
    "    plt.xlabel(\"Radius (px)\"); plt.ylabel(\"Mean |SHAP|\")\n",
    "    plt.title(f\"Radial profile of |SHAP| — {model_name}\\n(look for peaks around the shell boundaries)\")\n",
    "    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "def topk_mask(abs_map, k_frac):\n",
    "    A = np.abs(abs_map).ravel()\n",
    "    k = max(1, int(k_frac * A.size))\n",
    "    kth = A.size - k\n",
    "    if kth < 0: kth = 0\n",
    "    thr = np.partition(A, kth)[kth]\n",
    "    return (np.abs(abs_map) >= thr)\n",
    "\n",
    "def iou_and_precision_vs_shell(stack, k_fracs=(0.01, 0.02, 0.05, 0.10)):\n",
    "    results = {}; shell = mask_shell\n",
    "    for kf in k_fracs:\n",
    "        ious, precs = [], []\n",
    "        for s in stack:\n",
    "            m = topk_mask(s, kf).reshape(H, W)\n",
    "            inter = np.logical_and(m, shell).sum()\n",
    "            union = np.logical_or(m, shell).sum()\n",
    "            iou = inter / (union + 1e-12)\n",
    "            prec = inter / (m.sum() + 1e-12)\n",
    "            ious.append(iou); precs.append(prec)\n",
    "        results[kf] = (float(np.mean(ious)), float(np.mean(precs)))\n",
    "    return results\n",
    "\n",
    "def print_iou_precision(name, results):\n",
    "    print(f\"\\n{name} — Ground‑truth alignment vs shell (IoU@k / Precision@k):\")\n",
    "    for kf, (iou, prec) in results.items():\n",
    "        print(f\"  k={int(kf * 100)}%  IoU={iou:.3f}  |  Precision={prec:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# A common test subset to *explain*\n",
    "# -----------------------------\n",
    "N_EXPLAIN = min(24, len(idx_test))\n",
    "test_sel_idx = rng.choice(idx_test, size=N_EXPLAIN, replace=False)\n",
    "test_imgs = images[test_sel_idx]\n",
    "y_obs_test_sel = y_obs_all[test_sel_idx]\n",
    "\n",
    "# ==========================================================\n",
    "# 1) OLS — LinearExplainer on flattened pixels\n",
    "# ==========================================================\n",
    "print(\"\\n[OLS] Expect a ring; most |SHAP| in the shell.\")\n",
    "X_flat = images.reshape(N, -1).astype(np.float64)\n",
    "\n",
    "bg_ols_idx = rng.choice(idx_train, size=min(200, len(idx_train)), replace=False)\n",
    "X_bg_ols = X_flat[bg_ols_idx]\n",
    "X_ols_test = X_flat[test_sel_idx]\n",
    "\n",
    "expl_ols = shap.LinearExplainer(ols, X_bg_ols)  # identity link\n",
    "shap_ols_flat = expl_ols.shap_values(X_ols_test)\n",
    "if isinstance(shap_ols_flat, list):  # defensive\n",
    "    shap_ols_flat = shap_ols_flat[0]\n",
    "shap_ols_imgs = shap_ols_flat.reshape(-1, H, W)\n",
    "\n",
    "grid_overlays(test_imgs[:8], shap_ols_imgs[:8], \"OLS\")\n",
    "heatmap2d(np.mean(np.abs(shap_ols_imgs), axis=0), \"Global mean |SHAP| — OLS\")\n",
    "summarise_region_and_profile(shap_ols_imgs, \"OLS\")\n",
    "print_iou_precision(\"OLS\", iou_and_precision_vs_shell(shap_ols_imgs))\n",
    "\n",
    "# ==========================================================\n",
    "# 2) Ridge — LinearExplainer on STANDARDISED features\n",
    "# ==========================================================\n",
    "print(\"\\n[Ridge] Expect more diffused credit spread; still ring‑biased.\")\n",
    "if not isinstance(final_ridge, Pipeline):\n",
    "    raise RuntimeError(\"Expected 'final_ridge' to be a Pipeline(StandardScaler, Ridge).\")\n",
    "scaler = final_ridge.named_steps[\"scaler\"]\n",
    "ridge  = final_ridge.named_steps[\"ridge\"]\n",
    "\n",
    "Z_bg   = scaler.transform(X_flat[bg_ols_idx])\n",
    "Z_test = scaler.transform(X_flat[test_sel_idx])\n",
    "\n",
    "expl_ridge = shap.LinearExplainer(ridge, Z_bg)\n",
    "shap_ridge_scaled = expl_ridge.shap_values(Z_test)\n",
    "if isinstance(shap_ridge_scaled, list):\n",
    "    shap_ridge_scaled = shap_ridge_scaled[0]\n",
    "shap_ridge_imgs = shap_ridge_scaled.reshape(-1, H, W)\n",
    "\n",
    "grid_overlays(test_imgs[:8], shap_ridge_imgs[:8], \"Ridge\")\n",
    "heatmap2d(np.mean(np.abs(shap_ridge_imgs), axis=0), \"Global mean |SHAP| — Ridge\")\n",
    "summarise_region_and_profile(shap_ridge_imgs, \"Ridge\")\n",
    "print_iou_precision(\"Ridge\", iou_and_precision_vs_shell(shap_ridge_imgs))\n",
    "\n",
    "# ==========================================================\n",
    "# 3) CNN — DeepExplainer on 4‑channel inputs (image + CoordConv x,y,r)\n",
    "#      SHAP‑compatible wrapper (no nn.Flatten; no in‑place ops); additivity off\n",
    "# ==========================================================\n",
    "print(\"\\n[CNN] Strong edges on inner/outer shell expected; baseline matters — we select it by fidelity.\")\n",
    "\n",
    "# --- preprocessing used for CNN inputs (match the training recipe) ---\n",
    "def estimate_background_offset(img: np.ndarray, k: int = 10) -> float:\n",
    "    patches = [img[:k, :k], img[:k, -k:], img[-k:, :k], img[-k:, -k:]]\n",
    "    return float(np.median(np.concatenate([p.ravel() for p in patches])))\n",
    "\n",
    "def subtract_background(X: np.ndarray) -> np.ndarray:\n",
    "    Xp = np.empty_like(X)\n",
    "    for i, img in enumerate(X):\n",
    "        bg = estimate_background_offset(img, k=10)\n",
    "        im = img - bg\n",
    "        im[im < 0.0] = 0.0\n",
    "        Xp[i] = im\n",
    "    return Xp\n",
    "\n",
    "pm_ps_found = False\n",
    "if \"pix_mean\" in globals() and \"pix_std\" in globals():\n",
    "    pm = float(pix_mean); ps = float(pix_std); pm_ps_found = True\n",
    "elif \"pixel_mean\" in globals() and \"pixel_std\" in globals():\n",
    "    pm = float(pixel_mean); ps = float(pixel_std); pm_ps_found = True\n",
    "if not pm_ps_found:\n",
    "    X_train_bgsub = subtract_background(images[idx_train])\n",
    "    pm = float(X_train_bgsub.mean()); ps = float(X_train_bgsub.std() + 1e-8)\n",
    "\n",
    "# CoordConv maps\n",
    "x_lin = (xs - CENTER) / CENTER\n",
    "y_lin = (ys - CENTER) / CENTER\n",
    "r_map = np.sqrt(x_lin ** 2 + y_lin ** 2)\n",
    "r_map = r_map / (r_map.max() + 1e-12)\n",
    "\n",
    "def build_cnn_input(img_batch: np.ndarray) -> np.ndarray:\n",
    "    Xs = subtract_background(img_batch)\n",
    "    Xn = (Xs - pm) / ps\n",
    "    n = Xn.shape[0]\n",
    "    X4 = np.zeros((n, 4, H, W), dtype=np.float32)\n",
    "    X4[:, 0] = Xn.astype(np.float32)\n",
    "    X4[:, 1] = x_lin.astype(np.float32)\n",
    "    X4[:, 2] = y_lin.astype(np.float32)\n",
    "    X4[:, 3] = r_map.astype(np.float32)\n",
    "    return X4\n",
    "\n",
    "X_cnn_test_4 = build_cnn_input(images[test_sel_idx])\n",
    "\n",
    "# --- SHAP‑compatible wrapper that removes nn.Flatten and copies Linear weights ---\n",
    "class SHAPCompatCNN(nn.Module):\n",
    "    def __init__(self, base: nn.Module):\n",
    "        super().__init__()\n",
    "        m = copy.deepcopy(base).to(DEVICE).eval()\n",
    "        for module in m.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.inplace = False\n",
    "        self.features = m.features\n",
    "        self.gap = m.gap\n",
    "        # Extract Linear layers from head (expects exactly two)\n",
    "        lin_layers = [mod for mod in m.head if isinstance(mod, nn.Linear)]\n",
    "        if len(lin_layers) != 2:\n",
    "            raise RuntimeError(\"Expected two Linear layers in model.head.\")\n",
    "        self.fc1 = nn.Linear(lin_layers[0].in_features, lin_layers[0].out_features, bias=True)\n",
    "        self.act = nn.ReLU(inplace=False)\n",
    "        drops = [mod for mod in m.head if isinstance(mod, nn.Dropout)]\n",
    "        self.drop = nn.Dropout(p=drops[0].p if drops else 0.0)\n",
    "        self.fc2 = nn.Linear(lin_layers[1].in_features, lin_layers[1].out_features, bias=True)\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.copy_(lin_layers[0].weight); self.fc1.bias.copy_(lin_layers[0].bias)\n",
    "            self.fc2.weight.copy_(lin_layers[1].weight); self.fc2.bias.copy_(lin_layers[1].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)  # functional flatten (no nn.Flatten module)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        y = self.fc2(x)\n",
    "        return y.clone()\n",
    "\n",
    "shap_model = SHAPCompatCNN(model).to(DEVICE).eval()\n",
    "\n",
    "# --- candidate baselines for CNN SHAP ---\n",
    "def build_baseline(kind=\"train_subset\", m=64):\n",
    "    \"\"\"\n",
    "    kind ∈ {\"train_subset\", \"mean\", \"median\", \"blurred_mean\"}.\n",
    "    Returns a tensor on DEVICE suitable for DeepExplainer background.\n",
    "    \"\"\"\n",
    "    if kind == \"train_subset\":\n",
    "        idx = rng.choice(idx_train, size=min(m, len(idx_train)), replace=False)\n",
    "        X_bg = build_cnn_input(images[idx])\n",
    "        return torch.from_numpy(X_bg).to(DEVICE)\n",
    "    if kind in {\"mean\", \"median\", \"blurred_mean\"}:\n",
    "        X_tr = build_cnn_input(images[idx_train])\n",
    "        if kind == \"mean\":\n",
    "            img_ch = X_tr[:, 0].mean(axis=0, keepdims=True).astype(np.float32)\n",
    "        elif kind == \"median\":\n",
    "            img_ch = np.median(X_tr[:, 0], axis=0, keepdims=True).astype(np.float32)\n",
    "        else:  # blurred_mean\n",
    "            mean_img = X_tr[:, 0].mean(axis=0)\n",
    "            k = 5; pad = k // 2\n",
    "            tmp = np.pad(mean_img, pad, mode=\"reflect\"); out = np.zeros_like(mean_img)\n",
    "            for y in range(H):\n",
    "                for x in range(W):\n",
    "                    out[y, x] = tmp[y:y + k, x:x + k].mean()\n",
    "            img_ch = out[None, :, :].astype(np.float32)\n",
    "        Xb = np.zeros((1, 4, H, W), dtype=np.float32)\n",
    "        Xb[0, 0] = img_ch[0]; Xb[0, 1] = x_lin; Xb[0, 2] = y_lin; Xb[0, 3] = r_map\n",
    "        return torch.from_numpy(Xb).to(DEVICE)\n",
    "    raise ValueError(\"Unknown baseline kind.\")\n",
    "\n",
    "# --- DeepExplainer helper that disables additivity check (critical fix) ---\n",
    "def safe_shap_values(explainer, x_tensor):\n",
    "    try:\n",
    "        return explainer.shap_values(x_tensor, check_additivity=False)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            import shap.explainers._deep.deep_utils as _du\n",
    "            _du.TOLERANCE = 1e9\n",
    "        except Exception:\n",
    "            pass\n",
    "        return explainer.shap_values(x_tensor)\n",
    "\n",
    "# --- Deep SHAP for several baselines; pick the best by fidelity ---\n",
    "BASELINES = [\"train_subset\", \"mean\", \"median\", \"blurred_mean\"]\n",
    "baseline_results = {}\n",
    "torch.set_grad_enabled(True)\n",
    "X_test_t = torch.from_numpy(X_cnn_test_4).to(DEVICE).requires_grad_(True)\n",
    "\n",
    "def cnn_predict_y_from_4(X4):\n",
    "    \"\"\"Predict y (not z) for a batch of 4‑channel inputs with the *trained* CNN clone used for SHAP.\"\"\"\n",
    "    y_train_obs = y_obs_all[idx_train]\n",
    "    y_mean = float(np.mean(y_train_obs)); y_std = float(np.std(y_train_obs) + 1e-8)\n",
    "    def from_z(z): return z * y_std + y_mean\n",
    "    shap_model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = shap_model(torch.from_numpy(X4).to(DEVICE)).cpu().numpy().squeeze(-1)\n",
    "    return from_z(z)\n",
    "\n",
    "fractions = [0.01, 0.02, 0.05, 0.10, 0.20]\n",
    "y_obs_sel = y_obs_test_sel\n",
    "\n",
    "for bkind in BASELINES:\n",
    "    bg_t = build_baseline(bkind, m=64 if bkind == \"train_subset\" else 1)\n",
    "    expl = shap.DeepExplainer(shap_model, bg_t)\n",
    "    vals = safe_shap_values(expl, X_test_t)\n",
    "    sh = vals[0] if isinstance(vals, list) else vals  # (n, 4, H, W)\n",
    "    sh_img = sh[:, 0, :, :]\n",
    "    baseline_results[bkind] = {\"all\": sh, \"img\": sh_img}\n",
    "\n",
    "    # IoU/Precision@k\n",
    "    io = iou_and_precision_vs_shell(sh_img, k_fracs=(0.01, 0.05))\n",
    "    baseline_results[bkind][\"iou_prec\"] = io\n",
    "\n",
    "    # Deletion AOPC\n",
    "    n, _, h, w = X_cnn_test_4.shape; D = h * w\n",
    "    order = np.argsort(-np.abs(sh_img.reshape(n, D)), axis=1)\n",
    "    y0 = cnn_predict_y_from_4(X_cnn_test_4); r2_base = r2_score(y_obs_sel, y0)\n",
    "    r2_list = []\n",
    "    for frac in fractions:\n",
    "        k = max(1, int(frac * D))\n",
    "        X_mod = X_cnn_test_4.copy()\n",
    "        for i in range(n):\n",
    "            idxk = order[i, :k]; rr, cc = idxk // w, idxk % w\n",
    "            X_mod[i, 0, rr, cc] = 0.0\n",
    "        y_pred_mod = cnn_predict_y_from_4(X_mod)\n",
    "        r2_list.append(r2_score(y_obs_sel, y_pred_mod))\n",
    "    drops = (r2_base - np.array(r2_list))\n",
    "    aopc = area_trapezoid(drops, np.array(fractions))\n",
    "    baseline_results[bkind][\"aopc\"] = float(aopc)\n",
    "    baseline_results[bkind][\"r2_base\"] = float(r2_base)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Pick best baseline by (IoU@5% + IoU@1%) + AOPC (simple normalised score)\n",
    "def normalise(v):\n",
    "    v = np.asarray(v, dtype=float)\n",
    "    if np.ptp(v) < 1e-12:\n",
    "        return np.ones_like(v)\n",
    "    return (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "scores = []; kinds = []\n",
    "for k, d in baseline_results.items():\n",
    "    io1 = d[\"iou_prec\"][0.01][0]; io5 = d[\"iou_prec\"][0.05][0]\n",
    "    kinds.append(k); scores.append([io1, io5, d[\"aopc\"]])\n",
    "scores = np.array(scores)\n",
    "score = normalise(scores[:, 0]) + normalise(scores[:, 1]) + normalise(scores[:, 2])\n",
    "BEST_BASELINE = kinds[int(np.argmax(score))]\n",
    "print(f\"\\n[CNN] Baseline selection by fidelity → chosen: **{BEST_BASELINE}**\")\n",
    "for k in BASELINES:\n",
    "    io1 = baseline_results[k][\"iou_prec\"][0.01][0]\n",
    "    io5 = baseline_results[k][\"iou_prec\"][0.05][0]\n",
    "    print(f\"  {k:13s}  IoU@1%={io1:.3f}  IoU@5%={io5:.3f}  AOPC={baseline_results[k]['aopc']:.4f}\")\n",
    "\n",
    "# Use the chosen baseline’s SHAP maps from now on\n",
    "shap_cnn_allch = baseline_results[BEST_BASELINE][\"all\"]\n",
    "shap_cnn_imgch = baseline_results[BEST_BASELINE][\"img\"]\n",
    "\n",
    "# Light background bootstrapping (variability hints)\n",
    "print(\"\\n[CNN] Background bootstrapping (variability over train‑subset backgrounds)\")\n",
    "B = 3\n",
    "boot_shares = []\n",
    "for b in range(B):\n",
    "    bg_t = build_baseline(\"train_subset\", m=32)\n",
    "    torch.set_grad_enabled(True)\n",
    "    e = shap.DeepExplainer(shap_model, bg_t)\n",
    "    vals = safe_shap_values(e, X_test_t)\n",
    "    torch.set_grad_enabled(False)\n",
    "    sh = vals[0] if isinstance(vals, list) else vals\n",
    "    sh_img = sh[:, 0]\n",
    "    shares = np.array([region_shares(np.abs(s)) for s in sh_img])\n",
    "    boot_shares.append(shares.mean(axis=0))\n",
    "boot_shares = np.stack(boot_shares, axis=0)\n",
    "means = boot_shares.mean(axis=0); stds = boot_shares.std(axis=0)\n",
    "print(f\"  Region share mean±sd (Hollow/Shell/Outside): \"\n",
    "      f\"{means[0]:.3f}±{stds[0]:.3f} / {means[1]:.3f}±{stds[1]:.3f} / {means[2]:.3f}±{stds[2]:.3f}\")\n",
    "\n",
    "# Local overlays & global maps for CNN (chosen baseline)\n",
    "grid_overlays(test_imgs[:8], shap_cnn_imgch[:8], f\"CNN [{BEST_BASELINE}]\")\n",
    "heatmap2d(np.mean(np.abs(shap_cnn_imgch), axis=0), f\"Global mean |SHAP| — CNN [{BEST_BASELINE}]\")\n",
    "summarise_region_and_profile(shap_cnn_imgch, f\"CNN [{BEST_BASELINE}]\")\n",
    "print_iou_precision(f\"CNN [{BEST_BASELINE}]\", iou_and_precision_vs_shell(shap_cnn_imgch))\n",
    "\n",
    "# -----------------------------\n",
    "# Deletion curves — measured as R² changes (OLS / Ridge / CNN)\n",
    "# -----------------------------\n",
    "print(\"\\n[Deletion curves: what they mean]\\n\"\n",
    "      \"• Remove top‑|SHAP| pixels and re‑score R² vs true labels.\\n\"\n",
    "      \"  A *faithful* explanation targets genuinely important pixels ⇒ R² **drops fast**. We summarise by AOPC.\\n\")\n",
    "\n",
    "def plot_deletion_r2(title, fractions, r2_base, r2_list):\n",
    "    plt.figure(figsize=(5.8, 4))\n",
    "    plt.plot([f * 100 for f in fractions], [r2_base] * len(fractions), linestyle=\"--\", label=\"Baseline R²\")\n",
    "    plt.plot([f * 100 for f in fractions], r2_list, marker=\"o\", label=\"R² after deletion\")\n",
    "    plt.xlabel(\"Top-|SHAP| pixels removed (%)\")\n",
    "    plt.ylabel(\"R² vs true labels (test subset)\")\n",
    "    plt.title(title); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    drops = (r2_base - np.array(r2_list))\n",
    "    aopc = area_trapezoid(drops, np.array(fractions))\n",
    "    print(f\"{title} — AOPC (higher → better fidelity): {aopc:.4f}\")\n",
    "    return aopc\n",
    "\n",
    "# --- OLS deletion\n",
    "y0_ols_pred = ols.predict(X_ols_test)\n",
    "r2_base_ols = r2_score(y_obs_test_sel, y0_ols_pred)\n",
    "order_ols = np.argsort(-np.abs(shap_ols_flat), axis=1)\n",
    "r2_list_ols_del = []\n",
    "mu = X_bg_ols.mean(axis=0)\n",
    "for frac in fractions:\n",
    "    k = max(1, int(frac * X_ols_test.shape[1]))\n",
    "    X_del = X_ols_test.copy()\n",
    "    for i in range(X_del.shape[0]):\n",
    "        X_del[i, order_ols[i, :k]] = mu[order_ols[i, :k]]\n",
    "    r2_list_ols_del.append(r2_score(y_obs_test_sel, ols.predict(X_del)))\n",
    "aopc_ols = plot_deletion_r2(\"Deletion curve — OLS (R² drop)\", fractions, r2_base_ols, r2_list_ols_del)\n",
    "\n",
    "# --- Ridge deletion\n",
    "y0_ridge_pred = final_ridge.predict(X_flat[test_sel_idx])\n",
    "r2_base_ridge = r2_score(y_obs_test_sel, y0_ridge_pred)\n",
    "order_ridge = np.argsort(-np.abs(shap_ridge_scaled), axis=1)\n",
    "r2_list_ridge_del = []\n",
    "Z_test = scaler.transform(X_flat[test_sel_idx])\n",
    "for frac in fractions:\n",
    "    k = max(1, int(frac * Z_test.shape[1]))\n",
    "    Z_del = Z_test.copy()\n",
    "    for i in range(Z_del.shape[0]):\n",
    "        Z_del[i, order_ridge[i, :k]] = 0.0  # 0 == scaled mean\n",
    "    y_pred_del = (Z_del @ ridge.coef_.ravel() + ridge.intercept_)\n",
    "    r2_list_ridge_del.append(r2_score(y_obs_test_sel, y_pred_del))\n",
    "aopc_ridge = plot_deletion_r2(\"Deletion curve — Ridge (R² drop)\", fractions, r2_base_ridge, r2_list_ridge_del)\n",
    "\n",
    "# --- CNN deletion (image channel only; coords intact)\n",
    "y_train_obs = y_obs_all[idx_train]\n",
    "y_mean = float(np.mean(y_train_obs)); y_std = float(np.std(y_train_obs) + 1e-8)\n",
    "def from_z(z): return z * y_std + y_mean\n",
    "\n",
    "def cnn_predict_y(_model, X4):\n",
    "    _model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = _model(torch.from_numpy(X4).to(DEVICE)).cpu().numpy().squeeze(-1)\n",
    "    return from_z(z)\n",
    "\n",
    "y0_cnn_pred = cnn_predict_y(shap_model, X_cnn_test_4)\n",
    "r2_base_cnn = r2_score(y_obs_test_sel, y0_cnn_pred)\n",
    "n, _, h, w = X_cnn_test_4.shape; D = h * w\n",
    "order_cnn = np.argsort(-np.abs(shap_cnn_imgch.reshape(n, D)), axis=1)\n",
    "\n",
    "r2_list_cnn_del = []\n",
    "for frac in fractions:\n",
    "    k = max(1, int(frac * D))\n",
    "    X_mod = X_cnn_test_4.copy()\n",
    "    for i in range(n):\n",
    "        idx = order_cnn[i, :k]; rr, cc = idx // w, idx % w\n",
    "        X_mod[i, 0, rr, cc] = 0.0\n",
    "    y_pred_mod = cnn_predict_y(shap_model, X_mod)\n",
    "    r2_list_cnn_del.append(r2_score(y_obs_test_sel, y_pred_mod))\n",
    "aopc_cnn = plot_deletion_r2(f\"Deletion curve — CNN [{BEST_BASELINE}] (R² drop)\", fractions, r2_base_cnn, r2_list_cnn_del)\n",
    "\n",
    "# -----------------------------\n",
    "# Bounding‑box analysis over top‑|SHAP| within the shell\n",
    "#   • Robust against shape issues (always flattens indices safely).\n",
    "#   • Takes the **largest weighted connected component** before boxing.\n",
    "# -----------------------------\n",
    "def circular_coverage(angles_rad):\n",
    "    \"\"\"Minimal angular span (radians) covering all given angles on the circle.\"\"\"\n",
    "    if angles_rad.size == 0:\n",
    "        return 0.0\n",
    "    a = np.sort(angles_rad)\n",
    "    diffs = np.diff(np.concatenate([a, a[:1] + 2*np.pi]))\n",
    "    max_gap = np.max(diffs)\n",
    "    return float(2*np.pi - max_gap)\n",
    "\n",
    "def _largest_component(mask2d, weights2d=None):\n",
    "    \"\"\"\n",
    "    8‑connected components on a boolean 2‑D mask.\n",
    "    Returns the mask of the component with largest total weight (or largest size if weights is None).\n",
    "    \"\"\"\n",
    "    M = np.ascontiguousarray(mask2d.astype(bool))\n",
    "    H_, W_ = M.shape\n",
    "    visited = np.zeros_like(M, dtype=bool)\n",
    "    best_weight = -1.0\n",
    "    best_comp = None\n",
    "\n",
    "    # Precompute neighbor offsets (8‑connectivity)\n",
    "    nbrs = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "\n",
    "    # Iterate over all True pixels\n",
    "    ys_, xs_ = np.where(M)\n",
    "    for y0, x0 in zip(ys_, xs_):\n",
    "        if visited[y0, x0]:\n",
    "            continue\n",
    "        # BFS/stack\n",
    "        stack = [(y0, x0)]\n",
    "        visited[y0, x0] = True\n",
    "        comp_pixels = [(y0, x0)]\n",
    "        while stack:\n",
    "            y, x = stack.pop()\n",
    "            for dy, dx in nbrs:\n",
    "                yy, xx = y + dy, x + dx\n",
    "                if 0 <= yy < H_ and 0 <= xx < W_ and (not visited[yy, xx]) and M[yy, xx]:\n",
    "                    visited[yy, xx] = True\n",
    "                    stack.append((yy, xx))\n",
    "                    comp_pixels.append((yy, xx))\n",
    "        # weight for this component\n",
    "        if weights2d is not None:\n",
    "            w = float(np.sum([weights2d[y, x] for (y, x) in comp_pixels]))\n",
    "        else:\n",
    "            w = float(len(comp_pixels))\n",
    "        if w > best_weight:\n",
    "            best_weight = w\n",
    "            best_comp = comp_pixels\n",
    "\n",
    "    comp_mask = np.zeros_like(M, dtype=bool)\n",
    "    if best_comp is not None:\n",
    "        ys_c, xs_c = zip(*best_comp)\n",
    "        comp_mask[ys_c, xs_c] = True\n",
    "    return comp_mask\n",
    "\n",
    "def compute_bbox_metrics(stack, model_name, imgs_for_overlay=None, draw_overlays=True, overlay_n=6,\n",
    "                         p_top=0.01):\n",
    "    \"\"\"\n",
    "    For each SHAP map in `stack`, within the shell:\n",
    "      • select the TOP p_top fraction of |SHAP| pixels (non-negative kth),\n",
    "      • take the **largest weighted connected component** of that mask,\n",
    "      • compute the tight axis-aligned bounding box,\n",
    "      • report area fraction of the shell covered by the box,\n",
    "               fraction of total |SHAP| in shell captured by the box,\n",
    "               angular coverage (deg) of selected pixels,\n",
    "               radial width normalised by shell thickness.\n",
    "    Returns a dict of per-sample arrays and prints robust summarised stats.\n",
    "    \"\"\"\n",
    "    area_fracs = []\n",
    "    shap_shares = []\n",
    "    ang_degs    = []\n",
    "    rad_widths  = []\n",
    "\n",
    "    shell_area = float(mask_shell.sum())\n",
    "    shell_thick = (OUTER_R - INNER_R)\n",
    "\n",
    "    overlays_done = 0\n",
    "    if imgs_for_overlay is None:\n",
    "        imgs_for_overlay = []\n",
    "\n",
    "    for i, S in enumerate(stack):\n",
    "        # ensure 2‑D map\n",
    "        S2 = np.asarray(S)\n",
    "        if S2.ndim != 2:\n",
    "            # if something off slipped in, reduce to the last two dims\n",
    "            S2 = np.asarray(S2)[-H:, -W:]\n",
    "            S2 = S2.reshape(H, W)\n",
    "        Sabs = np.abs(S2)\n",
    "\n",
    "        vals = Sabs[mask_shell].ravel()\n",
    "\n",
    "        if vals.size == 0 or not np.any(vals > 0):\n",
    "            # skip gracefully\n",
    "            continue\n",
    "\n",
    "        # TOP‑p% threshold (non‑negative kth)\n",
    "        k = max(1, int(np.ceil(p_top * vals.size)))\n",
    "        kth = vals.size - k\n",
    "        if kth < 0: kth = 0\n",
    "        thr = np.partition(vals, kth)[kth]\n",
    "\n",
    "        # binary mask of top‑p% within shell\n",
    "        M = np.zeros((H, W), dtype=bool)\n",
    "        M_shell = (Sabs >= thr) & mask_shell\n",
    "\n",
    "        # Focus on **largest weighted component** (weights = |SHAP|)\n",
    "        M_sel = _largest_component(M_shell, weights2d=Sabs)\n",
    "\n",
    "        # fallback: if component is empty (pathological), take the single max pixel in shell\n",
    "        if not np.any(M_sel):\n",
    "            shell_idx = np.column_stack(np.where(mask_shell))\n",
    "            # argmax over shell pixels\n",
    "            argmax_flat = np.argmax(Sabs[mask_shell])\n",
    "            yx = shell_idx[argmax_flat]\n",
    "            M_sel = np.zeros((H, W), dtype=bool)\n",
    "            M_sel[yx[0], yx[1]] = True\n",
    "\n",
    "        # coords of selected pixels (robust way, works for any shape)\n",
    "        coords = np.column_stack(np.where(M_sel))\n",
    "        y_min, y_max = int(coords[:, 0].min()), int(coords[:, 0].max())\n",
    "        x_min, x_max = int(coords[:, 1].min()), int(coords[:, 1].max())\n",
    "\n",
    "        # area fraction of shell covered by the BOX ∩ shell\n",
    "        box_mask = np.zeros((H, W), dtype=bool)\n",
    "        box_mask[y_min:y_max+1, x_min:x_max+1] = True\n",
    "        box_shell = np.logical_and(box_mask, mask_shell)\n",
    "        area_frac = float(box_shell.sum() / (shell_area + 1e-12))\n",
    "        area_fracs.append(area_frac)\n",
    "\n",
    "        # |SHAP| share captured by the BOX within the shell\n",
    "        shap_shell_total = float(Sabs[mask_shell].sum() + 1e-12)\n",
    "        shap_in_box = float(Sabs[box_shell].sum())\n",
    "        shap_share = shap_in_box / shap_shell_total\n",
    "        shap_shares.append(shap_share)\n",
    "\n",
    "        # angular coverage (deg) of SELECTED pixels\n",
    "        ang = theta[M_sel]\n",
    "        ang_span = circular_coverage(ang.ravel())\n",
    "        ang_deg = float(ang_span * 180.0 / np.pi)\n",
    "        ang_degs.append(ang_deg)\n",
    "\n",
    "        # radial width normalised by shell thickness (using selected pixels)\n",
    "        r_sel = R[M_sel]\n",
    "        rad_w = float((r_sel.max() - r_sel.min()) / (shell_thick + 1e-12))\n",
    "        rad_widths.append(rad_w)\n",
    "\n",
    "        # overlays (draw box + highlight selected component)\n",
    "        if draw_overlays and (overlays_done < overlay_n) and (i < len(imgs_for_overlay)):\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(3.2, 3.2))\n",
    "            ax.imshow(imgs_for_overlay[i], cmap=\"gray\", vmin=0, vmax=1)\n",
    "            rect = patches.Rectangle((x_min, y_min), x_max - x_min + 1, y_max - y_min + 1,\n",
    "                                     linewidth=1.8, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            # faint fill for the selected component\n",
    "            M_alpha = np.zeros((H, W), dtype=float)\n",
    "            M_alpha[M_sel] = 0.5\n",
    "            ax.imshow(M_alpha, cmap=\"Reds\", alpha=0.25, vmin=0, vmax=1)\n",
    "            ax.set_title(f\"{model_name} — bbox on top-{int(p_top*100)}% |SHAP| (largest component)\", fontsize=9)\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "            overlays_done += 1\n",
    "\n",
    "    # Summaries\n",
    "    area_fracs = np.array(area_fracs, dtype=float)\n",
    "    shap_shares = np.array(shap_shares, dtype=float)\n",
    "    ang_degs    = np.array(ang_degs, dtype=float)\n",
    "    rad_widths  = np.array(rad_widths, dtype=float)\n",
    "\n",
    "    def summ(name, arr, unit=\"\"):\n",
    "        if arr.size == 0:\n",
    "            print(f\"  {name}: n=0\")\n",
    "            return (np.nan, np.nan, np.nan)\n",
    "        print(f\"  {name}: median={np.nanmedian(arr):.3f}{unit}  \"\n",
    "              f\"IQR=({np.nanpercentile(arr,25):.3f}{unit}, {np.nanpercentile(arr,75):.3f}{unit})\")\n",
    "        return (float(np.nanmedian(arr)),\n",
    "                float(np.nanpercentile(arr,25)),\n",
    "                float(np.nanpercentile(arr,75)))\n",
    "\n",
    "    print(f\"\\n[{model_name}] Bounding‑box metrics on top‑|SHAP| within the shell (largest component; p_top={p_top*100:.1f}%):\")\n",
    "    s1 = summ(\"Area fraction of shell (BOX∩shell / shell)\", area_fracs)\n",
    "    s2 = summ(\"|SHAP| share captured (BOX∩shell / shell)\",   shap_shares)\n",
    "    s3 = summ(\"Angular coverage (deg) of selected pixels\",     ang_degs, unit=\"°\")\n",
    "    s4 = summ(\"Radial width / shell thickness\",                rad_widths)\n",
    "\n",
    "    return {\n",
    "        \"area_frac\": area_fracs, \"shap_share\": shap_shares,\n",
    "        \"angle_deg\": ang_degs, \"rad_width\": rad_widths,\n",
    "        \"summary\": {\"area_frac\": s1, \"shap_share\": s2, \"angle_deg\": s3, \"rad_width\": s4}\n",
    "    }\n",
    "\n",
    "# --- Run bounding‑box metrics for each model (top 1% by default)\n",
    "P_TOP = 0.01\n",
    "bbox_stats_ols   = compute_bbox_metrics(shap_ols_imgs,   \"OLS\",   imgs_for_overlay=test_imgs[:8], draw_overlays=False, p_top=P_TOP)\n",
    "bbox_stats_ridge = compute_bbox_metrics(shap_ridge_imgs, \"Ridge\", imgs_for_overlay=test_imgs[:8], draw_overlays=False, p_top=P_TOP)\n",
    "bbox_stats_cnn   = compute_bbox_metrics(shap_cnn_imgch,  f\"CNN [{BEST_BASELINE}]\",\n",
    "                                        imgs_for_overlay=test_imgs[:8], draw_overlays=True, p_top=P_TOP)\n",
    "\n",
    "print(\"\\n[Reading the bounding‑box summaries]\")\n",
    "print(\"• A small **Area fraction** with a large **|SHAP| share** and **low angular coverage** means the model relies on a tight, local cue.\\n\"\n",
    "      \"• For the *hard/star* dataset, the CNN should have the **smallest boxes** and **tightest angles** among the three, \"\n",
    "      \"while OLS/Ridge remain diffuse.\\n\"\n",
    "      \"• For the *simple* dataset (no star), the CNN’s boxes will typically hug thin arcs on the inner/outer edges; \"\n",
    "      \"angles will be modest, not full‑ring, and radial width < 0.5.\\n\")\n",
    "\n",
    "# ================================\n",
    "# Global star-capture analysis (robust 2-D handling + enrichment)\n",
    "# ================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n[Global star-capture analysis]\")\n",
    "\n",
    "# --- Helpers to guarantee 2-D (H,W) arrays and clean values\n",
    "def _ensure_hw(x):\n",
    "    a = np.asarray(x)\n",
    "    a = np.squeeze(a)\n",
    "    if a.ndim != 2:\n",
    "        # Force-shape to (H, W) if a weird singleton remains\n",
    "        a = a.reshape(H, W)\n",
    "    # Clean NaN/Inf for safety\n",
    "    a = np.nan_to_num(a, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return a\n",
    "\n",
    "# Reuse ring geometry already defined: R, theta, mask_shell, INNER_R, OUTER_R\n",
    "R_mid   = 0.5 * (INNER_R + OUTER_R)\n",
    "R_sigma = 0.18 * (OUTER_R - INNER_R)\n",
    "RING_W  = np.exp(-0.5 * ((R - R_mid) / (R_sigma + 1e-8))**2) * mask_shell\n",
    "shell_area = float(mask_shell.sum())\n",
    "shell_thick = (OUTER_R - INNER_R)\n",
    "\n",
    "def _box_blur3(a):\n",
    "    a = _ensure_hw(a)\n",
    "    tmp = np.pad(a, 1, mode=\"reflect\")\n",
    "    out = np.zeros_like(a, dtype=float)\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            out[y, x] = tmp[y:y+3, x:x+3].mean()\n",
    "    return out\n",
    "\n",
    "def find_star_proxy_center(img):\n",
    "    # Ring-weighted, high-tail emphasis\n",
    "    z = _ensure_hw(img) - float(np.median(img))\n",
    "    z[z < 0.0] = 0.0\n",
    "    m = z * RING_W\n",
    "    m = _box_blur3(m)\n",
    "    yi, xi = np.unravel_index(np.argmax(m), m.shape)\n",
    "    return int(yi), int(xi)\n",
    "\n",
    "def disc_mask(yc, xc, radius=4):\n",
    "    yy, xx = np.ogrid[:H, :W]\n",
    "    return ((yy - yc)**2 + (xx - xc)**2) <= (radius*radius)\n",
    "\n",
    "def circular_coverage(angles_rad):\n",
    "    if angles_rad.size == 0:\n",
    "        return 0.0\n",
    "    a = np.sort(angles_rad)\n",
    "    diffs = np.diff(np.concatenate([a, a[:1] + 2*np.pi]))\n",
    "    max_gap = np.max(diffs)\n",
    "    return float(2*np.pi - max_gap)\n",
    "\n",
    "def circ_mean_angle(angles, weights=None):\n",
    "    if angles.size == 0:\n",
    "        return np.nan\n",
    "    if weights is None:\n",
    "        weights = np.ones_like(angles, dtype=float)\n",
    "    c = np.sum(weights * np.cos(angles))\n",
    "    s = np.sum(weights * np.sin(angles))\n",
    "    return float(np.arctan2(s, c))\n",
    "\n",
    "def circ_abs_diff_deg(a, b):\n",
    "    d = np.abs((a - b + np.pi) % (2*np.pi) - np.pi)\n",
    "    return float(d * 180.0 / np.pi)\n",
    "\n",
    "# ---- robust 8-connected component on 2-D boolean masks\n",
    "def _largest_component(mask2d, weights2d=None):\n",
    "    M = _ensure_hw(mask2d).astype(bool)\n",
    "    Wts = _ensure_hw(weights2d) if weights2d is not None else None\n",
    "    H_, W_ = M.shape\n",
    "    visited = np.zeros_like(M, dtype=bool)\n",
    "    best_weight = -1.0\n",
    "    best_comp = None\n",
    "    nbrs = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "    ys_, xs_ = np.where(M)\n",
    "    for y0, x0 in zip(ys_, xs_):\n",
    "        if visited[y0, x0]:\n",
    "            continue\n",
    "        stack = [(y0, x0)]\n",
    "        visited[y0, x0] = True\n",
    "        comp_pixels = [(y0, x0)]\n",
    "        while stack:\n",
    "            y, x = stack.pop()\n",
    "            for dy, dx in nbrs:\n",
    "                yy, xx = y + dy, x + dx\n",
    "                if 0 <= yy < H_ and 0 <= xx < W_ and (not visited[yy, xx]) and M[yy, xx]:\n",
    "                    visited[yy, xx] = True\n",
    "                    stack.append((yy, xx))\n",
    "                    comp_pixels.append((yy, xx))\n",
    "        if Wts is not None:\n",
    "            w = float(np.sum([Wts[y, x] for (y, x) in comp_pixels]))\n",
    "        else:\n",
    "            w = float(len(comp_pixels))\n",
    "        if w > best_weight:\n",
    "            best_weight = w\n",
    "            best_comp = comp_pixels\n",
    "    comp_mask = np.zeros_like(M, dtype=bool)\n",
    "    if best_comp:\n",
    "        ys_c, xs_c = zip(*best_comp)\n",
    "        comp_mask[ys_c, xs_c] = True\n",
    "    return comp_mask\n",
    "\n",
    "def largest_component_mask(abs_map, top_frac=0.01):\n",
    "    S = _ensure_hw(abs_map)\n",
    "    vals = S[mask_shell].ravel()\n",
    "    if vals.size == 0:\n",
    "        return np.zeros((H, W), dtype=bool)\n",
    "    vals = np.nan_to_num(vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    k = max(1, int(np.ceil(top_frac * vals.size)))\n",
    "    kth = max(0, vals.size - k)           # non-negative kth\n",
    "    thr = np.partition(vals, kth)[kth]\n",
    "    M_shell = (S >= thr) & mask_shell\n",
    "    return _largest_component(M_shell, weights2d=S)\n",
    "\n",
    "def global_star_capture(shap_stack, model_name, imgs_subset, disc_radius=4):\n",
    "    shares, enrichments, ang_errs = [], [], []\n",
    "    for i in range(len(shap_stack)):\n",
    "        img = _ensure_hw(imgs_subset[i])\n",
    "        S   = np.abs(_ensure_hw(shap_stack[i]))\n",
    "\n",
    "        # (1) star disc\n",
    "        ys_, xs_ = find_star_proxy_center(img)\n",
    "        D = disc_mask(ys_, xs_, radius=disc_radius)\n",
    "        disc_shell = np.logical_and(D, mask_shell)\n",
    "\n",
    "        # (2) |SHAP| share inside disc (normalised by |SHAP| on shell)\n",
    "        denom = float(S[mask_shell].sum() + 1e-12)\n",
    "        numer = float(S[disc_shell].sum())\n",
    "        share = numer / denom\n",
    "        shares.append(share)\n",
    "\n",
    "        # (3) enrichment vs area fraction of disc in the shell\n",
    "        area_frac_disc = float(disc_shell.sum() / (shell_area + 1e-12))\n",
    "        enrichments.append(share / (area_frac_disc + 1e-12))\n",
    "\n",
    "        # (4) angular alignment vs largest |SHAP| component\n",
    "        M_sel = largest_component_mask(S, top_frac=0.01)\n",
    "        if np.any(M_sel):\n",
    "            th_star = float(theta[ys_, xs_])\n",
    "            ww = S[M_sel]\n",
    "            th_comp = circ_mean_angle(theta[M_sel], weights=ww / (ww.sum() + 1e-12))\n",
    "            ang_errs.append(circ_abs_diff_deg(th_star, th_comp))\n",
    "        else:\n",
    "            ang_errs.append(np.nan)\n",
    "\n",
    "    shares = np.array(shares, dtype=float)\n",
    "    enrichments = np.array(enrichments, dtype=float)\n",
    "    ang_errs = np.array(ang_errs, dtype=float)\n",
    "\n",
    "    def summarise(name, arr, unit=\"\"):\n",
    "        nfin = int(np.sum(np.isfinite(arr)))\n",
    "        print(f\"  {model_name} — {name}: \"\n",
    "              f\"median={np.nanmedian(arr):.3f}{unit} | \"\n",
    "              f\"IQR=({np.nanpercentile(arr,25):.3f}{unit}, {np.nanpercentile(arr,75):.3f}{unit}) | n={nfin}\")\n",
    "\n",
    "    print(f\"\\n[{model_name}] Star-disc capture (radius={disc_radius}px), enrichment, and angular alignment\")\n",
    "    summarise(\"SHAP share inside star disc\", shares)\n",
    "    summarise(\"enrichment (share / area)\",   enrichments)\n",
    "    summarise(\"abs angular error vs star (deg)\", ang_errs, unit=\"°\")\n",
    "    return shares, enrichments, ang_errs\n",
    "\n",
    "# Run on the same subset used for SHAP (test_imgs / shap_*_imgs)\n",
    "shares_ols,   enrich_ols,   ang_ols   = global_star_capture(shap_ols_imgs,   \"OLS\",   test_imgs, disc_radius=4)\n",
    "shares_ridge, enrich_ridge, ang_ridge = global_star_capture(shap_ridge_imgs, \"Ridge\", test_imgs, disc_radius=4)\n",
    "shares_cnn,   enrich_cnn,   ang_cnn   = global_star_capture(shap_cnn_imgch,  f\"CNN [{BEST_BASELINE}]\", test_imgs, disc_radius=4)\n",
    "\n",
    "# Optional: global comparison plots\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([shares_ols, shares_ridge, shares_cnn], labels=[\"OLS\",\"Ridge\",\"CNN\"], showmeans=True)\n",
    "plt.ylabel(\"Star-disc |SHAP| share (fraction of |SHAP| in shell)\")\n",
    "plt.title(\"Global star capture across explained test subset\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.boxplot([enrich_ols, enrich_ridge, enrich_cnn], labels=[\"OLS\",\"Ridge\",\"CNN\"], showmeans=True)\n",
    "plt.ylabel(\"Enrichment (|SHAP| share / area fraction of disc)\")\n",
    "plt.title(\"Global enrichment of |SHAP| at the star location\")\n",
    "plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CNN baseline sensitivity (train subset vs mean baseline)\n",
    "# -----------------------------\n",
    "torch.set_grad_enabled(True)\n",
    "bg_train_t = build_baseline(\"train_subset\", m=64)\n",
    "bg_mean_t  = build_baseline(\"mean\", m=1)\n",
    "e_train = shap.DeepExplainer(shap_model, bg_train_t)\n",
    "e_mean  = shap.DeepExplainer(shap_model, bg_mean_t)\n",
    "vals_a = safe_shap_values(e_train, X_test_t)\n",
    "vals_b = safe_shap_values(e_mean, X_test_t)\n",
    "torch.set_grad_enabled(False)\n",
    "A = (vals_a[0] if isinstance(vals_a, list) else vals_a)[:, 0]\n",
    "B = (vals_b[0] if isinstance(vals_b, list) else vals_b)[:, 0]\n",
    "if HAVE_SPEARMAN:\n",
    "    corrs = []\n",
    "    for i in range(A.shape[0]):\n",
    "        a = np.abs(A[i]).ravel(); b = np.abs(B[i]).ravel()\n",
    "        if (np.std(a) < 1e-12) or (np.std(b) < 1e-12):\n",
    "            corrs.append(np.nan)\n",
    "        else:\n",
    "            corrs.append(spearmanr(a, b, nan_policy=\"omit\").correlation)\n",
    "    corrs = np.array(corrs)\n",
    "    print(\"\\n[Baseline sensitivity — CNN]\")\n",
    "    print(f\"  Spearman rank correlation of |SHAP| (train‑baseline vs mean‑baseline): \"\n",
    "          f\"median={np.nanmedian(corrs):.3f}, IQR=({np.nanpercentile(corrs, 25):.3f}, {np.nanpercentile(corrs, 75):.3f})\")\n",
    "else:\n",
    "    print(\"\\n[Baseline sensitivity — CNN]\\n  SciPy not available; install SciPy to report Spearman correlation.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Polar superpixels / regional analysis\n",
    "# ==========================================================\n",
    "print(\"\\n[Polar superpixels / regional analysis]\")\n",
    "NB_RADIAL = 10; NB_THETA = 16\n",
    "rad_edges  = np.linspace(0, R.max() + 1e-6, NB_RADIAL + 1)\n",
    "theta_edges= np.linspace(0, 2 * np.pi, NB_THETA + 1)\n",
    "rad_bin = np.digitize(R, rad_edges) - 1\n",
    "theta_bin = np.digitize(theta, theta_edges) - 1\n",
    "rad_bin[rad_bin == NB_RADIAL] = NB_RADIAL - 1\n",
    "theta_bin[theta_bin == NB_THETA] = NB_THETA - 1\n",
    "\n",
    "def polar_bin_aggregate(abs_shap_map):\n",
    "    out = np.zeros((NB_RADIAL, NB_THETA), dtype=np.float64)\n",
    "    counts = np.zeros_like(out)\n",
    "    for r in range(NB_RADIAL):\n",
    "        for t in range(NB_THETA):\n",
    "            m = (rad_bin == r) & (theta_bin == t)\n",
    "            if np.any(m):\n",
    "                out[r, t] = abs_shap_map[m].mean()\n",
    "                counts[r, t] = m.sum()\n",
    "    return out, counts\n",
    "\n",
    "def polar_summary(stack, model_name):\n",
    "    mats = []\n",
    "    for s in stack:\n",
    "        M, _ = polar_bin_aggregate(np.abs(s))\n",
    "        mats.append(M)\n",
    "    Mmean = np.mean(mats, axis=0)\n",
    "    ring_strength = Mmean.mean(axis=1)\n",
    "    r_idx = int(np.argmax(ring_strength))\n",
    "    r_lo, r_hi = rad_edges[r_idx], rad_edges[r_idx + 1]\n",
    "    row = Mmean[r_idx, :]\n",
    "    cv = float(np.std(row) / (np.mean(row) + 1e-12))\n",
    "\n",
    "    plt.figure(figsize=(6, 3.8))\n",
    "    plt.imshow(Mmean, aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.yticks(np.arange(NB_RADIAL), [f\"{rad_edges[i]:.0f}-{rad_edges[i + 1]:.0f}\" for i in range(NB_RADIAL)])\n",
    "    plt.xticks(np.arange(0, NB_THETA, 4), [f\"{int(360 * t / NB_THETA)}°\" for t in range(0, NB_THETA, 4)])\n",
    "    plt.xlabel(\"Angle (θ)\"); plt.ylabel(\"Radius band (px)\")\n",
    "    plt.title(f\"{model_name} — polar mean |SHAP|\\n(peak ring ~ {r_lo:.0f}–{r_hi:.0f}px; anisotropy CV={cv:.3f})\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    print(f\"{model_name} — peak ring: ~{r_lo:.0f}–{r_hi:.0f} px, anisotropy (CV across angle) = {cv:.3f}\")\n",
    "    return (r_idx, (r_lo, r_hi), cv, Mmean)\n",
    "\n",
    "r_ols, band_ols, cv_ols, M_ols   = polar_summary(shap_ols_imgs,   \"OLS\")\n",
    "r_rid, band_rid, cv_rid, M_rid   = polar_summary(shap_ridge_imgs, \"Ridge\")\n",
    "r_cnn, band_cnn, cv_cnn, M_cnn   = polar_summary(shap_cnn_imgch,  f\"CNN [{BEST_BASELINE}]\")\n",
    "\n",
    "print(\"\\n[Interpretation — polar summaries]\\n\"\n",
    "      \"• Peak ring should sit on the shell radius; CNN often shows sharper peaks on inner/outer edges.\\n\"\n",
    "      \"• Anisotropy (CV) near zero ⇒ isotropic ring; higher CV ⇒ focus on arcs (useful for local defects).\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# CNN regional ablation + effect sizes (optional sanity)\n",
    "# -----------------------------\n",
    "print(\"\\n[CNN regional ablation + effect sizes]\")\n",
    "K_REGIONS = 12\n",
    "flat_scores = M_cnn.ravel()\n",
    "top_idx = np.argsort(-flat_scores)[:K_REGIONS]\n",
    "top_regions = [(i // NB_THETA, i % NB_THETA) for i in top_idx]\n",
    "\n",
    "mask_top = np.zeros((H, W), dtype=bool)\n",
    "for (rr, tt) in top_regions:\n",
    "    mask_top |= ((rad_bin == rr) & (theta_bin == tt))\n",
    "\n",
    "X_cnn_test_4_ablate = X_cnn_test_4.copy()\n",
    "X_cnn_test_4_ablate[:, 0, :, :] = np.where(mask_top[None, :, :], 0.0, X_cnn_test_4_ablate[:, 0, :, :])\n",
    "\n",
    "y_pred_cnn_base = y0_cnn_pred\n",
    "y_pred_cnn_abl  = cnn_predict_y(shap_model, X_cnn_test_4_ablate)\n",
    "\n",
    "r2_cnn_base = r2_score(y_obs_test_sel, y_pred_cnn_base)\n",
    "r2_cnn_abl  = r2_score(y_obs_test_sel, y_pred_cnn_abl)\n",
    "print(f\"  CNN R² (baseline on this subset): {r2_cnn_base:.4f}\")\n",
    "print(f\"  CNN R² after ablating top‑{K_REGIONS} polar regions: {r2_cnn_abl:.4f}\")\n",
    "print(f\"  R² drop: {r2_cnn_base - r2_cnn_abl:.4f} (larger ⇒ those regions matter)\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Augmentation‑invariance sanity check (flips)\n",
    "# -----------------------------\n",
    "print(\"\\n[Augmentation‑invariance sanity check — CNN SHAP under flips]\")\n",
    "def flip_h(arr):  # horizontal flip\n",
    "    return arr[..., :, ::-1]\n",
    "\n",
    "Xt_flip_np = flip_h(X_cnn_test_4.copy()).copy()\n",
    "Xt_flip = torch.from_numpy(Xt_flip_np).to(DEVICE).requires_grad_(True)\n",
    "torch.set_grad_enabled(True)\n",
    "e_best = shap.DeepExplainer(shap_model, build_baseline(BEST_BASELINE, m=(64 if BEST_BASELINE == \"train_subset\" else 1)))\n",
    "vals_flip = safe_shap_values(e_best, Xt_flip)\n",
    "torch.set_grad_enabled(False)\n",
    "sh_flip = vals_flip[0] if isinstance(vals_flip, list) else vals_flip\n",
    "sh_flip_img_unflipped = flip_h(sh_flip[:, 0]).copy()\n",
    "\n",
    "if HAVE_SPEARMAN:\n",
    "    corrs = []\n",
    "    for i in range(N_EXPLAIN):\n",
    "        a = np.abs(shap_cnn_imgch[i]).ravel()\n",
    "        b = np.abs(sh_flip_img_unflipped[i]).ravel()\n",
    "        if np.std(a) < 1e-12 or np.std(b) < 1e-12:\n",
    "            corrs.append(np.nan)\n",
    "        else:\n",
    "            corrs.append(spearmanr(a, b, nan_policy=\"omit\").correlation)\n",
    "    corrs = np.array(corrs)\n",
    "    print(f\"  Flip‑invariance — Spearman(|SHAP|, original vs unflipped‑flip): \"\n",
    "          f\"median={np.nanmedian(corrs):.3f}, IQR=({np.nanpercentile(corrs, 25):.3f}, {np.nanpercentile(corrs, 75):.3f})\")\n",
    "else:\n",
    "    print(\"  (Install SciPy to compute Spearman rank correlations.)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional: Integrated Gradients (IG) for triangulation (no external libs)\n",
    "# -----------------------------\n",
    "print(\"\\n[Integrated Gradients (IG) — optional triangulation]\")\n",
    "def integrated_gradients(model_torch, X_np, baseline_np, steps=32):\n",
    "    was_enabled = torch.is_grad_enabled()\n",
    "    try:\n",
    "        torch.set_grad_enabled(True)\n",
    "        model_torch.eval()\n",
    "        X = torch.from_numpy(np.ascontiguousarray(X_np)).to(DEVICE)\n",
    "        B = torch.from_numpy(np.ascontiguousarray(baseline_np)).to(DEVICE)\n",
    "        attr = torch.zeros_like(X)\n",
    "        for s in range(1, steps + 1):\n",
    "            t = s / steps\n",
    "            Xi = B + t * (X - B)\n",
    "            Xi.requires_grad_(True)\n",
    "            out = model_torch(Xi)\n",
    "            grads = torch.autograd.grad(out.sum(), Xi, retain_graph=False, create_graph=False)[0]\n",
    "            attr += grads\n",
    "        attr = (X - B) * attr / steps\n",
    "        return attr.detach().cpu().numpy()\n",
    "    finally:\n",
    "        torch.set_grad_enabled(was_enabled)\n",
    "\n",
    "X_base_ig = X_cnn_test_4.copy(); X_base_ig[:, 0] = 0.0\n",
    "attr_ig = integrated_gradients(shap_model, X_cnn_test_4, X_base_ig, steps=32)[:, 0]\n",
    "\n",
    "if HAVE_SPEARMAN:\n",
    "    igcorrs = []\n",
    "    for i in range(N_EXPLAIN):\n",
    "        a = np.abs(attr_ig[i]).ravel()\n",
    "        b = np.abs(shap_cnn_imgch[i]).ravel()\n",
    "        if np.std(a) < 1e-12 or np.std(b) < 1e-12:\n",
    "            igcorrs.append(np.nan)\n",
    "        else:\n",
    "            igcorrs.append(spearmanr(a, b, nan_policy=\"omit\").correlation)\n",
    "    igcorrs = np.array(igcorrs)\n",
    "    print(f\"  IG vs Deep SHAP — Spearman(|attr|): median={np.nanmedian(igcorrs):.3f} \"\n",
    "          f\"IQR=({np.nanpercentile(igcorrs, 25):.3f}, {np.nanpercentile(igcorrs, 75):.3f})\")\n",
    "else:\n",
    "    print(\"  (Install SciPy to compute Spearman rank correlations.)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final, conclusions\n",
    "# -----------------------------\n",
    "print(\"\\n=== Conclusions ===\")\n",
    "print(\"• OLS: clean ring; Ridge: more diffused ring; CNN: crisp double band on inner/outer edges \"\n",
    "      \"with local arcs where defects (or the star) dominate.\")\n",
    "print(\"• Deletion curves (AOPC) quantify faithfulness; higher AOPC → better.\")\n",
    "print(\"• Bounding‑box metrics (largest component) — CNN concentrates |SHAP| into **smaller**, more **angularly tight** regions, \"\n",
    "      \"capturing a **larger share** of |SHAP| with less area, especially on the hard dataset.\")\n",
    "print(\"• Baseline matters; this cell auto‑selects one by IoU@k + AOPC. IG triangulation is included as a sanity check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# SHAP interpretations — **hard (star) dataset**\n",
    "\n",
    "This note interprets SHAP outputs for OLS, Ridge, and the CNN on the **“Death‑Star”** dataset where a small, energy‑neutral star on the shell is the dominant *local* driver of the label. Linear baselines remain informative via a mild global anchor; the CNN can localise the star.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) What changes vs the simple dataset?\n",
    "\n",
    "- The star is **local** and can appear **anywhere on the shell**, so a faithful model must be **anisotropic**: short angular arcs should light up.\n",
    "- Energy neutrality prevents a linear model from exploiting the **global mean**; attribution that leaves the shell or spreads uniformly is a red flag.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Core definitions (as used in the simple note)\n",
    "\n",
    "- **Region share / mean / concentration / IoU / Precision / AOPC:** identical to the definitions given earlier:\n",
    "  - $\\mathrm{share}_\\Omega(S)$, $\\mathrm{mean}_\\Omega(S)$, $\\mathrm{conc}_p(S)$,\n",
    "  - $\\mathrm{IoU}_k$, $\\mathrm{Prec}_k$,\n",
    "  - $\\displaystyle \\mathrm{AOPC}=\\int_0^{f_{\\max}}\\big(R^2_0-R^2(f)\\big)\\,\\mathrm{d}f$.\n",
    "- **Star‑disc share & enrichment:**\n",
    "  $$\n",
    "  \\text{share}=\\frac{\\sum_{D\\cap\\text{Shell}}|S|}{\\sum_{\\text{Shell}}|S|},\\qquad\n",
    "  \\text{enrich}=\\frac{\\text{share}}{\\frac{|D\\cap\\text{Shell}|}{|\\text{Shell}|}}.\n",
    "  $$\n",
    "  Here $D$ is a radius‑4 disc centred at a **star proxy** (ring‑weighted bright maximum).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) OLS — interpretation on the star dataset\n",
    "\n",
    "**Numbers**\n",
    "- **Region share:** Hollow **2.37%**, Shell **94.34%**, Outside **3.29%**.\n",
    "- **Per‑pixel mean:** Hollow **2.4e‑5**, Shell **2.11e‑4**, Outside **7e‑6**.\n",
    "- **Concentration:** top‑1% **32.46%**, top‑5% **64.60%**.\n",
    "- **IoU / Precision:** at 1%: IoU **0.022**, Precision **0.982**; at 10%: IoU **0.217**, Precision **0.987**.\n",
    "- **Deletion AOPC:** **0.0545** (modest).\n",
    "\n",
    "**Reading the plots**\n",
    "- **Overlays / global mean:** a very clear ring, much like the simple dataset — OLS stays edge‑driven.  \n",
    "- **Radial profile:** sharp inner‑edge peak with outer‑edge shoulder.\n",
    "\n",
    "**Interpretation**\n",
    "- OLS still behaves plausibly (attribution on the shell), but the deletion curve shows that these pixels explain **less** of the prediction when the star is the true driver. The linear model lacks a mechanism to focus tightly on the small star.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Ridge — interpretation on the star dataset\n",
    "\n",
    "**Numbers**\n",
    "- **Region share:** Hollow **12.43%**, Shell **28.52%**, Outside **59.05%**.\n",
    "- **Per‑pixel mean:** Hollow **5.6e‑5**, Shell **2.9e‑5**, Outside **6.1e‑5**.\n",
    "- **Concentration:** top‑1% **9.62%**, top‑5% **27.02%**.\n",
    "- **IoU / Precision:** at 1%: IoU **0.003**, Precision **0.130**; at 10%: IoU **0.026**, Precision **0.142**.\n",
    "- **Deletion AOPC:** **−0.0115** (negative).\n",
    "\n",
    "**Reading the plots**\n",
    "- **Overlays:** diffuse speckle into the background; weak ring signal.\n",
    "- **Radial profile:** mostly flat; no strong peaks.\n",
    "\n",
    "**Interpretation**\n",
    "- The negative AOPC means removing the “important” pixels **does not** hurt performance — sometimes it even helps. This is a hallmark of **unfaithful** attributions: the model’s predictions are not controlled by the regions that the explanation highlights.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) CNN — interpretation on the star dataset (baseline by fidelity)\n",
    "\n",
    "**Baseline selection:** `train_subset` is chosen by a combined score (IoU@k + AOPC). Other baselines are close; the choice is consistent across runs.\n",
    "\n",
    "**Numbers**\n",
    "- **Bootstrapping (train‑subset backgrounds):** Hollow **0.014±0.001**, Shell **0.982±0.002**, Outside **0.004±0.000**.\n",
    "- **Region share:** Hollow **1.47%**, Shell **98.15%**, Outside **0.37%**.\n",
    "- **Per‑pixel mean:** Hollow **5.1e‑5**, Shell **8.25e‑4**, Outside **3e‑6**.\n",
    "- **Concentration:** top‑1% **37.58%**, top‑5% **69.52%**.\n",
    "- **IoU / Precision:** at 1%: IoU **0.022**, Precision **0.996**; at 10%: IoU **0.219**, Precision **0.994**.\n",
    "- **Deletion AOPC:** **0.2745** (large, and much higher than OLS/Ridge).\n",
    "\n",
    "**Reading the plots**\n",
    "- **Overlays / global mean:** short, high‑contrast **arcs** on the ring, frequently where the star sits.  \n",
    "- **Radial profile:** two pronounced peaks at the inner and outer edges; little mass away from the shell.\n",
    "\n",
    "**Interpretation**\n",
    "- High concentration and very large AOPC together indicate that the CNN’s predictions are driven by a **compact set** of shell pixels — consistent with a **local star**.  \n",
    "- Precision ≈ 1.0 across $k$ confirms that the top attributions rarely leave the shell.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Star‑oriented diagnostics (qualitative on the provided plots)\n",
    "\n",
    "- **Star‑disc share / enrichment boxplots:** the CNN has the **highest** median share and enrichment at the star location; OLS is lower; Ridge is lowest.  \n",
    "  A good outcome on this dataset is **CNN ≫ OLS ≫ Ridge**.  \n",
    "- **Bboxes (largest component, top‑1%):** the CNN’s boxes are **small** and slanted along the ring with high $|{\\rm SHAP}|$ density inside; OLS boxes are thin arcs; Ridge boxes often sit on noise.\n",
    "\n",
    "**What “good” looks like**\n",
    "- High enrichment at the star disc, small boxes, and large AOPC.  \n",
    "**What “bad” looks like**\n",
    "- Enrichment $\\approx 1$ or $<1$ for the CNN, boxes drifting off the shell, or negative AOPC.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Local vs global reading on the star dataset\n",
    "\n",
    "- **Local:** the CNN’s top‑1% pixels form a compact arc aligned with the star (small angular span).  \n",
    "- **Global:** the mean map remains annular — the model knows *where* interesting things live (the shell) but it only **needs** a small part at test time.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Practical pitfalls\n",
    "\n",
    "- **Baseline sensitivity (Deep SHAP):** different baselines shift magnitudes. Use the fidelity‑based picker and report the choice.\n",
    "- **Area bias:** “region share” favours large regions; always pair it with the **per‑pixel mean**.\n",
    "- **IoU at tiny $k$:** will be small for thin structures even when explanations are correct.\n",
    "- **Negative AOPC:** treat it as a red flag for explanation faithfulness.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Takeaways for the **star** dataset\n",
    "\n",
    "- **OLS** remains edge‑based but cannot prioritise the small star strongly (AOPC **0.0545**).  \n",
    "- **Ridge** is unfaithful here (AOPC **−0.0115**).  \n",
    "- **CNN** shows the intended behaviour: attributions concentrate on **short shell arcs**, deletion hurts substantially (AOPC **0.2745**), and enrichment at the star location is highest among the three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "# SHAP interpretations — harder “Death‑Star” dataset (part 2)\n",
    "\n",
    "This note interprets the **local** and **global** SHAP evidence for the harder dataset in which a bright 5‑arm *star* is injected on the metallic shell. It focuses on what each diagnostic is measuring, what the numbers imply for **OLS**, **Ridge**, and the **CNN**, and how to read the results safely.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Quick orientation — local vs global attribution\n",
    "\n",
    "- **Local** views answer *“where, in this specific image, did the model pick up signal?”*  \n",
    "  Examples here: **bounding‑box** metrics over top‑$1\\%$ $|{\\rm SHAP}|$ and the **star‑disc** capture at a tiny disc centred on the detected star.\n",
    "\n",
    "- **Global** views answer *“across many images, what patterns does the model rely on?”*  \n",
    "  Examples here: region **shares** (hollow / shell / outside), **deletion curves** (AOPC), **polar superpixels** (radius–angle maps), and **baseline sensitivity**.\n",
    "\n",
    "The two perspectives should agree: if a model relies on a strictly local driver (the star), local metrics will be tight, and global metrics will report heavy concentration near the star’s radius and angle.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Bounding‑box metrics (top‑$1\\%$ $|{\\rm SHAP}|$ inside the shell)\n",
    "\n",
    "**What is computed?**  \n",
    "For each image:\n",
    "\n",
    "1. Select the top $p=1\\%$ of $|{\\rm SHAP}|$ **within the shell**.  \n",
    "2. Keep the **largest 8‑connected component** (by total weight).  \n",
    "3. Draw the tight axis‑aligned **bounding box** around that component.  \n",
    "4. Report:\n",
    "   - **Area fraction** of the shell covered:  \n",
    "     $$\n",
    "     \\text{AreaFrac} \\;=\\; \\frac{|{\\rm Box}\\cap \\text{Shell}|}{|\\text{Shell}|}\\,.\n",
    "     $$\n",
    "   - **Share of $|{\\rm SHAP}|$** captured by that box (within the shell):  \n",
    "     $$\n",
    "     \\text{Share} \\;=\\; \\frac{\\sum_{(i,j)\\in{\\rm Box}\\cap\\text{Shell}} |s_{ij}|}{\\sum_{(i,j)\\in\\text{Shell}} |s_{ij}|}\\,.\n",
    "     $$\n",
    "   - **Angular coverage** of selected pixels (degrees).  \n",
    "   - **Radial width** normalised by shell thickness.\n",
    "\n",
    "**Why it matters.**  \n",
    "Small **AreaFrac** yet large **Share** means a **local cue** explains a lot of the model’s behaviour. Angular coverage tells whether that cue is a short arc or a long sector; radial width indicates whether the emphasis is on a thin edge vs a broader band.\n",
    "\n",
    "**Your medians (hard dataset).**\n",
    "\n",
    "- **OLS**: Area $0.003$, Share $0.043$, Angle $6.7^\\circ$, Radial width $0.090$.  \n",
    "  **Enrichment of the box** $\\approx 0.043 / 0.003 \\approx 14\\times$. Tight arcs near shell edges dominate the linear model’s credit.\n",
    "- **Ridge**: Area $0.009$, Share $0.069$, Angle $8.9^\\circ$, Radial width $0.226$.  \n",
    "  Enrichment $\\approx 7.7\\times$. Credit is more **diffuse**; radial width is larger, consistent with the model spreading mass beyond crisp edges (and even outside the shell; see §4).\n",
    "- **CNN**: Area $0.011$, Share $0.268$, Angle $11.7^\\circ$, Radial width $0.239$.  \n",
    "  Enrichment $\\approx 24\\times$. The box captures the star **plus** adjacent inner/outer edges, hence wider radial span but still a small angular sector.\n",
    "\n",
    "**How to read this.**  \n",
    "The CNN concentrates a large portion of $|{\\rm SHAP}|$ into **very small boxes**. OLS also forms tight boxes but captures far less $|{\\rm SHAP}|$. Ridge is the most diffuse.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Global star‑capture and enrichment (local cue measured **globally**)\n",
    "\n",
    "**What is measured?**\n",
    "\n",
    "1. **Star centre** $(y_\\*,x_\\*)$ is estimated directly from the *image* via a ring‑weighted, smoothed peak finder.  \n",
    "2. A small **disc** of radius $r=4$ px is placed at $(y_\\*,x_\\*)$:  \n",
    "   $$\n",
    "   D \\;=\\; \\big\\{(i,j) : (i-y_\\*)^2 + (j-x_\\*)^2 \\le r^2\\big\\}.\n",
    "   $$\n",
    "3. **Share inside the disc** (relative to the shell):  \n",
    "   $$\n",
    "   \\text{Share}_D \\;=\\; \\frac{\\sum_{(i,j)\\in D\\cap\\text{Shell}} |s_{ij}|}{\\sum_{(i,j)\\in\\text{Shell}} |s_{ij}|}\\,.\n",
    "   $$\n",
    "4. **Enrichment** compares this share to the **area fraction** of the disc in the shell:  \n",
    "   $$\n",
    "   \\text{Enrich} \\;=\\; \\frac{\\text{Share}_D}{\\,|D\\cap\\text{Shell}|/|\\text{Shell}|\\,}\\,.\n",
    "   $$\n",
    "   For this geometry, the disc occupies only about $1\\%$ of the shell, so an enrichment of $25$ means “about $25\\times$ more $|{\\rm SHAP}|$ than random area would explain.”\n",
    "5. **Angular alignment** compares the star’s angle $\\theta_\\*$ with the **circular mean angle** of the largest $|{\\rm SHAP}|$ component:  \n",
    "   $$\n",
    "   \\bar\\theta \\;=\\; \\operatorname{atan2}\\!\\Big(\\sum_k w_k\\sin\\theta_k,\\; \\sum_k w_k\\cos\\theta_k\\Big),\\qquad\n",
    "   \\Delta\\theta \\;=\\; \\big|((\\theta_\\*-\\bar\\theta+\\pi)\\bmod 2\\pi) - \\pi\\big|\\,.\n",
    "   $$\n",
    "\n",
    "**Why it matters.**  \n",
    "This couples **where the true local driver sits** with **where attribution mass sits**. It turns a local property (a tiny star) into a global, comparable score.\n",
    "\n",
    "**Medians (hard dataset).**\n",
    "\n",
    "- **OLS**: Share $0.056$, Enrich $5.19$, Angle error $\\tilde{\\Delta\\theta}=1.01^\\circ$.  \n",
    "  Linear pixels do respond to the star location, but far less than the CNN.\n",
    "- **Ridge**: Share $0.070$, Enrich $6.45$, Angle error $0.53^\\circ$.  \n",
    "  Alignment is precise; mass is limited because attribution is spread outside the shell too.\n",
    "- **CNN**: Share $0.273$, Enrich $25.19$, Angle error $0.48^\\circ$.  \n",
    "  A quarter of all shell‑attribution is packed into a ~1% disc at the star. This is the expected signature of a **local causal driver**.\n",
    "\n",
    "**Important control — when there is *no star*.**  \n",
    "On the simple dataset the disc is just a random place on the ring. The expected share then is near its **area fraction** (≈1%), and enrichment hovers around $1$. Large angle errors are normal because there is no canonical angle. Observing near‑unity enrichment and large angular errors in that case confirms the metric is not spuriously locking onto unrelated structure.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Region shares and polar superpixels (global structure)\n",
    "\n",
    "**Region shares** summarise where $|{\\rm SHAP}|$ lives on average. Denote $s_{ij}$ the SHAP map and regions $\\mathcal{H}$ (hollow), $\\mathcal{S}$ (shell), $\\mathcal{O}$ (outside). The mean **per‑pixel** strength in a region is\n",
    "\n",
    "$$\n",
    "\\mu_{\\mathcal{R}} \\;=\\; \\frac{1}{|\\mathcal{R}|}\\sum_{(i,j)\\in\\mathcal{R}} |s_{ij}| \\,.\n",
    "$$\n",
    "\n",
    "- **OLS**: shell‑centred pattern, as desired for a pixel‑wise linear model of a ring.  \n",
    "- **Ridge**: *outside* dominates (e.g. Outside share $59.05\\%$; per‑pixel mean outside > shell). This indicates sensitivity to background / normalisation rather than shell physics.  \n",
    "- **CNN**: $98.15\\%$ of area‑biased share in the shell; per‑pixel mean in the shell is an order of magnitude above elsewhere.\n",
    "\n",
    "**Polar superpixels** bin $|{\\rm SHAP}|$ by radius and angle. The **CV across angle** in the peak radial band captures **anisotropy**: \n",
    "\n",
    "$$\n",
    "\\mathrm{CV} \\;=\\; \\frac{\\sigma_\\theta}{\\mu_\\theta}\\,.\n",
    "$$\n",
    "\n",
    "- **Peak radius**: OLS at $14$–$21$ px (inner edge); CNN at $28$–$35$ px (towards outer edge); Ridge peaks outside the shell ( $49$–$56$ px ), echoing the region‑share warning.\n",
    "- **Anisotropy**: CNN CV $0.413$ (focused arcs), OLS CV $0.146$, Ridge CV $0.091$ (nearly isotropic, but largely off‑shell). Focused arcs are the expected footprint of a **local** defect (the star).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Deletion curves and AOPC (faithfulness check)\n",
    "\n",
    "Pixels are ranked by $|{\\rm SHAP}|$ on the image channel; the top fraction is zeroed; the model is re‑scored against test labels. Define $R^2(f)$ the score after deleting a fraction $f$; the **Area Over the Perturbation Curve** is\n",
    "\n",
    "$$\n",
    "\\text{AOPC} \\;=\\; \\int_0^{f_{\\max}}\\big(R^2(0)-R^2(f)\\big)\\,df\\,,\n",
    "$$\n",
    "\n",
    "approximated by the trapezoid rule in the plots. Larger AOPC $\\Rightarrow$ faster *drop* in $R^2$ $\\Rightarrow$ higher **fidelity** of the explanations.\n",
    "\n",
    "- **OLS**: AOPC $0.0545$ — some faithfulness; deletion hurts performance moderately.\n",
    "- **Ridge**: AOPC $-0.0115$ — deleting “important” pixels slightly **improves** $R^2$. This is a red flag: attribution seems mis‑placed (consistent with outside‑shell emphasis).\n",
    "- **CNN**: AOPC $0.2745$ — large drop; attributions are highly **causal** for the prediction.\n",
    "\n",
    "**Pitfall.** Deletion is baseline‑dependent. If the replacement value inadvertently regularises the input (e.g. removes background confounds for Ridge), AOPC can be pessimistic or even negative, without implying the SHAP algorithm is wrong.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Baseline sensitivity, flips, and IG triangulation\n",
    "\n",
    "- **Deep SHAP baseline:** median Spearman rank correlation $0.759$ between train‑subset and mean baselines indicates **robust** ordering of pixel importance, but not perfect invariance.  \n",
    "  Baseline choice should be guided by downstream **fidelity** (IoU@k and AOPC), not just convenience.\n",
    "- **Flip‑invariance:** median Spearman $0.787$ (original vs horizontally flipped‑&‑unflipped explanations) shows the CNN’s attributions move **with** the image, as expected from a spatial model.\n",
    "- **IG vs Deep SHAP:** median Spearman $0.551$ on $|{\\rm attr}|$ suggests both methods agree on the main loci of evidence; differences are expected because IG integrates gradients along a path from a baseline, while Deep SHAP uses a background distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Putting local and global evidence together\n",
    "\n",
    "- **Local (star‑disc, boxes).** The CNN allocates roughly **a quarter** of its shell attribution to a **1% disc** centred on the star (enrichment $\\approx 25\\times$) and packs **$26$–$33\\%$** of shell attribution into tiny boxes. This is the canonical footprint of a **strong, local causal driver**. OLS notices the star but gives it much less weight; Ridge is distracted by background structure.\n",
    "- **Global (deletion, region shares, polar maps).** The CNN’s large AOPC and high anisotropy in the correct radial band confirm the local picture globally. Ridge’s negative AOPC and off‑shell peak expose a global mismatch with the task.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Practical reading guide (what “good” looks like)\n",
    "\n",
    "- **Good local + good global:** high disc enrichment ($\\gg 1$), tiny angular error ($<1^\\circ$), small boxes with high $|{\\rm SHAP}|$ share, large AOPC, peak radius on the shell, high anisotropy when a local cue should matter. → **CNN here.**\n",
    "- **Looks ring‑like but shallow:** moderate disc enrichment ($\\approx 5$–$7$), small angular error, modest box share, modest AOPC. → **OLS here.**\n",
    "- **Diffuse or off‑shell:** low precision vs shell, negative/flat AOPC, peak radius outside shell. → **Ridge here.**\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Common pitfalls and how to avoid them\n",
    "\n",
    "1. **Confusing area‑biased with per‑pixel means.** The “region share” can be dominated by area. Always compare with per‑pixel means to spot off‑shell leakage.  \n",
    "2. **Threshold sensitivity.** Changing the top‑$p\\%$ affects boxes and IoU. Stability across a few $p$ values is a good sanity check.  \n",
    "3. **Deletion baseline artefacts.** Zeroing pixels can regularise inputs and artificially help a model (Ridge). Report AOPC alongside a qualitative check of deletion images.  \n",
    "4. **Assuming causality from a single view.** Align local and global evidence before claiming a driver is causal.  \n",
    "5. **Ignoring the background choice for Deep SHAP.** Baselines shift absolute magnitudes. Prefer baselines that maximise **fidelity** on held‑out images.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Condensed numerical summary (medians)\n",
    "\n",
    "- **Bounding boxes (Area / Share / Angle / Radial width):**  \n",
    "  OLS $0.003 / 0.043 / 6.7^\\circ / 0.090$ → $\\sim14\\times$ enrichment.  \n",
    "  Ridge $0.009 / 0.069 / 8.9^\\circ / 0.226$ → $\\sim7.7\\times$.  \n",
    "  CNN $0.011 / 0.268 / 11.7^\\circ / 0.239$ → $\\sim24\\times$.\n",
    "\n",
    "- **Star‑disc (Share / Enrich / Angle error):**  \n",
    "  OLS $0.056 / 5.19 / 1.01^\\circ$; Ridge $0.070 / 6.45 / 0.53^\\circ$; CNN $0.273 / 25.19 / 0.48^\\circ$.\n",
    "\n",
    "- **Deletion (AOPC):** OLS $0.0545$; Ridge $-0.0115$; CNN $0.2745$.\n",
    "\n",
    "- **Polar peaks (radius band; anisotropy CV):**  \n",
    "  OLS $14$–$21$ px; $0.146$. Ridge $49$–$56$ px; $0.091$. CNN $28$–$35$ px; $0.413$.\n",
    "\n",
    "---\n",
    "\n",
    "### Bottom line\n",
    "\n",
    "The CNN’s explanations show a **coherent story** across all lenses: a **local** driver on the shell dominates the prediction. Linear models track ring structure but either **under‑weight** the star (OLS) or **misallocate** credit to the background (Ridge). The combination of **high disc enrichment**, **small angular error**, **large AOPC**, and **on‑shell polar peaks** is precisely the pattern expected when a model has learned to localise and use the star.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
