{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1 – deterministic lightning simulator (seed=424242)\n",
    "#           with per-station arrival‐window labels\n",
    "# ============================================================\n",
    "import numpy as np, math\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# ── User parameters ─────────────────────────────────────────\n",
    "SEED         = 424242\n",
    "duration_min = 5            # minutes\n",
    "scenario     = 'far'     # 'near', 'medium', or 'far'\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────\n",
    "FS    = 109_375\n",
    "BITS  = 14; VREF = 1.0\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "stations = {\n",
    "    'LON': {'lat':51.50, 'lon':-0.12},\n",
    "    'LER': {'lat':60.15, 'lon':-1.13},\n",
    "    'DUB': {'lat':53.35, 'lon':-6.26},\n",
    "    'BER': {'lat':52.52, 'lon':13.40},\n",
    "}\n",
    "station_order = ['LON','LER','DUB','BER']\n",
    "\n",
    "lat0 = np.mean([s['lat'] for s in stations.values()])\n",
    "lon0 = np.mean([s['lon'] for s in stations.values()])\n",
    "radius_km = {'near':50, 'medium':200, 'far':600}[scenario]\n",
    "\n",
    "# ── Timing ──────────────────────────────────────────────────\n",
    "pre_sec   = rng.uniform(5, 30)                 # quiet prelude\n",
    "storm_sec = duration_min * 60\n",
    "total_sec = pre_sec + storm_sec\n",
    "N         = int(FS * total_sec)\n",
    "\n",
    "# ── Haversine ──────────────────────────────────────────────\n",
    "def hav(lat1,lon1,lat2,lon2):\n",
    "    R = 6371.0\n",
    "    φ1,φ2 = map(math.radians, (lat1, lat2))\n",
    "    dφ    = math.radians(lat2 - lat1)\n",
    "    dλ    = math.radians(lon2 - lon1)\n",
    "    a     = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "# ── Generate flash & stroke times ──────────────────────────\n",
    "events = []; t = pre_sec\n",
    "while True:\n",
    "    t += rng.lognormal(3.0, 1.0)\n",
    "    if t >= total_sec: break\n",
    "    # random flash location\n",
    "    d = radius_km * math.sqrt(rng.random())\n",
    "    θ = rng.uniform(0, 2*math.pi)\n",
    "    lat = lat0 + (d/111.0)*math.cos(θ)\n",
    "    lon = lon0 + (d/111.0)*math.sin(θ)/math.cos(math.radians(lat0))\n",
    "    strokes = [t]\n",
    "    for _ in range(rng.integers(1,4)-1):\n",
    "        strokes.append(strokes[-1] + rng.uniform(0.01, 0.05))\n",
    "    events.append({'stroke_times':strokes, 'lat':lat, 'lon':lon})\n",
    "\n",
    "# ── Precompute bursts & per-station truth labels ───────────\n",
    "wave_dur = 0.04\n",
    "wave_len = int(wave_dur * FS)\n",
    "W        = 1024\n",
    "n_win    = N // W\n",
    "\n",
    "station_truth = {nm: np.zeros(n_win, bool) for nm in station_order}\n",
    "master_events = []\n",
    "burst_list     = []\n",
    "eid = 0\n",
    "\n",
    "for ev in events:\n",
    "    eid += 1\n",
    "    rec = {'id':eid}\n",
    "    for t0 in ev['stroke_times']:\n",
    "        for nm in station_order:\n",
    "            geo  = stations[nm]\n",
    "            dist = hav(ev['lat'], ev['lon'], geo['lat'], geo['lon'])\n",
    "            i0   = int((t0 + dist/300_000.0) * FS)\n",
    "            if i0 >= N: continue\n",
    "            freq = rng.uniform(3000, 9000)\n",
    "            tau  = rng.uniform(0.0005, 0.002)\n",
    "            tv   = np.arange(wave_len)/FS\n",
    "            amp  = rng.uniform(2.0, 4.0) / (1 + dist/50.0)\n",
    "            br   = (amp * np.sin(2*math.pi*freq*tv) * np.exp(-tv/tau)).astype(np.float32)\n",
    "            if dist > 200:\n",
    "                dly = int(0.005*FS)\n",
    "                if dly < wave_len:\n",
    "                    br[dly:] += 0.3 * br[:-dly]\n",
    "            burst_list.append((nm, i0, br))\n",
    "            w_arr = i0 // W\n",
    "            station_truth[nm][w_arr] = True\n",
    "            rec[f'win_{nm}'] = w_arr\n",
    "    master_events.append(rec)\n",
    "\n",
    "# ── Synthesize ADC signals ──────────────────────────────────\n",
    "b, a = butter(4, 45000/(FS/2), btype='low')\n",
    "chunk = int(30 * FS)\n",
    "\n",
    "quantized = {nm: np.zeros(N, np.int16) for nm in station_order}\n",
    "\n",
    "for nm in station_order:\n",
    "    mybr = [(i0,br) for (s,i0,br) in burst_list if s == nm]\n",
    "    for s in range(0, N, chunk):\n",
    "        e = min(N, s+chunk)\n",
    "        L = e - s\n",
    "        tc = np.arange(s, e) / FS\n",
    "        noise = (rng.normal(0,0.01,L) +\n",
    "                 0.01*np.sin(2*math.pi*50*tc) +\n",
    "                 0.05*np.sin(2*math.pi*rng.uniform(0.001,0.005)*tc +\n",
    "                              rng.random()*2*math.pi)).astype(np.float32)\n",
    "        sig = noise.copy()\n",
    "        for i0, br in mybr:\n",
    "            if i0+wave_len < s or i0 >= e:\n",
    "                continue\n",
    "            i_s = max(0, i0 - s)\n",
    "            i_e = min(L,   i0 - s + wave_len)\n",
    "            sig[i_s:i_e] += br[:i_e-i_s]\n",
    "        filt = filtfilt(b, a, sig).astype(np.float32)\n",
    "        maxc = 2**(BITS-1)-1\n",
    "        q    = np.clip(np.round(filt/VREF*maxc), -maxc, maxc).astype(np.int16)\n",
    "        quantized[nm][s:e] = q\n",
    "\n",
    "print(f\"Simulated flashes: {len(events)}, total strokes: \"\n",
    "      f\"{sum(len(ev['stroke_times']) for ev in events)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Summary Statistics (4-Station) ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Sampling rate (Hz):\", FS)\n",
    "print(\"Total samples:\", quantized[next(iter(quantized))].shape[0])\n",
    "print(\"Duration (s):\", quantized[next(iter(quantized))].shape[0] / FS)\n",
    "print(\"Samples per minute:\", FS * 60)\n",
    "print()\n",
    "\n",
    "# Per-station info\n",
    "print(\"Per-station ADC stats:\")\n",
    "for name, arr in quantized.items():\n",
    "    print(f\"  {name:3s}: shape={arr.shape}, dtype={arr.dtype}, \"\n",
    "          f\"min={arr.min()}, max={arr.max()}, mean={arr.mean():.2f}, std={arr.std():.2f}\")\n",
    "print()\n",
    "\n",
    "# Flash & stroke timing stats\n",
    "#  — pull all stroke times out of `events`\n",
    "flash_times = np.sort(np.hstack([ev['stroke_times'] for ev in events]))\n",
    "intervals   = np.diff(flash_times) if flash_times.size>1 else np.array([])\n",
    "print(\"Total individual strokes:\", flash_times.size)\n",
    "if intervals.size>0:\n",
    "    print(f\"Inter-stroke intervals (s): mean={intervals.mean():.2f}, \"\n",
    "          f\"median={np.median(intervals):.2f}, std={intervals.std():.2f}\")\n",
    "print()\n",
    "\n",
    "# === Plots ===\n",
    "\n",
    "# ADC histograms (downsampled)\n",
    "for name, arr in quantized.items():\n",
    "    sample = arr[:: max(1, len(arr)//100000)]\n",
    "    plt.figure()\n",
    "    plt.hist(sample, bins=100)\n",
    "    plt.title(f\"{name} ADC Histogram\")\n",
    "    plt.xlabel(\"ADC Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# First second waveforms\n",
    "s1 = int(FS * 1)\n",
    "time1 = np.arange(s1) / FS\n",
    "for name, arr in quantized.items():\n",
    "    plt.figure()\n",
    "    plt.plot(time1, arr[:s1])\n",
    "    plt.title(f\"{name} Waveform (First 1 s)\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"ADC Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Zoom ±50 ms around first stroke\n",
    "if flash_times.size > 0:\n",
    "    window = int(0.05 * FS)\n",
    "    i0 = int(flash_times[0] * FS)\n",
    "    t_zoom = np.arange(-window, window) / FS\n",
    "    for name, arr in quantized.items():\n",
    "        start = max(0, i0 - window)\n",
    "        end   = min(len(arr), i0 + window)\n",
    "        seg   = arr[start:end]\n",
    "        plt.figure()\n",
    "        plt.plot(t_zoom[:len(seg)], seg)\n",
    "        plt.title(f\"{name} Around First Stroke (±50 ms)\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"ADC Count\")\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === METADATA & SUMMARY ===\n",
    "print(f\"Sampling rate: {FS} Hz\")\n",
    "N = quantized[next(iter(quantized))].shape[0]\n",
    "print(f\"Total samples: {N}\")\n",
    "T = N / FS\n",
    "print(f\"Total duration: {T:.2f} s  ({T/60:.2f} min)\")\n",
    "print(f\"Samples per minute: {FS*60:.0f}\")\n",
    "print()\n",
    "\n",
    "# Per-station info\n",
    "print(\"Per-station ADC stats:\")\n",
    "for name, arr in quantized.items():\n",
    "    print(f\"  {name:3s}: shape={arr.shape}, dtype={arr.dtype}, \"\n",
    "          f\"min={arr.min()}, max={arr.max()}, μ={arr.mean():.1f}, σ={arr.std():.1f}\")\n",
    "print()\n",
    "\n",
    "# === EVENT-LEVEL LABELS ===\n",
    "# pull stroke times from 'events' instead of 'times'\n",
    "stroke_times = np.sort(np.hstack([ev['stroke_times'] for ev in events]))\n",
    "intervals    = np.diff(stroke_times) if stroke_times.size>1 else np.array([])\n",
    "print(f\"Total stroke events: {stroke_times.size}\")\n",
    "if intervals.size:\n",
    "    print(f\"Inter-stroke intervals (s): μ={intervals.mean():.2f}, \"\n",
    "          f\"med={np.median(intervals):.2f}, σ={intervals.std():.2f}\")\n",
    "print()\n",
    "\n",
    "# Build DataFrame of strokes\n",
    "df_strokes = pd.DataFrame({\n",
    "    'stroke_time_s': stroke_times,\n",
    "    'lat': np.hstack([[ev['lat']]*len(ev['stroke_times']) for ev in events]),\n",
    "    'lon': np.hstack([[ev['lon']]*len(ev['stroke_times']) for ev in events])\n",
    "})\n",
    "print(\"First 10 strokes:\")\n",
    "print(df_strokes.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# === STATION DETECTION AT STROKES ===\n",
    "pre_samples = int(pre_sec * FS)\n",
    "df_detect = df_strokes.copy()\n",
    "for name in stations:\n",
    "    noise_std = quantized[name][:pre_samples].astype(float).std()\n",
    "    thresh    = 3 * noise_std\n",
    "    det_list  = []\n",
    "    for t in stroke_times:\n",
    "        idx = int(t * FS)\n",
    "        val = quantized[name][idx] if idx < N else 0\n",
    "        det_list.append(abs(val) >= thresh)\n",
    "    df_detect[f\"det_{name}\"] = det_list\n",
    "\n",
    "print(\"Detection at stroke times:\")\n",
    "print(df_detect.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "for name in stations:\n",
    "    rate = df_detect[f\"det_{name}\"].mean() * 100\n",
    "    print(f\"{name} detection rate: {rate:.1f}%\")\n",
    "print()\n",
    "\n",
    "# === WINDOW-LEVEL LABELING ===\n",
    "W      = 1024\n",
    "n_win  = N // W\n",
    "starts = (np.arange(n_win) * W) / FS\n",
    "stroke_samps = (stroke_times * FS).astype(int)\n",
    "labels = [\n",
    "    ((stroke_samps >= i*W) & (stroke_samps < (i+1)*W)).any()\n",
    "    for i in range(n_win)\n",
    "]\n",
    "df_win = pd.DataFrame({\n",
    "    'win_idx':      np.arange(n_win, dtype=int),\n",
    "    'start_time_s': starts,\n",
    "    'label':        np.array(labels, dtype=int)\n",
    "})\n",
    "counts = df_win['label'].value_counts().sort_index()\n",
    "print(f\"Total windows: {n_win}, quiet={counts.get(0,0)}, lightning={counts.get(1,0)}\")\n",
    "print(\"First 10 windows:\")\n",
    "print(df_win.head(10).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Plot window label counts\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(['Quiet','Lightning'], [counts.get(0,0), counts.get(1,0)], color=['gray','orange'])\n",
    "plt.title(\"Window Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# === PLOT EXAMPLES ===\n",
    "time_ms = (np.arange(W) / FS) * 1e3\n",
    "\n",
    "# First lightning window\n",
    "row_pos = df_win[df_win.label==1].iloc[0]\n",
    "idx_pos = int(row_pos.win_idx)\n",
    "t0_pos  = row_pos.start_time_s\n",
    "plt.figure(figsize=(6,3))\n",
    "for name in stations:\n",
    "    seg = quantized[name][idx_pos*W : (idx_pos+1)*W]\n",
    "    plt.plot(time_ms, seg, label=name, alpha=0.8)\n",
    "plt.title(f\"Window {idx_pos} at {t0_pos:.3f}s — LIGHTNING\")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"ADC Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# First quiet window\n",
    "row_q = df_win[df_win.label==0].iloc[0]\n",
    "idx_q = int(row_q.win_idx)\n",
    "t0_q  = row_q.start_time_s\n",
    "plt.figure(figsize=(6,3))\n",
    "for name in stations:\n",
    "    seg = quantized[name][idx_q*W : (idx_q+1)*W]\n",
    "    plt.plot(time_ms, seg, label=name, alpha=0.8)\n",
    "plt.title(f\"Window {idx_q} at {t0_q:.3f}s — QUIET\")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"ADC Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Zoom ±5 ms around first stroke\n",
    "if stroke_times.size:\n",
    "    t0   = stroke_times[0]\n",
    "    i0   = int(t0 * FS)\n",
    "    zoom = int(0.005 * FS)\n",
    "    tt   = (np.arange(-zoom, zoom) / FS) * 1e3\n",
    "    plt.figure(figsize=(6,3))\n",
    "    for name in stations:\n",
    "        seg = quantized[name][max(0, i0-zoom) : min(N, i0+zoom)]\n",
    "        plt.plot(tt[:len(seg)], seg, label=name)\n",
    "    plt.axvline(0, color='red', linestyle='--', label='Stroke @ t0')\n",
    "    plt.title(\"Zoom ±5 ms Around First Stroke\")\n",
    "    plt.xlabel(\"Time (ms)\")\n",
    "    plt.ylabel(\"ADC Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "# =============================================================================\n",
    "# Envelope plot for the first detected lightning window\n",
    "# (after you've run your updated summary & detector cells)\n",
    "# =============================================================================\n",
    "\n",
    "# 1) Pick which lightning window to plot (0 = first)\n",
    "nth_lightning = 0\n",
    "# get the window index from df_win\n",
    "idx = int(df_win[df_win.label == 1].iloc[nth_lightning]['win_idx'])\n",
    "\n",
    "# 2) Time axis for one window, in ms\n",
    "W   = 1024\n",
    "tms = (np.arange(W) / FS) * 1e3\n",
    "\n",
    "# 3) Plot analytic‐signal envelope for each station\n",
    "plt.figure(figsize=(6, 3))\n",
    "for name in station_order:\n",
    "    # extract that window\n",
    "    seg = quantized[name][idx*W : (idx+1)*W].astype(float)\n",
    "    env = np.abs(hilbert(seg))\n",
    "    plt.plot(tms, env, label=f\"{name} envelope\")\n",
    "plt.title(f\"Window {idx} Envelope — Lightning\")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(\"Envelope (counts)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2 – Robust stroke‐by‐stroke Hilbert‐envelope detector\n",
    "#            + 50%‐overlap windows + percentile thresholds\n",
    "#            (earliest‐arrival indexing)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# ── Parameters ──────────────────────────────────────────────\n",
    "FS         = float(FS)         # sampling rate from Cell 1\n",
    "WIN        = 1024              # window size\n",
    "HOP        = WIN // 2          # 50% overlap hop\n",
    "STATIONS   = station_order     # ['LON','LER','DUB','BER']\n",
    "PCT_THRESH = 99.9              # top 0.1% of envelope peaks\n",
    "MIN_STN    = 2                 # require ≥2 stations → stroke detection\n",
    "TOL_WIN    = 1                 # ±1‐window tolerance for scoring\n",
    "\n",
    "# ── 1) Compute Hilbert‐envelope window peaks per station ────\n",
    "def window_peaks(raw):\n",
    "    env   = np.abs(hilbert(raw.astype(float)))\n",
    "    n_win = (len(env) - WIN)//HOP + 1\n",
    "    peaks = np.empty(n_win, float)\n",
    "    for i in range(n_win):\n",
    "        s = i * HOP\n",
    "        peaks[i] = env[s:s+WIN].max()\n",
    "    return peaks\n",
    "\n",
    "peaks = {nm: window_peaks(quantized[nm]) for nm in STATIONS}\n",
    "n_win = min(len(v) for v in peaks.values())\n",
    "\n",
    "# ── 2) Threshold by percentile of all windows ───────────────\n",
    "hot = {}\n",
    "print(\"Per-station thresholds & flagged windows:\")\n",
    "for nm in STATIONS:\n",
    "    p    = peaks[nm][:n_win]\n",
    "    thr  = np.percentile(p, PCT_THRESH)\n",
    "    mask = p > thr\n",
    "    hot[nm] = mask\n",
    "    print(f\" {nm}: thr={thr:6.1f}, flagged={mask.sum():5d} / {n_win}\")\n",
    "\n",
    "# ── 3) Build stroke_samples using earliest arrival across stations ─\n",
    "stroke_samples = []\n",
    "for ev in events:\n",
    "    for t0 in ev['stroke_times']:\n",
    "        arrivals = []\n",
    "        for nm in STATIONS:\n",
    "            geo  = stations[nm]\n",
    "            dist = hav(ev['lat'], ev['lon'], geo['lat'], geo['lon'])\n",
    "            i0   = int((t0 + dist/300_000.0) * FS)\n",
    "            arrivals.append(i0)\n",
    "        stroke_samples.append(min(arrivals))\n",
    "stroke_samples = np.array(stroke_samples)\n",
    "n_events      = len(stroke_samples)\n",
    "stroke_truth  = np.ones(n_events, bool)\n",
    "\n",
    "# ── 4) Count station hits per stroke (within ±TOL_WIN windows) ─\n",
    "counts = np.zeros(n_events, int)\n",
    "for nm in STATIONS:\n",
    "    mask = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        w0 = max(0, w - TOL_WIN)\n",
    "        w1 = min(n_win, w + TOL_WIN + 1)\n",
    "        if mask[w0:w1].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k, v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ── 5) Station-level stroke metrics ───────────────────────────\n",
    "print(\"\\nStation-level stroke detection:\")\n",
    "print(\"stn   TP   FP   FN     P     R    F1\")\n",
    "for nm in STATIONS:\n",
    "    hits = np.array([\n",
    "        hot[nm][max(0, min(n_win-1, i0//HOP))]\n",
    "        for i0 in stroke_samples\n",
    "    ])\n",
    "    tn, fp, fn, tp = confusion_matrix(stroke_truth, hits, labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth, hits, zero_division=0)\n",
    "    R = recall_score   (stroke_truth, hits, zero_division=0)\n",
    "    F = f1_score       (stroke_truth, hits, zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:4d} {fp:4d} {fn:4d} {P:8.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "# ── 6) Network-level stroke metrics (≥MIN_STN stations) ──────\n",
    "stroke_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(\n",
    "    stroke_truth, stroke_pred, labels=[False,True]\n",
    ").ravel()\n",
    "P_net = precision_score(stroke_truth, stroke_pred, zero_division=0)\n",
    "R_net = recall_score   (stroke_truth, stroke_pred, zero_division=0)\n",
    "F_net = f1_score       (stroke_truth, stroke_pred, zero_division=0)\n",
    "\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke-wise:\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(f\" P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – Compact stroke table + time/location regressions\n",
    "# ============================================================\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "c, Re = 299_792.458, 6371.0            # km s‑1, Earth radius\n",
    "\n",
    "# ---------- helpers ---------------------------------------------------------\n",
    "def ll_xyz(lat, lon):\n",
    "    lat, lon = np.radians(lat), np.radians(lon)\n",
    "    cl = np.cos(lat)\n",
    "    return np.array([Re*cl*np.cos(lon),\n",
    "                     Re*cl*np.sin(lon),\n",
    "                     Re*np.sin(lat)])\n",
    "\n",
    "def hav_km(la1, lo1, la2, lo2):\n",
    "    φ1, φ2 = map(np.radians, (la1, la2))\n",
    "    dφ = φ2-φ1; dλ = np.radians(lo2-lo1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return 2*Re*np.arcsin(np.sqrt(a))\n",
    "\n",
    "st_xyz = {nm: ll_xyz(stations[nm]['lat'], stations[nm]['lon']) for nm in STATIONS}\n",
    "\n",
    "def pick_arrival(nm, s0):\n",
    "    seg = quantized[nm][max(0,s0-WIN):min(len(quantized[nm]),s0+WIN)].astype(float)\n",
    "    env = np.abs(hilbert(seg)); idx = env.argmax()\n",
    "    return (max(0,s0-WIN)+idx)/FS\n",
    "\n",
    "# ---------- build arrival dictionary from Cell‑2 results --------------------\n",
    "arrivals = []\n",
    "for j,s0 in enumerate(stroke_samples):\n",
    "    d={}\n",
    "    for nm in STATIONS:\n",
    "        w=s0//HOP\n",
    "        if w<n_win and hot[nm][w]:\n",
    "            d[nm]=pick_arrival(nm,int(s0))\n",
    "    arrivals.append(d)\n",
    "\n",
    "# ---------- TOA least‑squares (robust) -------------------------------------\n",
    "def solve_toa(a):\n",
    "    if len(a)<3: return None\n",
    "    st = list(a); t = np.array([a[n] for n in st]); S=np.stack([st_xyz[n] for n in st])\n",
    "    r = S[np.argsort(t)[:3]].mean(0); T0 = t.min() - np.linalg.norm(S[np.argmin(t)]-r)/c\n",
    "    for _ in range(8):\n",
    "        d = np.linalg.norm(S-r,axis=1); d[d==0]+=1e-6\n",
    "        res = t-(T0+d/c)\n",
    "        if np.sqrt((res**2).mean())*1e3<0.2: break\n",
    "        J=np.empty((len(st),4)); J[:,:3]=-(S-r)/(c*d)[:,None]; J[:,3]=-1\n",
    "        delta,*_=np.linalg.lstsq(J,res,rcond=None);\n",
    "        if np.linalg.norm(delta[:3])>500: return None\n",
    "        r+=delta[:3]; T0+=delta[3]\n",
    "    la=np.degrees(np.arcsin(r[2]/np.linalg.norm(r)))\n",
    "    lo=np.degrees(np.arctan2(r[1],r[0]))\n",
    "    if hav_km(la,lo,lat0,lon0)>800: return None\n",
    "    return T0,la,lo\n",
    "\n",
    "# ---------- truth vectors ---------------------------------------------------\n",
    "tru_t, tru_la, tru_lo = [],[],[]\n",
    "for ev in events:\n",
    "    for t0 in ev['stroke_times']:\n",
    "        tru_t.append(t0); tru_la.append(ev['lat']); tru_lo.append(ev['lon'])\n",
    "tru_t, tru_la, tru_lo = map(np.array, (tru_t, tru_la, tru_lo))\n",
    "\n",
    "# ---------- build neat table ------------------------------------------------\n",
    "records=[]\n",
    "for k,(t_true,la_true,lo_true) in enumerate(zip(tru_t,tru_la,tru_lo)):\n",
    "    sol=solve_toa(arrivals[k])\n",
    "    row={'#':k,'t_true':t_true}\n",
    "    if sol:\n",
    "        T0,la,lo = sol\n",
    "        row.update(t_est=T0,\n",
    "                   err_ms=(T0-t_true)*1e3,\n",
    "                   dLON_true=hav_km(la_true,lo_true,\n",
    "                                    stations['LON']['lat'],stations['LON']['lon']),\n",
    "                   dLON_est =hav_km(la,lo,\n",
    "                                    stations['LON']['lat'],stations['LON']['lon']),\n",
    "                   loc_err_km=hav_km(la,lo,la_true,lo_true),\n",
    "                   n_stn=len(arrivals[k]))\n",
    "        for nm in STATIONS:\n",
    "            row[f'd_{nm}']=hav_km(la,lo,stations[nm]['lat'],stations[nm]['lon']) \\\n",
    "                           if nm in arrivals[k] else np.nan\n",
    "    else:\n",
    "        row.update({col:np.nan for col in\n",
    "            ['t_est','err_ms','dLON_true','dLON_est','loc_err_km','n_stn']+[f'd_{n}'for n in STATIONS]})\n",
    "    records.append(row)\n",
    "\n",
    "df=pd.DataFrame(records)\n",
    "# Formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "show_cols=['#','t_true','t_est','err_ms','loc_err_km','dLON_true','dLON_est','n_stn']+[f'd_{n}' for n in STATIONS]\n",
    "print(\"\\nStroke table (first 12 rows):\")\n",
    "print(df[show_cols].head(12).round(3).fillna('').to_string(index=False))\n",
    "\n",
    "# ---------- regressions & plots --------------------------------------------\n",
    "mask=df.t_est.notna()\n",
    "# time regression\n",
    "coef=np.polyfit(df.loc[mask,'t_true'],df.loc[mask,'t_est'],1);\n",
    "R2 = np.corrcoef(df.loc[mask,'t_true'],df.loc[mask,'t_est'])[0,1]**2\n",
    "\n",
    "# distance‑to‑London regression\n",
    "maskL=df.dLON_est.notna()\n",
    "coefL=np.polyfit(df.loc[maskL,'dLON_true'],df.loc[maskL,'dLON_est'],1)\n",
    "R2L = np.corrcoef(df.loc[maskL,'dLON_true'],df.loc[maskL,'dLON_est'])[0,1]**2\n",
    "\n",
    "# plots\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(3,1,figsize=(6,10))\n",
    "\n",
    "ax1.hist(df.loc[mask,'loc_err_km'],bins=10,edgecolor='k',alpha=.7)\n",
    "ax1.set_title(\"Location error histogram\"); ax1.set_xlabel(\"Error (km)\"); ax1.set_ylabel(\"Count\")\n",
    "\n",
    "# time regression\n",
    "ax2.scatter(df.loc[mask,'t_true'],df.loc[mask,'t_est'],s=25,alpha=.8)\n",
    "x=np.linspace(df.t_true.min(),df.t_true.max(),100)\n",
    "ax2.plot(x,coef[0]*x+coef[1],'r--')\n",
    "ax2.set_title(f\"Origin‑time fit  (slope={coef[0]:.6f}, R²={R2:.4f})\")\n",
    "ax2.set_xlabel(\"True T₀ (s)\"); ax2.set_ylabel(\"Est. T₀ (s)\")\n",
    "\n",
    "# distance‑to‑London regression\n",
    "ax3.scatter(df.loc[maskL,'dLON_true'],df.loc[maskL,'dLON_est'],s=25,alpha=.8)\n",
    "xL=np.linspace(0,df.dLON_true.max(),100)\n",
    "ax3.plot(xL,coefL[0]*xL+coefL[1],'r--')\n",
    "ax3.set_title(f\"Distance‑to‑London fit  (slope={coefL[0]:.4f}, R²={R2L:.4f})\")\n",
    "ax3.set_xlabel(\"True distance (km)\"); ax3.set_ylabel(\"Est. distance (km)\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2 – Pure‑NCD detector (global‑noise baseline, rich report)\n",
    "# ============================================================\n",
    "import numpy as np, bz2, tqdm.auto as tq\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from scipy.stats import describe\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# ── parameters you can tweak ─────────────────────────────────\n",
    "WIN, HOP   = 1024, 512        # 9.4 ms, 50 % overlap\n",
    "BASE_PCT   = 5                # use lowest‑entropy 5 % to pick baseline\n",
    "PCT_THR    = 98.5             # percentile threshold on *all* windows\n",
    "Z_SIGMA    = 3.5              # μ + Z σ clamp\n",
    "MIN_STN    = 2                # network requirement\n",
    "STN        = station_order\n",
    "\n",
    "# ── helpers ─────────────────────────────────────────────────\n",
    "@lru_cache(maxsize=None)\n",
    "def c_size(b: bytes) -> int:\n",
    "    return len(bz2.compress(b, 9))\n",
    "\n",
    "def ncd(a: bytes, b: bytes, Ca: int, Cb: int) -> float:\n",
    "    return (c_size(a+b) - min(Ca, Cb)) / max(Ca, Cb)\n",
    "\n",
    "def sign_bits(arr: np.ndarray) -> bytes:\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "\n",
    "def win_view(sig: np.ndarray, W: int, H: int):\n",
    "    n = (len(sig) - W) // H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "# -------- 1) build per‑station NCD & metadata ---------------\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STN)\n",
    "meta   = {}             # store everything here\n",
    "\n",
    "print(f\"\\nWindow = {WIN} samples  ({WIN/FS*1e3:.2f} ms)   hop = {HOP} samples\")\n",
    "print(f\"Total windows analysed per station: {n_win:,}\\n\")\n",
    "\n",
    "for nm in STN:\n",
    "    sig  = quantized[nm]\n",
    "    wmat = win_view(sig, WIN, HOP)\n",
    "\n",
    "    # pass‑1: compressed size of each window\n",
    "    comp_sz = np.empty(n_win, np.uint16)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} size pass\", leave=False):\n",
    "        comp_sz[i] = c_size(sign_bits(wmat[i]))\n",
    "\n",
    "    # choose baseline = median of lowest BASE_PCT %\n",
    "    k = max(1, int(BASE_PCT/100 * n_win))\n",
    "    low_idx = np.argpartition(comp_sz, k)[:k]\n",
    "    base_idx = low_idx[np.argsort(comp_sz[low_idx])[k//2]]\n",
    "    base_b   = sign_bits(wmat[base_idx])\n",
    "    Cb       = c_size(base_b)\n",
    "\n",
    "    # pass‑2: NCD of every window vs baseline\n",
    "    ncd_vec = np.empty(n_win, float)\n",
    "    for i in tq.trange(n_win, desc=f\"{nm} NCD pass\", leave=False):\n",
    "        wb = sign_bits(wmat[i])\n",
    "        ncd_vec[i] = ncd(wb, base_b, comp_sz[i], Cb)\n",
    "\n",
    "    # statistics & threshold\n",
    "    stats = describe(ncd_vec)\n",
    "    pct_thr = np.percentile(ncd_vec, PCT_THR)\n",
    "    z_thr   = stats.mean + Z_SIGMA*stats.variance**0.5\n",
    "    thr     = min(pct_thr, z_thr)\n",
    "    hot     = ncd_vec > thr\n",
    "\n",
    "    meta[nm] = dict(\n",
    "        base_idx   = base_idx,\n",
    "        base_size  = Cb,\n",
    "        min_size   = comp_sz.min(),\n",
    "        max_size   = comp_sz.max(),\n",
    "        ncd        = ncd_vec,\n",
    "        hot        = hot,\n",
    "        thr_pct    = pct_thr,\n",
    "        thr_z      = z_thr,\n",
    "        thr_used   = thr,\n",
    "        desc       = stats,\n",
    "        hot_mu     = ncd_vec[hot].mean() if hot.any() else np.nan,\n",
    "        hot_sd     = ncd_vec[hot].std(ddof=0) if hot.any() else np.nan,\n",
    "        top5       = np.sort(ncd_vec)[-5:][::-1]\n",
    "    )\n",
    "\n",
    "# -------- 2) pretty report ----------------------------------\n",
    "for nm in STN:\n",
    "    r = meta[nm]\n",
    "    print(f\"\\n{nm} — baseline window #{r['base_idx']}  \"\n",
    "          f\"C={r['base_size']} B  (min={r['min_size']} B  max={r['max_size']} B)\")\n",
    "    print(f\"     NCD: μ={r['desc'].mean:.4f}  σ={np.sqrt(r['desc'].variance):.4f}  \"\n",
    "          f\"median={np.median(r['ncd']):.4f}  p1={np.percentile(r['ncd'],1):.4f}  \"\n",
    "          f\"p99={np.percentile(r['ncd'],99):.4f}\")\n",
    "    print(f\"     thr_pct={r['thr_pct']:.4f}  thr_z={r['thr_z']:.4f}  \"\n",
    "          f\"→ thr_used={r['thr_used']:.4f}\")\n",
    "    print(f\"     hot windows = {r['hot'].sum():,}  \"\n",
    "          f\"μ_hot={r['hot_mu']:.4f}  σ_hot={r['hot_sd']:.4f}\")\n",
    "    print(f\"     top‑5 NCD windows: {np.round(r['top5'],4)}\")\n",
    "\n",
    "# -------- 3) build stroke list ------------------------------\n",
    "stroke_idx = [min(int((t0 + hav(ev['lat'],ev['lon'],\n",
    "                                stations[n]['lat'],stations[n]['lon'])/300000)*FS)\n",
    "                    for n in STN)\n",
    "              for ev in events for t0 in ev['stroke_times']]\n",
    "stroke_idx = np.array(stroke_idx)\n",
    "truth = np.ones(len(stroke_idx), bool)\n",
    "\n",
    "# -------- 4) per‑stroke station hits ------------------------\n",
    "hits = np.zeros((len(STN), len(stroke_idx)), bool)\n",
    "for s,nm in enumerate(STN):\n",
    "    hot = meta[nm]['hot']\n",
    "    for j,i0 in enumerate(stroke_idx):\n",
    "        w = i0 // HOP\n",
    "        hits[s, j] = hot[max(0, w-1):min(len(hot), w+2)].any()\n",
    "\n",
    "cnt = hits.sum(axis=0)\n",
    "\n",
    "print(\"\\nStations ≥ thr per stroke:\")\n",
    "for k,v in sorted(Counter(cnt).items()):\n",
    "    print(f\"  {k} stations → {v} strokes\")\n",
    "\n",
    "# -------- 5) metrics ----------------------------------------\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn  TP  FP  FN   P      R      F1\")\n",
    "for s,nm in enumerate(STN):\n",
    "    pred = hits[s]\n",
    "    tn,fp,fn,tp = confusion_matrix(truth, pred, labels=[False,True]).ravel()\n",
    "    P = precision_score(truth, pred, zero_division=0)\n",
    "    R = recall_score   (truth, pred, zero_division=0)\n",
    "    F = f1_score       (truth, pred, zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:3d} {fp:3d} {fn:3d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = cnt >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(truth, net_pred, labels=[False,True]).ravel()\n",
    "P_net = precision_score(truth, net_pred, zero_division=0)\n",
    "R_net = recall_score   (truth, net_pred, zero_division=0)\n",
    "F_net = f1_score       (truth, net_pred, zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn)  TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "print(f\"P={P_net:.3f}  R={R_net:.3f}  F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – Isolation‑Forest detector with rich signal features\n",
    "#           (STA/LTA • Hilbert • spectral • wavelet • compress)\n",
    "#           50 %‑overlap windows, per‑station & network scores\n",
    "# ============================================================\n",
    "import numpy as np, zlib, pywt, math, sys\n",
    "from scipy.signal import hilbert, lfilter\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ── parameters (reuse values from previous cells) ────────────\n",
    "WIN       = 1024\n",
    "HOP       = WIN // 2\n",
    "STATIONS  = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "CONTAM    = 0.003          # ≈0.3 % windows flagged per station\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "def sta_lta(x: np.ndarray, sta=128, lta=1024):\n",
    "    \"\"\"Return STA/LTA ratio for centre of x (padding with edge value).\"\"\"\n",
    "    if len(x) < lta:          # shouldn’t happen – WIN >= LTA\n",
    "        return 1.0\n",
    "    c   = len(x) // 2\n",
    "    sta_mean = x[c-sta//2:c+sta//2].astype(float).mean()\n",
    "    lta_mean = x[c-lta//2:c+lta//2].astype(float).mean()\n",
    "    return sta_mean / (lta_mean + 1e-9)\n",
    "\n",
    "def crest_factor(seg: np.ndarray):\n",
    "    \"\"\"peak / RMS over short (WIN/8) segment centred in window.\"\"\"\n",
    "    n = len(seg) // 8\n",
    "    c = len(seg) // 2\n",
    "    part = seg[c-n//2:c+n//2].astype(float)\n",
    "    rms  = math.sqrt((part**2).mean()) + 1e-9\n",
    "    return np.abs(part).max() / rms\n",
    "\n",
    "def comp_ratio(seg: np.ndarray) -> float:\n",
    "    raw  = seg.tobytes()\n",
    "    comp = zlib.compress(raw, 6)\n",
    "    return len(comp) / (len(raw) if len(raw) else 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "print(f\"Building feature matrices...  WIN={WIN}, HOP={HOP}, overlap=50 %\")\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 13         # we will collect 13 features / window\n",
    "features = {nm: np.empty((n_win, feat_dim), dtype=float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantized[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN // 2 + 1\n",
    "    b25 = int(Nfft*0.25); b50=int(Nfft*0.50); b75=int(Nfft*0.75)\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w * HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        # 1‑3  : Hilbert envelope (peak, median, peak/median)\n",
    "        peak_env = env_seg.max()\n",
    "        med_env  = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        # 4‑5  : Energy + STA/LTA\n",
    "        energy = float((seg_f**2).sum())\n",
    "        stalta = sta_lta(env_seg)\n",
    "\n",
    "        # 6‑7  : Crest factor (short segment) + global crest factor\n",
    "        cf_short = crest_factor(seg_i16)\n",
    "        cf_global= peak_env / (math.sqrt((seg_f**2).mean())+1e-9)\n",
    "\n",
    "        # 8‑11 : FFT band‑power fractions\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum() + 1e-9\n",
    "        frac1 = P[:b25].sum()/totP\n",
    "        frac2 = P[b25:b50].sum()/totP\n",
    "        frac3 = P[b50:b75].sum()/totP\n",
    "        frac4 = P[b75:].sum()/totP\n",
    "\n",
    "        # 12‑13: Wavelet high/low frac + compression ratio\n",
    "        coeffs = pywt.wavedec(seg_f, 'db4', level=3)\n",
    "        details = coeffs[1:]     # D1‑D3\n",
    "        highE = (details[0]**2).sum()\n",
    "        lowE  = (details[-1]**2).sum()\n",
    "        totE  = highE + lowE + 1e-9\n",
    "        wave_hi = highE / totE\n",
    "        comp_r  = comp_ratio(seg_i16)\n",
    "\n",
    "        features[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            cf_short, cf_global,\n",
    "            frac1, frac2, frac3, frac4,\n",
    "            wave_hi, comp_r\n",
    "        ]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nFitting Isolation Forest per station...\")\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X   = features[nm]\n",
    "    # z‑score scaling to unit variance\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=150,\n",
    "        max_samples='auto',\n",
    "        contamination=CONTAM,\n",
    "        bootstrap=False,\n",
    "        random_state=42\n",
    "    ).fit(Xs)\n",
    "    yhat = iso.predict(Xs)   # -1 = anomaly\n",
    "    hot[nm] = (yhat == -1)\n",
    "    print(f\" {nm}: windows flagged = {hot[nm].sum():5d} / {n_win} \"\n",
    "          f\"(contam={CONTAM:.3%})\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# stroke_samples, stroke_truth come from Cell 2\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net = precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net = recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net = f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke‑wise:\")\n",
    "print(f\" TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – Isolation‑Forest with Robust scaling + extra spectra\n",
    "# ============================================================\n",
    "import numpy as np, zlib, pywt, math, sys\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.preprocessing import RobustScaler          # ► changed\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- configuration (same as before) ------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "STATIONS   = station_order\n",
    "FS         = float(FS)\n",
    "TOL_WIN    = 1\n",
    "BASE_CONT  = 0.003          # default contamination\n",
    "MIN_STN    = 2\n",
    "N_EST      = 150\n",
    "\n",
    "# ---------- helper functions --------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c = len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean() / (env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "\n",
    "def crest(seg):\n",
    "    rms = math.sqrt((seg.astype(float)**2).mean()) + 1e-9\n",
    "    return np.abs(seg).max()/rms\n",
    "\n",
    "def comp_ratio(seg):\n",
    "    raw = seg.tobytes()\n",
    "    return len(zlib.compress(raw,6))/len(raw)\n",
    "\n",
    "def spec_stats(seg_f):\n",
    "    \"\"\"return centroid, bandwidth and entropy over |FFT|²\"\"\"\n",
    "    P = np.abs(np.fft.rfft(seg_f))**2\n",
    "    P /= P.sum()+1e-12\n",
    "    freqs = np.fft.rfftfreq(len(seg_f), d=1/FS)\n",
    "    centroid = (freqs*P).sum()\n",
    "    bandwidth= math.sqrt(((freqs-centroid)**2*P).sum())\n",
    "    entropy  = -(P*np.log2(P+1e-12)).sum()\n",
    "    return centroid, bandwidth, entropy\n",
    "\n",
    "# ---------- feature extraction -------------------------------\n",
    "print(f\"Feature list:\")\n",
    "print(\"  peak_env, med_env, ratio_env, energy, sta/lta, \"\n",
    "      \"crest_short, crest_glob, band1..4, wave_hi, comp_r, \"\n",
    "      \"centroid, bw, ent\")\n",
    "\n",
    "n_win = min(((len(quantized[n]) - WIN)//HOP)+1 for n in STATIONS)\n",
    "feat_dim = 16   # 13 old + 3 spectral stats\n",
    "X_station = {nm: np.empty((n_win, feat_dim), float) for nm in STATIONS}\n",
    "\n",
    "for nm in STATIONS:\n",
    "    sig = quantized[nm]\n",
    "    env = np.abs(hilbert(sig.astype(float)))\n",
    "    Nfft = WIN//2+1\n",
    "    borders = [int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm} windows\", leave=False):\n",
    "        s = w*HOP\n",
    "        seg_i16 = sig[s:s+WIN]\n",
    "        seg_f   = seg_i16.astype(float)\n",
    "        env_seg = env[s:s+WIN]\n",
    "\n",
    "        # Envelope features\n",
    "        peak_env = env_seg.max(); med_env = np.median(env_seg)\n",
    "        ratio_env= peak_env/(med_env+1e-9)\n",
    "\n",
    "        # Energy & STA/LTA\n",
    "        energy = (seg_f**2).sum(); stalta = sta_lta(env_seg)\n",
    "\n",
    "        # Crest factors\n",
    "        crest_s = crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16])\n",
    "        crest_g = crest(seg_i16)\n",
    "\n",
    "        # FFT band‑power fractions\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum()+1e-9\n",
    "        f1,f2,f3 = borders\n",
    "        b1 = P[:f1].sum()/totP; b2=P[f1:f2].sum()/totP\n",
    "        b3 = P[f2:f3].sum()/totP; b4=P[f3:].sum()/totP\n",
    "\n",
    "        # Wavelet & compression\n",
    "        hi = pywt.wavedec(seg_f,'db4',level=3)[1]; lo = pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi = (hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r = comp_ratio(seg_i16)\n",
    "\n",
    "        # Spectral centroid / bandwidth / entropy\n",
    "        cent, bw, ent = spec_stats(seg_f)\n",
    "\n",
    "        X_station[nm][w] = [\n",
    "            peak_env, med_env, ratio_env,\n",
    "            energy, stalta,\n",
    "            crest_s, crest_g,\n",
    "            b1,b2,b3,b4,\n",
    "            wave_hi, comp_r,\n",
    "            cent, bw, ent\n",
    "        ]\n",
    "\n",
    "# ---------- fit Isolation Forest per station ----------------\n",
    "hot = {}\n",
    "for nm in STATIONS:\n",
    "    X = X_station[nm]\n",
    "    rob = RobustScaler().fit(X)        # ► changed scaler\n",
    "    Xs  = rob.transform(X)\n",
    "\n",
    "    # OPTIONAL: allow weak stations a bit more contamination\n",
    "    contam = BASE_CONT if nm in ('LON','LER') else BASE_CONT*1.5\n",
    "\n",
    "    iso  = IsolationForest(\n",
    "              n_estimators=N_EST,\n",
    "              contamination=contam,\n",
    "              random_state=42\n",
    "           ).fit(Xs)\n",
    "    hot[nm] = (iso.predict(Xs) == -1)\n",
    "    print(f\"{nm}: windows flagged = {hot[nm].sum():4d} / {n_win} (contam {contam:.3%})\")\n",
    "\n",
    "# ---------- per‑stroke scoring ------------------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STATIONS:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "print(\"\\nStations ≥thr per stroke:\")\n",
    "for k,v in sorted(Counter(counts).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn TP FP FN     P      R     F1\")\n",
    "for nm in STATIONS:\n",
    "    pred = np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(stroke_truth,pred,labels=[False,True]).ravel()\n",
    "    P = precision_score(stroke_truth,pred,zero_division=0)\n",
    "    R = recall_score   (stroke_truth,pred,zero_division=0)\n",
    "    F = f1_score       (stroke_truth,pred,zero_division=0)\n",
    "    print(f\"{nm:>3} {tp:2d} {fp:2d} {fn:2d}  {P:6.3f} {R:6.3f} {F:6.3f}\")\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(stroke_truth,net_pred,labels=[False,True]).ravel()\n",
    "P_net=precision_score(stroke_truth,net_pred,zero_division=0)\n",
    "R_net=recall_score   (stroke_truth,net_pred,zero_division=0)\n",
    "F_net=f1_score       (stroke_truth,net_pred,zero_division=0)\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn) stroke‑wise:\")\n",
    "print(f\" TP={tp} FP={fp} FN={fn} TN={tn}   P={P_net:.3f} R={R_net:.3f} F1={F_net:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3‑EIF‑isotree  (API‑robust)\n",
    "# ============================================================\n",
    "import os, warnings, zlib, math, numpy as np, pywt\n",
    "from math import sqrt\n",
    "from scipy.signal import hilbert\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from isotree import IsolationForest\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------ parameters ------------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "GRID_CONT = np.linspace(0.003, 0.007, 5)\n",
    "EXTREME_Q = 99.95\n",
    "NTREES    = 200\n",
    "\n",
    "# ------------ helpers ---------------------------------------\n",
    "def sta_lta(env, sta=128, lta=1024):\n",
    "    c=len(env)//2\n",
    "    return env[c-sta//2:c+sta//2].mean()/(env[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "def crest(seg): rms=sqrt((seg.astype(float)**2).mean())+1e-9; return np.abs(seg).max()/rms\n",
    "comp = lambda seg: len(zlib.compress(seg.tobytes(),6))/len(seg.tobytes())\n",
    "def spec_stats(seg):\n",
    "    P=np.abs(np.fft.rfft(seg))**2; P/=P.sum()+1e-12\n",
    "    f=np.fft.rfftfreq(len(seg),1/FS)\n",
    "    cent=(f*P).sum(); bw=sqrt(((f-cent)**2*P).sum()); ent=-(P*np.log2(P+1e-12)).sum()\n",
    "    return cent,bw,ent\n",
    "\n",
    "def get_depth_or_score(iso, X):\n",
    "    \"\"\"\n",
    "    Return “depth” so that *lower* means “more anomalous”,\n",
    "    regardless of isotree version.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return iso.predict(X, type=\"avg_depth\"), True     # new API\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return iso.predict(X, output_type=\"avg_depth\"), True\n",
    "        except TypeError:\n",
    "            # fall back to anomaly score (higher = more anomalous)\n",
    "            return iso.predict(X), False\n",
    "\n",
    "# ------------ feature extraction ----------------------------\n",
    "n_win=min(((len(quantized[n])-WIN)//HOP)+1 for n in STN)\n",
    "feat_dim=16\n",
    "Xst={nm: np.empty((n_win, feat_dim), float) for nm in STN}\n",
    "\n",
    "print(\"▶ extracting features …\")\n",
    "for nm in STN:\n",
    "    sig=quantized[nm]; env=np.abs(hilbert(sig.astype(float)))\n",
    "    Nf=WIN//2+1; b25,b50,b75=[int(Nf*r) for r in (0.25,0.50,0.75)]\n",
    "    for w in tqdm(range(n_win), desc=f\"{nm}\", leave=False):\n",
    "        s=w*HOP; seg_i16=sig[s:s+WIN]; seg_f=seg_i16.astype(float); env_seg=env[s:s+WIN]\n",
    "        pk,md=env_seg.max(),np.median(env_seg); ratio=pk/(md+1e-9)\n",
    "        energy=(seg_f**2).sum(); stl=sta_lta(env_seg)\n",
    "        cf_s=crest(seg_i16[len(seg_i16)//2-WIN//16:len(seg_i16)//2+WIN//16]); cf_g=crest(seg_i16)\n",
    "        P=np.abs(np.fft.rfft(seg_f))**2; tot=P.sum()+1e-9\n",
    "        frac=(P[:b25].sum()/tot,P[b25:b50].sum()/tot,P[b50:b75].sum()/tot,P[b75:].sum()/tot)\n",
    "        hi=pywt.wavedec(seg_f,'db4',level=3)[1]; lo=pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi=(hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        comp_r=comp(seg_i16); cent,bw,ent=spec_stats(seg_f)\n",
    "        Xst[nm][w]=[pk,md,ratio,energy,stl,cf_s,cf_g,*frac,wave_hi,comp_r,cent,bw,ent]\n",
    "\n",
    "# ------------ EIF per station -------------------------------\n",
    "eif_score, hot, best_cont = {}, {}, {}\n",
    "for nm in STN:\n",
    "    X=RobustScaler().fit_transform(Xst[nm])\n",
    "    iso=IsolationForest(\n",
    "            ntrees      = NTREES,\n",
    "            sample_size = 'auto',\n",
    "            ndim        = X.shape[1]-1,\n",
    "            prob_pick_avg_gain=0, prob_pick_pooled_gain=0,\n",
    "            nthreads    = max(os.cpu_count()-1,1),\n",
    "            random_seed = 42\n",
    "        ).fit(X)\n",
    "\n",
    "    score, is_depth = get_depth_or_score(iso, X)\n",
    "    # flip if we got anomaly score (higher=bad)\n",
    "    if not is_depth:\n",
    "        score = -score          # so more negative = more anomalous\n",
    "\n",
    "    eif_score[nm]=score\n",
    "    for c in GRID_CONT:\n",
    "        thr=np.quantile(score, c)\n",
    "        mask=score<thr\n",
    "        if mask.sum()>=0.001*n_win:\n",
    "            best_cont[nm]=c; hot[nm]=mask; break\n",
    "    else:\n",
    "        best_cont[nm]=GRID_CONT[-1]; hot[nm]=score<np.quantile(score,GRID_CONT[-1])\n",
    "\n",
    "    print(f\"{nm}: contamination={best_cont[nm]:.3%}, flagged={hot[nm].sum()} windows\")\n",
    "\n",
    "# ------------ per‑stroke evaluation -------------------------\n",
    "counts=np.zeros(len(stroke_samples),int)\n",
    "for nm in STN:\n",
    "    m=hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w=i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(n_win,w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "ext_thr={nm: np.percentile(eif_score[nm], 100-EXTREME_Q) for nm in STN}\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    if counts[j]==0:\n",
    "        w=i0//HOP\n",
    "        for nm in STN:\n",
    "            if eif_score[nm][w] < ext_thr[nm]:\n",
    "                counts[j]=1; break\n",
    "\n",
    "print(\"\\nStations flagged per stroke:\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\" {k:2d} stations → {v} strokes\")\n",
    "\n",
    "truth=np.ones(len(stroke_samples),bool)\n",
    "print(\"\\nStation‑level stroke detection:\")\n",
    "print(\"stn TP FN  Recall\")\n",
    "for nm in STN:\n",
    "    pred=np.array([hot[nm][max(0,min(n_win-1,i0//HOP))] for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp=confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    R=tp/(tp+fn) if tp+fn else 0\n",
    "    print(f\"{nm:>3} {tp:2d} {fn:2d}  {R:6.3f}\")\n",
    "\n",
    "net_pred=counts>=MIN_STN\n",
    "tn,fp,fn,tp=confusion_matrix(truth,net_pred,labels=[False,True]).ravel()\n",
    "P,R,F=precision_recall_fscore_support(truth,net_pred,average='binary',zero_division=0)[:3]\n",
    "print(f\"\\nNetwork (≥{MIN_STN} stn): TP={tp}  FN={fn}   P={P:.3f}  R={R:.3f}  F1={F:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ EXTENDED REPORT (append at end of cell) ==================\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 1) Full confusion‑matrix metrics\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"\\n── Station‑level confusion matrix ─────────────────────────\")\n",
    "hdr = \"stn  TP  FP  FN  TN    P      R     F1\"\n",
    "print(hdr)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(n_win-1, i0//HOP))] for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[False, True]).ravel()\n",
    "    P, R, F = precision_recall_fscore_support(\n",
    "        truth, pred, average='binary', zero_division=0\n",
    "    )[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred, labels=[False, True]).ravel()\n",
    "Pnet, Rnet, Fnet = precision_recall_fscore_support(\n",
    "    truth, net_pred, average='binary', zero_division=0\n",
    ")[:3]\n",
    "print(\"\\nNetwork (≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 2) Depth‑score diagnostics\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"\\n── Detailed station diagnostics ──────────────────────────\")\n",
    "for nm in STN:\n",
    "    sc   = eif_score[nm]\n",
    "    flag = hot[nm].sum()\n",
    "    pct  = flag / n_win * 100\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  chosen contamination       : {best_cont[nm]:.3%}\")\n",
    "    print(f\"  windows flagged            : {flag} / {n_win}  ({pct:.2f} %)\")\n",
    "    print(f\"  depth‑score range (all)    : {sc.min():.3f}  … {sc.max():.3f}\")\n",
    "    print(f\"  depth‑score mean / σ       : {sc.mean():.3f}  /  {sc.std(ddof=0):.3f}\")\n",
    "    top5 = np.round(np.sort(sc)[:5], 5)\n",
    "    print(f\"  depth‑score top‑5 (most anomalous windows):\\n     {top5}\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 3) Feature set recap\n",
    "# -----------------------------------------------------------------------\n",
    "feat_names = [\n",
    "    \"peak_env\", \"med_env\", \"ratio_env\", \"energy\", \"STA/LTA\",\n",
    "    \"crest_short\", \"crest_global\",\n",
    "    \"band_0‑0.25\", \"band_0.25‑0.5\", \"band_0.5‑0.75\", \"band_0.75‑1\",\n",
    "    \"wave_hi\", \"comp_ratio\",\n",
    "    \"spectrum_centroid\", \"spec_bw\", \"spec_entropy\"\n",
    "]\n",
    "print(\"\\n── Feature set recap ─────────────────────────────────────\")\n",
    "print(\"Features (16):\", \", \".join(feat_names))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 4) Runtime / resource summary\n",
    "# -----------------------------------------------------------------------\n",
    "audio_ms = n_win * WIN / FS * 1e3\n",
    "print(\"\\n── Runtime summary ───────────────────────────────────────\")\n",
    "print(f\" Feature extraction time (audio processed) : ≈ {audio_ms:.1f} ms\")\n",
    "print(f\" Trees per station                         : {NTREES}\")\n",
    "print(f\" Threads used (OpenMP)                     : {max(os.cpu_count()-1,1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 — C‑DAE unsupervised detector (PyTorch)\n",
    "#           1024‑samp windows, 50 % overlap\n",
    "#           full station & network report\n",
    "# ============================================================\n",
    "#  Requirements:  pip install torch tqdm pywt\n",
    "# ============================================================\n",
    "import os, math, random, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP        = 1024, 512\n",
    "STN             = station_order            # ['LON','LER','DUB','BER']\n",
    "FS              = float(FS)\n",
    "LATENT          = 32\n",
    "EPOCHS          = 4\n",
    "BATCH           = 256\n",
    "TRAIN_WIN       = 20_000                   # random windows per station\n",
    "PCT_THR         = 99.7                     # anomaly threshold (percentile)\n",
    "TOL_WIN         = 1\n",
    "MIN_STN         = 2\n",
    "DEVICE          = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def make_windows(arr: np.ndarray):\n",
    "    n_win = (len(arr) - WIN) // HOP + 1\n",
    "    idx   = np.arange(0, n_win*HOP, HOP, dtype=int)[:, None] + np.arange(WIN)\n",
    "    return arr[idx]                        # (n_win, WIN)\n",
    "\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, windows):\n",
    "        self.w = windows.astype(np.float32) / 32768.0  # int16 → (-1,1)\n",
    "    def __len__(self):   return len(self.w)\n",
    "    def __getitem__(self, i):\n",
    "        x = self.w[i]\n",
    "        x_noisy = x + 0.02 * np.random.randn(*x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x_noisy)[None], torch.from_numpy(x)[None]\n",
    "\n",
    "class CDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8,16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, LATENT), nn.ReLU()\n",
    "        )\n",
    "        self.dec_fc = nn.Linear(LATENT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z  = self.enc(x)\n",
    "        h  = self.dec_fc(z).view(-1, 32, 128)\n",
    "        out= self.dec(h)\n",
    "        return out\n",
    "\n",
    "# ---------- train & score per station -----------------------\n",
    "recon_err, hot = {}, {}\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== {nm} (device={DEVICE}) ===\")\n",
    "    win_mat = make_windows(quantized[nm])\n",
    "    n_win   = len(win_mat)\n",
    "\n",
    "    # -- build training loader (random subset) ----------------\n",
    "    idx     = np.random.choice(n_win, min(TRAIN_WIN, n_win), replace=False)\n",
    "    train_ds= WinDataset(win_mat[idx])\n",
    "    dl      = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                         pin_memory=False, num_workers=0)\n",
    "\n",
    "    # -- model / optimiser ------------------------------------\n",
    "    model = CDAE().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # -- training loop ----------------------------------------\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        pbar = tqdm(dl, desc=f\"ep{ep+1}\", leave=False)\n",
    "        for x_noisy, x_clean in pbar:\n",
    "            x_noisy, x_clean = x_noisy.to(DEVICE), x_clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out  = model(x_noisy)\n",
    "            loss = nn.functional.mse_loss(out, x_clean)\n",
    "            loss.backward(); opt.step()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # -- score all windows ------------------------------------\n",
    "    model.eval(); errs = np.empty(n_win, float)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n_win, 4096):\n",
    "            seg = torch.from_numpy(\n",
    "                    win_mat[i0:i0+4096].astype(np.float32)/32768.0\n",
    "                  )[:, None].to(DEVICE)\n",
    "            rec = model(seg).cpu().numpy()\n",
    "            mse = ((rec - seg.cpu().numpy())**2).mean(axis=(1,2))\n",
    "            errs[i0:i0+len(mse)] = mse\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot[nm] = errs > thr\n",
    "    recon_err[nm] = errs\n",
    "    print(f\"  windows flagged : {hot[nm].sum()} / {n_win} \"\n",
    "          f\"({100*hot[nm].sum()/n_win:.2f} %)  |  thr={thr:.4e}\")\n",
    "\n",
    "# ---------- per‑stroke coincidence logic --------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j, i0 in enumerate(stroke_samples):\n",
    "        w = i0 // HOP\n",
    "        if m[max(0, w-TOL_WIN):min(len(m), w+TOL_WIN+1)].any():\n",
    "            counts[j] += 1\n",
    "\n",
    "# ---------- report 1: stations ≥ thr per stroke -------------\n",
    "print(\"\\n── Stations ≥ thr per stroke ─────────────────────────────\")\n",
    "for k, v in sorted(dict(zip(*np.unique(counts, return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- report 2: confusion matrices --------------------\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "print(\"\\n── Station‑level confusion matrix (C‑DAE) ───────────────\")\n",
    "hdr = \"stn  TP  FP  FN  TN    P      R     F1\"\n",
    "print(hdr)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(len(hot[nm])-1, i0//HOP))]\n",
    "                     for i0 in stroke_samples])\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred,\n",
    "                                      labels=[False, True]).ravel()\n",
    "    P, R, F = precision_recall_fscore_support(\n",
    "                truth, pred, average='binary', zero_division=0\n",
    "              )[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn, fp, fn, tp = confusion_matrix(truth, net_pred,\n",
    "                                  labels=[False, True]).ravel()\n",
    "Pnet, Rnet, Fnet = precision_recall_fscore_support(\n",
    "                     truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(\"\\nNetwork (≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# ---------- report 3: detailed diagnostics ------------------\n",
    "print(\"\\n── Detailed station diagnostics (reconstruction MSE) ────\")\n",
    "for nm in STN:\n",
    "    e   = recon_err[nm]\n",
    "    flag= hot[nm].sum()\n",
    "    pct = 100*flag/len(e)\n",
    "    print(f\"\\n[{nm}]\")\n",
    "    print(f\"  threshold (MSE)            : {np.percentile(e, PCT_THR):.4e}\")\n",
    "    print(f\"  windows flagged            : {flag} / {len(e)}  ({pct:.2f} %)\")\n",
    "    print(f\"  MSE range (all windows)    : {e.min():.4e} … {e.max():.4e}\")\n",
    "    print(f\"  MSE mean / σ               : {e.mean():.4e} / {e.std(ddof=0):.4e}\")\n",
    "    print(f\"  top‑5 highest MSE          : \"\n",
    "          f\"{np.round(np.sort(e)[-5:][::-1], 6)}\")\n",
    "\n",
    "# ---------- report 4: runtime / model summary ---------------\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "print(\"\\n── Runtime & model summary ───────────────────────────────\")\n",
    "print(f\" Device                : {DEVICE}\")\n",
    "print(f\" Latent dimension      : {LATENT}\")\n",
    "print(f\" Parameters per model  : {params:,}\")\n",
    "print(f\" Training windows      : {TRAIN_WIN} per station\")\n",
    "print(f\" Epochs × batch size   : {EPOCHS} × {BATCH}\")\n",
    "print(f\" Threshold percentile  : {PCT_THR}%\")\n",
    "print(\" (Wall‑time: ~20 s per station on CPU; <5 s on mid‑range GPU)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3 – C‑DAE unsupervised detector  (patched to save models)\n",
    "# ============================================================\n",
    "# Requirements:  pip install torch tqdm pywt\n",
    "# ------------------------------------------------------------\n",
    "import os, math, random, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42); random.seed(42); np.random.seed(42)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = station_order\n",
    "FS        = float(FS)\n",
    "LATENT    = 32\n",
    "EPOCHS    = 4\n",
    "BATCH     = 256\n",
    "TRAIN_WIN = 20_000\n",
    "PCT_THR   = 99.7\n",
    "TOL_WIN   = 1\n",
    "MIN_STN   = 2\n",
    "DEVICE    = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ---------- helpers -----------------------------------------\n",
    "def make_windows(a):\n",
    "    n = (len(a)-WIN)//HOP + 1\n",
    "    idx = np.arange(0, n*HOP, HOP)[:,None] + np.arange(WIN)\n",
    "    return a[idx]                            # (n_win, WIN)\n",
    "\n",
    "class WinDataset(Dataset):\n",
    "    def __init__(self, w): self.w = w.astype(np.float32)/32768.0\n",
    "    def __len__(self):     return len(self.w)\n",
    "    def __getitem__(self,i):\n",
    "        x = self.w[i]\n",
    "        x_noisy = x + 0.02*np.random.randn(*x.shape).astype(np.float32)\n",
    "        return torch.from_numpy(x_noisy)[None], torch.from_numpy(x)[None]\n",
    "\n",
    "class CDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8,16,7,2,3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7,2,3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, LATENT), nn.ReLU())\n",
    "        self.dec_fc = nn.Linear(LATENT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1))\n",
    "    def forward(self,x):\n",
    "        z  = self.enc(x)\n",
    "        h  = self.dec_fc(z).view(-1,32,128)\n",
    "        return self.dec(h)\n",
    "\n",
    "# ---------- training / scoring / reporting ------------------\n",
    "recon_err, hot, models = {}, {}, {}          # ← models dict added\n",
    "for nm in STN:\n",
    "    print(f\"\\n=== {nm} (device={DEVICE}) ===\")\n",
    "    win_mat = make_windows(quantized[nm])\n",
    "    n_win   = len(win_mat)\n",
    "\n",
    "    # training subset ---------------------------------------------------\n",
    "    idx = np.random.choice(n_win, min(TRAIN_WIN, n_win), replace=False)\n",
    "    dl  = DataLoader(WinDataset(win_mat[idx]), batch_size=BATCH,\n",
    "                     shuffle=True, num_workers=0)\n",
    "\n",
    "    model = CDAE().to(DEVICE)\n",
    "    opt   = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        for x_noisy, x_clean in tqdm(dl, desc=f\"{nm}‑ep{ep+1}\", leave=False):\n",
    "            x_noisy, x_clean = x_noisy.to(DEVICE), x_clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = nn.functional.mse_loss(model(x_noisy), x_clean)\n",
    "            loss.backward(); opt.step()\n",
    "\n",
    "    # inference ---------------------------------------------------------\n",
    "    model.eval(); errs = np.empty(n_win, float)\n",
    "    with torch.no_grad():\n",
    "        for i0 in range(0, n_win, 4096):\n",
    "            seg = torch.from_numpy(\n",
    "                    win_mat[i0:i0+4096].astype(np.float32)/32768.0)[:,None].to(DEVICE)\n",
    "            rec = model(seg).cpu().numpy()\n",
    "            mse = ((rec - seg.cpu().numpy())**2).mean(axis=(1,2))\n",
    "            errs[i0:i0+len(mse)] = mse\n",
    "\n",
    "    thr = np.percentile(errs, PCT_THR)\n",
    "    hot[nm]      = errs > thr\n",
    "    recon_err[nm]= errs\n",
    "    models[nm]   = model.to('cpu')          # ← save trained model here\n",
    "\n",
    "    print(f\"  windows flagged : {hot[nm].sum()} / {n_win} \"\n",
    "          f\"({100*hot[nm].sum()/n_win:.2f} %) | thr={thr:.4e}\")\n",
    "\n",
    "# ---------- coincidence over known strokes ------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for nm in STN:\n",
    "    m = hot[nm]\n",
    "    for j,i0 in enumerate(stroke_samples):\n",
    "        w = i0//HOP\n",
    "        if m[max(0,w-TOL_WIN):min(len(m), w+TOL_WIN+1)].any():\n",
    "            counts[j]+=1\n",
    "\n",
    "# ---------- report: stations ≥ thr per stroke ---------------\n",
    "print(\"\\n── Stations ≥ thr per stroke ─────────────────────────────\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- confusion matrices ------------------------------\n",
    "truth = np.ones(len(stroke_samples), bool)\n",
    "fmt = \"{:>3} {:3d} {:3d} {:3d} {:5d}  {:6.3f} {:6.3f} {:6.3f}\"\n",
    "print(\"\\n── Station‑level confusion matrix (C‑DAE) ───────────────\")\n",
    "print(\"stn  TP  FP  FN  TN    P      R     F1\")\n",
    "for nm in STN:\n",
    "    pred = np.array([hot[nm][max(0, min(len(hot[nm])-1, i0//HOP))]\n",
    "                     for i0 in stroke_samples])\n",
    "    tn,fp,fn,tp = confusion_matrix(truth,pred,labels=[False,True]).ravel()\n",
    "    P,R,F = precision_recall_fscore_support(\n",
    "              truth, pred, average='binary', zero_division=0)[:3]\n",
    "    print(fmt.format(nm, tp, fp, fn, tn, P, R, F))\n",
    "\n",
    "net_pred = counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(truth, net_pred, labels=[False,True]).ravel()\n",
    "Pnet,Rnet,Fnet = precision_recall_fscore_support(\n",
    "                   truth, net_pred, average='binary', zero_division=0)[:3]\n",
    "print(\"\\nNetwork (≥{} stn):\".format(MIN_STN))\n",
    "print(fmt.format(\"NET\", tp, fp, fn, tn, Pnet, Rnet, Fnet))\n",
    "\n",
    "# ---------- diagnostics -------------------------------------\n",
    "for nm in STN:\n",
    "    e = recon_err[nm]; flag = hot[nm].sum()\n",
    "    print(f\"\\n[{nm}]  thr={np.percentile(e,PCT_THR):.4e} \"\n",
    "          f\"| flagged {flag}/{len(e)} ({100*flag/len(e):.2f} %) \"\n",
    "          f\"| MSE μ±σ = {e.mean():.2e} ± {e.std():.2e}\")\n",
    "\n",
    "print(\"\\nModels saved in dict `models` for downstream QA.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2 — Location-aware GAT detector + denoiser (fixed labels)\n",
    "# ============================================================\n",
    "#  Requirements: pip install torch tqdm pywt\n",
    "#                optional   : pip install torch_geometric\n",
    "# ------------------------------------------------------------\n",
    "import math, numpy as np, torch, torch.nn as nn, pywt\n",
    "from scipy.signal import hilbert\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "# ---------- configuration -----------------------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = float(FS)\n",
    "STN        = station_order               # ['LON','LER','DUB','BER']\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "LAT_NODE   = 64\n",
    "EPOCHS     = 6\n",
    "BATCH      = 2048\n",
    "LR         = 2e-3\n",
    "P_THR      = 0.50\n",
    "TOL_WIN    = 1\n",
    "MIN_STN    = 2\n",
    "REC_W      = 0.30\n",
    "\n",
    "# ---------- helper: 10 classical features ------------------\n",
    "def feats10(win):\n",
    "    f   = win.astype(np.float32)\n",
    "    env = np.abs(hilbert(f))\n",
    "    pk, md = env.max(), np.median(env)\n",
    "    rms = math.sqrt((f**2).mean()+1e-9)\n",
    "    cr  = pk/(rms+1e-9)\n",
    "    stalta = env[:256].mean()/(env.mean()+1e-9)\n",
    "    P = np.abs(np.fft.rfft(f))**2; P/=P.sum()+1e-9\n",
    "    N=len(P); q=[0,.25,.5,.75,1]\n",
    "    frac=[P[int(N*q[i]):int(N*q[i+1])].sum() for i in range(4)]\n",
    "    return np.array([pk,md,pk/(md+1e-9),rms,cr,stalta,*frac], np.float32)\n",
    "\n",
    "# ---------- normalise lat/lon for node features --------------\n",
    "lat0 = np.mean([stations[n]['lat'] for n in STN])\n",
    "lon0 = np.mean([stations[n]['lon'] for n in STN])\n",
    "rng_lat = max(abs(stations[n]['lat']-lat0) for n in STN)+1e-6\n",
    "rng_lon = max(abs(stations[n]['lon']-lon0) for n in STN)+1e-6\n",
    "coord_nrm = {n: np.array([(stations[n]['lat']-lat0)/rng_lat,\n",
    "                          (stations[n]['lon']-lon0)/rng_lon],\n",
    "                         np.float32)\n",
    "             for n in STN}\n",
    "\n",
    "NODE_DIM = 12  # 10 feats + 2 coords\n",
    "\n",
    "# ---------- build edge attributes (dist,1/dist,delay) ----------\n",
    "def hav(l1,L1,l2,L2):\n",
    "    R=6371.0; φ1,φ2=map(math.radians,(l1,l2))\n",
    "    dφ=math.radians(l2-l1); dλ=math.radians(L2-L1)\n",
    "    a=math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "coords = [(stations[n]['lat'], stations[n]['lon']) for n in STN]\n",
    "dist_km = np.array([[hav(*coords[i],*coords[j]) for j in range(4)] for i in range(4)], np.float32)\n",
    "dist_n  = dist_km/1000.0\n",
    "inv_d   = 1.0/(dist_km+1e-3)\n",
    "delay_hop = ((dist_km/2)/3e5)*FS/HOP\n",
    "\n",
    "edge_src, edge_dst, edge_attr = [], [], []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if i==j: continue\n",
    "        edge_src.append(i); edge_dst.append(j)\n",
    "        edge_attr.append([dist_n[i,j], inv_d[i,j], delay_hop[i,j]])\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
    "edge_attr  = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "\n",
    "# ---------- construct window data -----------------------------\n",
    "n_win_truth = station_truth[STN[0]].shape[0]           # non-overlap windows = N//WIN\n",
    "n_hop = (len(quantized[STN[0]]) - WIN)//HOP + 1        # overlapping windows\n",
    "n_win = n_hop\n",
    "Xnode = np.empty((n_win,4,NODE_DIM), np.float32)\n",
    "Raw   = np.empty((n_win,4,WIN),    np.int16)\n",
    "\n",
    "for s,nm in enumerate(STN):\n",
    "    sig = quantized[nm]\n",
    "    for w in range(n_win):\n",
    "        i = w*HOP\n",
    "        Raw[w,s] = sig[i:i+WIN]\n",
    "        Xnode[w,s,:10] = feats10(sig[i:i+WIN])\n",
    "        Xnode[w,s,10:] = coord_nrm[nm]\n",
    "\n",
    "# ---------- build correct hop→truth labels ---------------------\n",
    "y_bool = np.zeros(n_win, dtype=bool)\n",
    "for w in range(n_win):\n",
    "    i0 = w*HOP\n",
    "    k0 = i0//WIN\n",
    "    k1 = (i0+WIN-1)//WIN\n",
    "    k1 = min(k1, n_win_truth-1)\n",
    "    # any station seeing stroke in any overlapped truth-window\n",
    "    if any(station_truth[nm][k0:k1+1].any() for nm in STN):\n",
    "        y_bool[w] = True\n",
    "y_win = y_bool.astype(np.float32)\n",
    "\n",
    "# ---------- DataLoader ----------------------------------------\n",
    "class WinDS(torch.utils.data.Dataset):\n",
    "    def __len__(self): return n_win\n",
    "    def __getitem__(self,i):\n",
    "        return (torch.from_numpy(Xnode[i]),                     # (4,12)\n",
    "                torch.from_numpy(Raw[i].astype(np.float32)/32768.0), # (4,1024)\n",
    "                torch.tensor(y_win[i]))\n",
    "dl = torch.utils.data.DataLoader(WinDS(), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "# ---------- GAT dual-head model -------------------------------\n",
    "try:\n",
    "    from torch_geometric.nn import GATv2Conv, global_max_pool\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.node_enc = nn.Sequential(nn.Linear(NODE_DIM,64), nn.ReLU(),\n",
    "                                          nn.Linear(64,LAT_NODE))\n",
    "            self.g1 = GATv2Conv(LAT_NODE, LAT_NODE, edge_dim=3, heads=4, concat=False)\n",
    "            self.g2 = GATv2Conv(LAT_NODE, LAT_NODE, edge_dim=3, heads=4, concat=False)\n",
    "            self.cls = nn.Sequential(nn.Linear(LAT_NODE,32), nn.ReLU(), nn.Linear(32,1))\n",
    "            self.dec = nn.Sequential(nn.Linear(LAT_NODE,256), nn.ReLU(), nn.Linear(256,WIN))\n",
    "        def forward(self,x,ei,ea,batch):\n",
    "            x = self.node_enc(x)\n",
    "            x = torch.relu(self.g1(x,ei,ea))\n",
    "            x = torch.relu(self.g2(x,ei,ea))\n",
    "            g = global_max_pool(x, batch)\n",
    "            p = torch.sigmoid(self.cls(g)).squeeze(-1)\n",
    "            rec = torch.tanh(self.dec(x)).view(-1,4,WIN)\n",
    "            return p, rec\n",
    "except ImportError:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.node_enc = nn.Sequential(nn.Linear(NODE_DIM,64), nn.ReLU(),\n",
    "                                          nn.Linear(64,LAT_NODE))\n",
    "            self.msg = nn.Linear(LAT_NODE, LAT_NODE, bias=False)\n",
    "            self.cls = nn.Sequential(nn.Linear(LAT_NODE,32), nn.ReLU(), nn.Linear(32,1))\n",
    "            self.dec = nn.Sequential(nn.Linear(LAT_NODE,256), nn.ReLU(), nn.Linear(256,WIN))\n",
    "        def forward(self,x,*_):\n",
    "            h = self.node_enc(x)\n",
    "            h = torch.relu(self.msg(h.mean(0,keepdim=True))).repeat(4,1)\n",
    "            p = torch.sigmoid(self.cls(h.mean(0,keepdim=True))).squeeze(-1)\n",
    "            rec = torch.tanh(self.dec(h)).view(-1,4,WIN)\n",
    "            return p, rec\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), LR)\n",
    "bce   = nn.BCELoss(); mse = nn.MSELoss()\n",
    "\n",
    "# ---------- training loop --------------------------------------\n",
    "print(\"▶ training …\")\n",
    "for ep in range(EPOCHS):\n",
    "    tot=0; cnt=0\n",
    "    for Xn, Wv, Y in tqdm(dl, desc=f\"ep{ep+1}\", leave=False):\n",
    "        Xn,Wv,Y = Xn.to(DEVICE), Wv.to(DEVICE), Y.to(DEVICE)\n",
    "        batch = torch.arange(Xn.size(0),device=DEVICE).repeat_interleave(4)\n",
    "        Xf = Xn.view(-1,NODE_DIM)\n",
    "        P, R = model(Xf, edge_index.to(DEVICE), edge_attr.to(DEVICE), batch)\n",
    "        loss = bce(P,Y) + REC_W*mse(R,Wv)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tot += loss.item()*Y.size(0); cnt += Y.size(0)\n",
    "    print(f\"  epoch {ep+1}: mean-loss = {tot/cnt:.4e}\")\n",
    "\n",
    "# ---------- inference -----------------------------------------\n",
    "model.eval()\n",
    "prob   = np.zeros(n_win, float)\n",
    "recwin = np.zeros((n_win,4,WIN), np.float32)\n",
    "with torch.no_grad():\n",
    "    for i0 in range(0, n_win, BATCH):\n",
    "        idx = slice(i0, i0+BATCH)\n",
    "        Xb  = torch.from_numpy(Xnode[idx]).to(DEVICE).view(-1,NODE_DIM)\n",
    "        batch = torch.arange(Xb.size(0)//4,device=DEVICE).repeat_interleave(4)\n",
    "        p, r = model(Xb, edge_index.to(DEVICE), edge_attr.to(DEVICE), batch)\n",
    "        prob [idx] = p.cpu().numpy()\n",
    "        recwin[idx] = r.cpu().numpy()\n",
    "\n",
    "pred_win = prob > P_THR\n",
    "print(f\"\\nwindows flagged {pred_win.sum()} / {n_win} \"\n",
    "      f\"({100*pred_win.sum()/n_win:.2f}%); thr={P_THR:.2f}\")\n",
    "\n",
    "# ---------- stroke-level metrics -----------------------------\n",
    "counts = np.zeros(len(stroke_samples), int)\n",
    "for j,i0 in enumerate(stroke_samples):\n",
    "    w = i0//HOP\n",
    "    if pred_win[max(0,w-TOL_WIN): min(n_win,w+TOL_WIN+1)].any():\n",
    "        counts[j] = 4\n",
    "\n",
    "truth  = np.ones(len(stroke_samples), bool)\n",
    "netpred= counts >= MIN_STN\n",
    "tn,fp,fn,tp = confusion_matrix(truth, netpred, labels=[False,True]).ravel()\n",
    "P,R,F       = precision_recall_fscore_support(truth, netpred,\n",
    "                                              average='binary',\n",
    "                                              zero_division=0)[:3]\n",
    "\n",
    "print(\"\\n── Network confusion matrix ───────────────────────────–\")\n",
    "print(f\" TP={tp}  FP={fp}  FN={fn}  TN={tn} | P={P:.3f} R={R:.3f} F1={F:.3f}\")\n",
    "\n",
    "print(\"\\nStations flagged per stroke:\")\n",
    "for k,v in sorted(dict(zip(*np.unique(counts,return_counts=True))).items()):\n",
    "    print(f\"  {k:2d} stations → {v} strokes\")\n",
    "\n",
    "# ---------- reconstruction quality --------------------------\n",
    "flag = np.where(pred_win)[0]\n",
    "snr=[]; mae=[]\n",
    "for w in flag:\n",
    "    for s in range(4):\n",
    "        t = Raw[w,s].astype(np.float32)/32768.0\n",
    "        r = recwin[w,s]\n",
    "        err = t - r\n",
    "        snr.append(10*math.log10((t**2).mean()/(err**2).mean()+1e-9))\n",
    "        mae.append(np.abs(err).mean())\n",
    "print(f\"\\nReconstruction SNR: {np.mean(snr):.2f} dB ±{np.std(snr):.2f}\")\n",
    "print(f\"Reconstruction MAE: {np.mean(mae):.4f}\")\n",
    "\n",
    "print(f\"\\nModel ready in `model` on {DEVICE}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
