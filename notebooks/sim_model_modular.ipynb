{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1 · Repository skeleton\n",
    "\n",
    "lightstorm/\n",
    "├─ lightstorm/                 # importable package  (pip install -e .)\n",
    "│  ├─ __init__.py\n",
    "│  ├─ config.py                # StormConfig, EvalConfig\n",
    "│  ├─ window_utils.py          # make_windows\n",
    "│  ├─ feature_registry.py\n",
    "│  ├─ features/\n",
    "│  │   ├─ __init__.py\n",
    "│  │   ├─ basic.py             # hilbert_peak, rms_db, ...\n",
    "│  │   ├─ iso13.py\n",
    "│  │   ├─ ae10.py\n",
    "│  │   └─ iso16.py\n",
    "│  ├─ extractor.py             # FeatureExtractor\n",
    "│  ├─ synth/\n",
    "│  │   ├─ __init__.py\n",
    "│  │   └─ storm.py             # StormGenerator + helpers\n",
    "│  ├─ detectors/\n",
    "│  │   ├─ __init__.py\n",
    "│  │   ├─ thresh_hilbert.py    # Hilbert baseline\n",
    "│  │   ├─ ncd.py               # NcdDetector\n",
    "│  │   ├─ iforest.py           # IsoForestModel\n",
    "│  │   ├─ iforest_ext.py       # ExtendedIsoForest\n",
    "│  │   ├─ cdae.py              # per‑station CDAE\n",
    "│  │   └─ graph_cdae.py        # Graph‑CDAE demo\n",
    "│  ├─ evaluation.py            # evaluate_windowed_model + pretty_metrics\n",
    "│  └─ interpret/\n",
    "│      └─ if_importance.py     # permutation‑importance helper\n",
    "├─ notebooks/\n",
    "│  ├─ 00_quick_demo.ipynb      # tiny demo orchestrating the pipeline\n",
    "│  └─ 01_graph_cdae_demo.ipynb # optional GPU walk‑through\n",
    "├─ tests/\n",
    "│  └─ test_feature_shapes.py   # pytest – sanity checks\n",
    "├─ docs/                       # MkDocs or Sphinx source\n",
    "│  └─ index.md\n",
    "├─ pyproject.toml              # build & dependencies\n",
    "└─ README.md\n",
    "Everything already written drops into these modules almost verbatim.\n",
    "\n",
    "2 · Pasting guide (cell → module)\n",
    "Notebook section\tPaste into file\tRequired edits\n",
    "StormConfig / EvalConfig\tlightstorm/config.py\tkeep imports; add __all__\n",
    "make_windows\tlightstorm/window_utils.py\tnone\n",
    "Registry + all feature cells\tseparate files in lightstorm/features/\treplace from ... relative imports\n",
    "FeatureExtractor\tlightstorm/extractor.py\tfrom .feature_registry import ...\n",
    "StormGenerator & helpers\tlightstorm/synth/storm.py\tprefix internal imports (from ..window_utils import make_windows)\n",
    "Hilbert threshold baseline\tlightstorm/detectors/thresh_hilbert.py\taccept StormBundle instead of globals\n",
    "NcdDetector\tlightstorm/detectors/ncd.py\tsame, plus from ..evaluation import evaluate_windowed_model\n",
    "IsoForestModel\tlightstorm/detectors/iforest.py\tchange FeatureExtractor import to relative\n",
    "ExtendedIsoForest\tlightstorm/detectors/iforest_ext.py\tidem\n",
    "CdaeModel\tlightstorm/detectors/cdae.py\timport make_windows from package\n",
    "Graph‑CDAE demo\tlightstorm/detectors/graph_cdae.py\tmove edge_index build into class or helper\n",
    "evaluate_windowed_model + pretty metrics\tlightstorm/evaluation.py\tcreate helper pretty_metrics\n",
    "Permutation‑importance utilities\tlightstorm/interpret/if_importance.py\tpass model instance, no globals\n",
    "\n",
    "3 · Light‑touch code edits\n",
    "Relative imports\n",
    "\n",
    "python\n",
    "Copy\n",
    "from .window_utils import make_windows\n",
    "from .features.iso16 import ISO16_NAMES\n",
    "Global constants → module attributes\n",
    "Move WIN, HOP, FS, … to config.py; import where needed:\n",
    "\n",
    "python\n",
    "Copy\n",
    "from ..config import DEFAULT_WIN as WIN\n",
    "Type annotations – add local imports (from __future__ import annotations)\n",
    "if you hit circular‑reference warnings.\n",
    "\n",
    "Edge‑index creation – wrap in a small function inside\n",
    "graph_cdae.py so demo notebook calls build_fully_connected(S).\n",
    "\n",
    "pretty_metrics – single helper in evaluation.py; every script\n",
    "can from ..evaluation import pretty_metrics.\n",
    "\n",
    "4 · Demo notebook (00_quick_demo.ipynb)\n",
    "Minimal contents:\n",
    "\n",
    "python\n",
    "Copy\n",
    "from lightstorm.config     import StormConfig\n",
    "from lightstorm.synth      import storm\n",
    "from lightstorm.detectors  import thresh_hilbert, iforest_ext\n",
    "from lightstorm.evaluation import evaluate_windowed_model, pretty_metrics\n",
    "\n",
    "# 1) generate synthetic data\n",
    "cfg   = StormConfig()\n",
    "bundle= storm.StormGenerator(cfg).generate()\n",
    "\n",
    "# 2) run detector\n",
    "det   = iforest_ext.ExtendedIsoForest()\n",
    "det.fit(bundle.quantised, fs=cfg.fs)\n",
    "hot   = det.predict(bundle.quantised, fs=cfg.fs)\n",
    "\n",
    "# 3) evaluate\n",
    "station, network, _ = evaluate_windowed_model(\n",
    "    hot=hot,\n",
    "    stroke_records=bundle.stroke_records,\n",
    "    quantized=bundle.quantised,\n",
    "    station_order=list(bundle.quantised),\n",
    "    cfg=None)\n",
    "print(pretty_metrics(network))\n",
    "No more hidden globals.\n",
    "\n",
    "5 · Documentation strategy\n",
    "Sphinx – autodoc + napoleon will pull your expanded docstrings\n",
    "automatically. Point the API section at lightstorm/.\n",
    "\n",
    "MkDocs‑Material – easier for Markdown lovers; use\n",
    "mkdocstrings[python] to render docstrings identically.\n",
    "\n",
    "Add one how‑to page: “Add a new feature function in two lines – see\n",
    "features/basic.py”.\n",
    "\n",
    "6 · GitLab CI / pre‑commit hooks\n",
    "pre‑commit – ruff, black, isort keep style uniform.\n",
    "\n",
    "pytest – lightweight shape tests ensure later refactors don’t\n",
    "break the IO contract. Example in tests/test_feature_shapes.py.\n",
    "\n",
    "GitLab CI – stages: lint, test, build-docs, nb-examples\n",
    "(execute demo notebooks with nbqa).\n",
    "\n",
    "7 · Packaging (pyproject.toml excerpt)\n",
    "toml\n",
    "Copy\n",
    "[project]\n",
    "name = \"lightstorm\"\n",
    "version = \"0.1.0\"\n",
    "dependencies = [\n",
    "    \"numpy\", \"scipy\", \"pandas\", \"scikit-learn\", \"isotree\",\n",
    "    \"torch\", \"torch-geometric\", \"tqdm\", \"matplotlib\"\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "docs = [\"sphinx\", \"furo\", \"mkdocstrings[python]\"]\n",
    "dev  = [\"pytest\", \"ruff\", \"black\", \"isort\", \"nbqa\"]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"setuptools>=64\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "Run:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "pip install -e .[dev,docs]\n",
    "pre-commit install      # enforce style\n",
    "pytest                  # quick unit checks\n",
    "mkdocs serve            # live docs preview\n",
    "8 · Migration checklist\n",
    " Copy/paste code into the indicated modules.\n",
    "\n",
    " Replace absolute imports with relative (from .. or from .).\n",
    "\n",
    " Move hard‑coded constants into config.py.\n",
    "\n",
    " Verify __all__ lists so from lightstorm import * stays clean.\n",
    "\n",
    " Execute 00_quick_demo.ipynb; expect identical metrics to the\n",
    "monolithic notebook.\n",
    "\n",
    " Commit, push, watch GitLab CI turn green.\n",
    "\n",
    "Following these steps yields a professional, pip‑installable, CI‑ready\n",
    "code‑base with minimal churn—largely copy‑paste plus import tweaks—yet\n",
    "sets the stage for documentation, unit tests and future expansion.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# MODULE TARGET : lightning_sim/sim/generator.py\n",
    "# ----------------------------------------------------------------------\n",
    "# This notebook cell refactors the original\n",
    "# “Lightning‑storm generator 2.4 – single‑RNG, fully reproducible”\n",
    "# into a *reusable* StormGenerator class.\n",
    "#\n",
    "# Quick test inside the notebook:\n",
    "# >>> cfg = StormConfig()          # accept defaults or tweak fields\n",
    "# >>> gen = StormGenerator(cfg)\n",
    "# >>> bundle = gen.generate()\n",
    "# >>> bundle.df_wave.head()\n",
    "#\n",
    "# When you migrate to a package:\n",
    "#   1. Move this code into lightning_sim/sim/generator.py\n",
    "#   2. Delete the inline StormConfig definition and instead\n",
    "#      `from lightning_sim.config import StormConfig`\n",
    "#   3. Make sure scipy is in your requirements.\n",
    "# ======================================================================\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "storm_synth.py  — Synthetic lightning‑storm data factory (UK English overview)\n",
    "===============================================================================\n",
    "\n",
    "Purpose of this cell\n",
    "--------------------\n",
    "Provide a **fully reproducible work‑bench** for generating multi‑station\n",
    "lightning data: raw 14‑bit ADC traces plus stroke‑level ground‑truth\n",
    "tables.  The output slots straight into the feature‑extraction toolkit\n",
    "shown earlier and lets you stress‑test detection / localisation\n",
    "algorithms at arbitrary *signal‑to‑noise ratio* (SNR) and *difficulty*\n",
    "levels.\n",
    "\n",
    "High‑level architecture\n",
    "-----------------------\n",
    "1. **`StormConfig`** – frozen `@dataclass` holding every tweakable\n",
    "   knob, from `snr_db` and `difficulty` (1 – 9) to ADC word‑length.\n",
    "   Changing a field creates an independent scenario yet the random\n",
    "   seed guarantees bit‑identical runs across machines.\n",
    "\n",
    "2. **`StormBundle`** – return container:\n",
    "   * `quantised`    → dict of station name ↦ `np.int16` waveform\n",
    "   * `events`       → flash‑level meta (lat/lon, stroke times)\n",
    "   * `stroke_records` → flat list, one row per *station × stroke*\n",
    "   * `df_wave`      → pandas timeline of every sample\n",
    "   * `df_labels`    → tidy DataFrame of stroke meta suitable for\n",
    "     supervised training\n",
    "\n",
    "3. **`StormGenerator`** – does the heavy lifting:\n",
    "   * Hard‑coded **station table** (Iceland to Cyprus) gives realistic\n",
    "     baselines for time‑of‑arrival work.\n",
    "   * `_hav` uses the **haversine** formula to convert lat/lon to km;\n",
    "     good segue into geodesy for the audience.\n",
    "   * `_path_loss` encapsulates an empirically tuned attenuation law\n",
    "     and adds coast‑to‑coast correction > 600 km.\n",
    "   * `_difficulty_flags` translates the integer *difficulty* into a\n",
    "     boolean feature set: sky‑wave, coloured noise, drop‑outs, &c.\n",
    "     Show the if‑ladder to illustrate scalable scenario design.\n",
    "   * `_make_burst` synthesises a 40 ms stroke waveform — choice of\n",
    "     damped sinusoid or “divergent” template, multipath echoes,\n",
    "     sprite rings, and ionospheric low‑pass (sky‑wave).\n",
    "   * `generate()` builds the full scene:\n",
    "        • pre‑storm quiet → flash scheduler → stroke loop\n",
    "        • inserts each burst into every station with propagation delay\n",
    "          and micro‑second timing jitter\n",
    "        • layers realistic analogue impairments (50 Hz hum, coloured\n",
    "          noise, RF tones, gain drift) before **ADC modelling**\n",
    "          (Butterworth anti‑alias, quantisation, optional clipping)\n",
    "\n",
    "Educational talking points\n",
    "--------------------------\n",
    "* **Reproducibility**   All randomness flows through `np.random.Generator`\n",
    "  seeded by `StormConfig.seed`; perfect for open‑science hand‑outs.\n",
    "* **Scaling difficulty**   Crank `difficulty` on stage to watch the\n",
    "  waveforms deteriorate: more multipath, sferic bed, clock skew —\n",
    "  students learn which artefacts cripple naïve detectors first.\n",
    "* **Geo‑physics realism**   Show how `_hav` + `_path_loss` convert\n",
    "  distance to amplitude; justify the magic numbers (ground‑wave vs\n",
    "  sky‑wave regions).\n",
    "* **Signal chain**   From physical burst → channel → pre‑amp →\n",
    "  anti‑alias → ADC; mirrors a real sensor stack and explains why\n",
    "  certain errors (clipping, skew) appear downstream.\n",
    "* **Output formats**   `df_wave` and `df_labels` align exactly with the\n",
    "  earlier FeatureExtractor.  Demonstrate a full pipeline:\n",
    "  ```python\n",
    "  gen = StormGenerator(StormConfig(difficulty=5, snr_db=-6))\n",
    "  storm = gen.generate()\n",
    "  fx   = FeatureExtractor([\"iso16\"])\n",
    "  X, _ = fx.transform(storm.quantised, win=1024, hop=512, fs=109_375)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "storm.py — Synthetic lightning‑storm generator (concise, UK English).\n",
    "\n",
    "Public API\n",
    "----------\n",
    "- :class:`StormConfig`  — immutable configuration for a synthetic scene.\n",
    "- :class:`StormBundle`  — container for generated artefacts.\n",
    "- :class:`StormGenerator` — end‑to‑end synthesis with reproducible RNG.\n",
    "\n",
    "Design notes\n",
    "------------\n",
    "- Units are explicit (km, Hz, dB **amplitude**, seconds, int16 codes).\n",
    "- Haversine distance and a simple path‑loss law set amplitude vs. range.\n",
    "- Difficulty tier toggles impairments (e.g. coloured noise, RFI, clipping).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import math, numpy as np, pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Configuration  (inline for notebook; later import from config.py)\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass(frozen=True)\n",
    "class StormConfig:\n",
    "    \"\"\"Immutable configuration for storm synthesis.\n",
    "\n",
    "    Attributes:\n",
    "        seed: Seed for the per‑instance NumPy RNG (reproducibility).\n",
    "        duration_min: Active storm duration in minutes (pre‑roll added internally).\n",
    "        scenario: One of ``'near'``, ``'medium'``, ``'far'`` (affects rate/extent).\n",
    "        difficulty: Integer 1–9; higher tiers enable more impairments.\n",
    "        snr_db: Requested SNR in **dB (amplitude)**; ``SNR_lin = 10**(snr_db/20)``.\n",
    "        fs: Sampling rate in Hz (default 109375 Hz).\n",
    "        bits: ADC resolution in bits (quantiser depth).\n",
    "        vref: ADC full‑scale reference voltage for clipping/quantisation.\n",
    "    \"\"\"\n",
    "    seed: int = 424242\n",
    "    duration_min: int = 5\n",
    "    scenario: str = 'medium'           # 'near' | 'medium' | 'far'\n",
    "    difficulty: int = 1                # 1 … 9\n",
    "    snr_db: float = 1.0               # –18 … +24 dB\n",
    "    fs: int = 109_375                  # Sampling rate (Hz)\n",
    "    bits: int = 14\n",
    "    vref: float = 1.0\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Return container\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class StormBundle:\n",
    "    \"\"\"All artefacts produced by :class:`StormGenerator`.\n",
    "\n",
    "    Attributes:\n",
    "        quantised: Station code → ``np.int16`` waveform; length ``N`` samples.\n",
    "        events: Per‑flash metadata (ID, type, epicentre, stroke times).\n",
    "        stroke_records: One row per **station × stroke** with alignment indices.\n",
    "        df_wave: DataFrame with ``['time_s', <station columns…>]``.\n",
    "        df_labels: Tidy labels DataFrame derived from ``stroke_records``.\n",
    "    \"\"\"\n",
    "    quantised: Dict[str, np.ndarray]\n",
    "    events: List[dict]\n",
    "    stroke_records: List[dict]\n",
    "    df_wave: pd.DataFrame\n",
    "    df_labels: pd.DataFrame\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Storm generator\n",
    "# ----------------------------------------------------------------------\n",
    "class StormGenerator:\n",
    "    \"\"\"Reproducible synthetic storm generator.\n",
    "\n",
    "    Class Attributes:\n",
    "        _stations: Fixed station catalogue (lat/lon in degrees).\n",
    "        _STN: Ordered list of station codes (stable across runs).\n",
    "\n",
    "    Instance Attributes:\n",
    "        cfg: (:class:`StormConfig`) Configuration for this generator.\n",
    "        rng: (:class:`numpy.random.Generator`) Per‑instance RNG seeded from\n",
    "            :attr:`cfg.seed`.\n",
    "    \"\"\"\n",
    "    _stations = {  # static table copied from notebook\n",
    "        'KEF': dict(lat=64.020, lon=-22.567),  'VAL': dict(lat=51.930, lon=-10.250),\n",
    "        'LER': dict(lat=60.150, lon= -1.130),  'HER': dict(lat=50.867, lon=  0.336),\n",
    "        'GIB': dict(lat=36.150, lon= -5.350),  'AKR': dict(lat=34.588, lon= 32.986),\n",
    "        'CAM': dict(lat=50.217, lon= -5.317),  'WAT': dict(lat=52.127, lon=  0.956),\n",
    "        'CAB': dict(lat=51.970, lon=  4.930),  'PAY': dict(lat=46.820, lon=  6.950),\n",
    "        'TAR': dict(lat=58.263, lon= 26.464),\n",
    "    }\n",
    "    _STN = list(_stations)\n",
    "\n",
    "    # ---------- helpers --------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _hav(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Great‑circle distance (km) via the haversine formula.\n",
    "\n",
    "        Args:\n",
    "            lat1: Latitude of point 1 in degrees.\n",
    "            lon1: Longitude of point 1 in degrees.\n",
    "            lat2: Latitude of point 2 in degrees.\n",
    "            lon2: Longitude of point 2 in degrees.\n",
    "\n",
    "        Returns:\n",
    "            Distance in kilometres on a sphere of radius 6 371 km.\n",
    "        \"\"\"\n",
    "        R = 6371.0\n",
    "        φ1, φ2 = map(math.radians, (lat1, lat2))\n",
    "        dφ, dλ = math.radians(lat2-lat1), math.radians(lon2-lon1)\n",
    "        a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "        return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "    @staticmethod\n",
    "    def _path_loss(dist_km: float) -> float:\n",
    "        \"\"\"Amplitude path‑loss factor as a function of range.\n",
    "\n",
    "        Args:\n",
    "            dist_km: Great‑circle path length in kilometres.\n",
    "\n",
    "        Returns:\n",
    "            Unitless amplitude scaling factor (≤ 1 typically). A minor\n",
    "            correction is applied for very long paths (> 600 km) to model\n",
    "            increased sky‑wave contribution.\n",
    "        \"\"\"\n",
    "        loss = (100/(dist_km + 100))**0.85 * math.exp(-0.0001*dist_km)\n",
    "        if dist_km > 600:\n",
    "            loss *= math.sqrt(2)\n",
    "        return loss\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    def __init__(self, cfg: StormConfig):\n",
    "        \"\"\"Initialise the generator with an immutable configuration.\n",
    "\n",
    "        Args:\n",
    "            cfg: Configuration bundle. The RNG seed drives reproducibility.\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    def _difficulty_flags(self) -> Dict[str, bool]:\n",
    "        \"\"\"Map :attr:`cfg.difficulty` to feature toggles.\n",
    "\n",
    "        Returns:\n",
    "            Dict of booleans keyed by feature name (e.g. ``'multipath'``,\n",
    "            ``'coloured_noise'``, ``'rfi_tones'``, ``'skywave'``, etc.).\n",
    "            Thresholds are monotonic in difficulty.\n",
    "        \"\"\"\n",
    "        d = self.cfg.difficulty\n",
    "        return dict(\n",
    "            ic_mix         = d >= 2,  multipath      = d >= 3,\n",
    "            coloured_noise = d >= 4,  rfi_tones      = d >= 5,\n",
    "            impulsive_rfi  = d >= 6,  sprite_ring    = d >= 5,\n",
    "            false_transient= d >= 6,  clipping       = d >= 5,\n",
    "            multi_cell     = d >= 6,  skywave        = d >= 7,\n",
    "            sferic_bed     = d >= 7,  clock_skew     = d >= 8,\n",
    "            gain_drift     = d >= 8,  dropouts       = d >= 8,\n",
    "            low_snr        = d >= 9,  burst_div      = d >= 9,\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    def _make_burst(self, dist_km: float, cg: bool, flags, SNR_lin, tv40, FS):\n",
    "        \"\"\"Synthetise a single 40 ms burst at a given range.\n",
    "\n",
    "        Args:\n",
    "            dist_km: Emitter→station distance in kilometres.\n",
    "            cg: ``True`` for cloud‑to‑ground; ``False`` for intra‑cloud.\n",
    "            flags: Feature flags from :meth:`_difficulty_flags`.\n",
    "            SNR_lin: Linear amplitude scale from ``snr_db``.\n",
    "            tv40: Time vector for 40 ms window (``np.arange(wave_len)/FS``).\n",
    "            FS: Sampling rate in Hz.\n",
    "\n",
    "        Returns:\n",
    "            ``np.float32`` array of length ``len(tv40)`` (burst waveform).\n",
    "        \"\"\"\n",
    "        rng, wave_len = self.rng, len(tv40)\n",
    "        if flags['burst_div'] and rng.random() < 0.15:\n",
    "            τ = 0.0008\n",
    "            burst = (tv40/τ) * np.exp(1 - tv40/τ)\n",
    "        else:\n",
    "            f0 = rng.uniform(3e3, 15e3)\n",
    "            τ  = rng.uniform(0.00025, 0.001 if cg else 0.0005)\n",
    "            burst = np.sin(2*math.pi*f0*tv40) * np.exp(-tv40/τ)\n",
    "\n",
    "        Vpeak = (0.15 if cg else 0.06)\n",
    "        Vpeak *= self._path_loss(dist_km) / self._path_loss(100.0)\n",
    "        Vpeak *= 10**(rng.uniform(-3, 3)/20) * SNR_lin\n",
    "        if flags['low_snr']:\n",
    "            Vpeak *= 0.5\n",
    "        burst *= Vpeak / (np.abs(burst).max() + 1e-12)\n",
    "\n",
    "        if flags['multipath'] and dist_km > 80 and rng.random() < 0.6:\n",
    "            dly = int(rng.uniform(0.001, 0.004) * FS)\n",
    "            if dly < wave_len:\n",
    "                burst[dly:] += 0.35 * burst[:-dly]\n",
    "        if flags['sprite_ring'] and rng.random() < 0.05:\n",
    "            dly = int(rng.uniform(0.008, 0.018) * FS)\n",
    "            if dly < wave_len:\n",
    "                burst[dly:] += 0.25 * burst[:-dly]\n",
    "        if flags['skywave'] and dist_km > 600:\n",
    "            f = np.fft.rfftfreq(wave_len, 1/FS)\n",
    "            H = np.exp(-0.00030*dist_km * (f/7e3)**2)\n",
    "            burst = np.fft.irfft(np.fft.rfft(burst)*H, n=wave_len)\n",
    "\n",
    "        return burst.astype(np.float32)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    def generate(self) -> StormBundle:\n",
    "        \"\"\"Generate a complete synthetic scene.\n",
    "\n",
    "        Steps (high level):\n",
    "            1) Build timeline (pre‑roll + active window).\n",
    "            2) Sample one or more drifting storm cells.\n",
    "            3) Poisson flash scheduler (scenario‑scaled rate).\n",
    "            4) For each stroke at each station:\n",
    "               - Compute propagation delay (:meth:`_hav`).\n",
    "               - Synthetise burst (:meth:`_make_burst`) and stage for mixing.\n",
    "            5) Add impairments (noise, hum, tones, impulsive/false events,\n",
    "               gain drift, clock skew).\n",
    "            6) Front‑end/ADC model: low‑pass, optional clipping, quantise\n",
    "               to signed 14‑bit codes.\n",
    "            7) Assemble DataFrames and return :class:`StormBundle`.\n",
    "\n",
    "        Returns:\n",
    "            A :class:`StormBundle` with waveforms, labels, and flash metadata.\n",
    "\n",
    "        Side effects:\n",
    "            Prints a concise summary line (tier/SNR/scenario/cells, counts).\n",
    "        \"\"\"\n",
    "        cfg, rng = self.cfg, self.rng\n",
    "        FS = cfg.fs\n",
    "        flags = self._difficulty_flags()\n",
    "\n",
    "        # ----- timeline ---------------------------------------------------\n",
    "        pre_sec   = rng.uniform(5, 30)\n",
    "        storm_sec = cfg.duration_min * 60\n",
    "        N         = int((pre_sec + storm_sec) * FS)\n",
    "\n",
    "        quantised       = {nm: np.zeros(N, np.int16) for nm in self._STN}\n",
    "        events, stroke_records, burst_book = [], [], []\n",
    "\n",
    "        # ----- storm cells ------------------------------------------------\n",
    "        lat_v = np.fromiter((s['lat'] for s in self._stations.values()), float)\n",
    "        lon_v = np.fromiter((s['lon'] for s in self._stations.values()), float)\n",
    "        lat_box = (lat_v.min()-0.9, lat_v.max()+0.9)\n",
    "        lon_box = (lon_v.min()-1.5, lon_v.max()+1.5)\n",
    "        new_cell = lambda: dict(lat=rng.uniform(*lat_box),\n",
    "                                lon=rng.uniform(*lon_box),\n",
    "                                drift=rng.uniform(-0.30, 0.30, 2))\n",
    "        cells = [new_cell() for _ in range(1 if not flags['multi_cell']\n",
    "                                           else rng.integers(2, 5))]\n",
    "        R0_km = dict(near=120, medium=400, far=1000)[cfg.scenario]\n",
    "\n",
    "        # ----- waveform library ------------------------------------------\n",
    "        wave_len = int(0.04 * FS)\n",
    "        tv40     = np.arange(wave_len) / FS\n",
    "        SNR_lin  = 10**(cfg.snr_db/20)\n",
    "\n",
    "        # ----- flash scheduler -------------------------------------------\n",
    "        λ_flash = dict(near=8, medium=4, far=2)[cfg.scenario]*(1+0.4*cfg.difficulty)\n",
    "        flash_ts, eid = pre_sec, 0\n",
    "\n",
    "        while flash_ts < pre_sec + storm_sec:\n",
    "            flash_ts += rng.exponential(60/λ_flash)\n",
    "            if flash_ts >= pre_sec + storm_sec:\n",
    "                break\n",
    "            eid += 1\n",
    "\n",
    "            cell  = rng.choice(cells)\n",
    "            age_h = (flash_ts - pre_sec) / 3600\n",
    "            f_lat = cell['lat'] + cell['drift'][0]*age_h\n",
    "            f_lon = cell['lon'] + cell['drift'][1]*age_h\n",
    "            r_km, θ = rng.uniform(0, R0_km), rng.uniform(0, 2*math.pi)\n",
    "            f_lat += (r_km/111)*math.cos(θ)\n",
    "            f_lon += (r_km/111)*math.sin(θ)/math.cos(math.radians(f_lat))\n",
    "\n",
    "            f_type = 'IC' if (flags['ic_mix'] and rng.random() < 0.35) else 'CG'\n",
    "            n_str  = rng.integers(1, 4 if f_type == 'IC' else 6)\n",
    "            gaps   = rng.exponential(0.008, n_str)\n",
    "            s_times = [flash_ts + float(gaps[:i+1].sum()) for i in range(n_str)]\n",
    "\n",
    "            events.append(dict(id=eid, flash_type=f_type, lat=f_lat, lon=f_lon,\n",
    "                               stroke_times=s_times))\n",
    "\n",
    "            for si, t0 in enumerate(s_times):\n",
    "                for nm in self._STN:\n",
    "                    dist = self._hav(f_lat, f_lon,\n",
    "                                     self._stations[nm]['lat'], self._stations[nm]['lon'])\n",
    "                    idx  = int((t0 + dist/300_000 + rng.normal(0, 40e-6)) * FS)\n",
    "                    if idx >= N - wave_len:\n",
    "                        continue\n",
    "                    burst = self._make_burst(dist, cg=(f_type == 'CG'), flags=flags,\n",
    "                                             SNR_lin=SNR_lin, tv40=tv40, FS=FS)\n",
    "                    burst_book.append((nm, idx, burst))\n",
    "                    stroke_records.append(dict(event_id=eid, stroke_i=si, station=nm,\n",
    "                                               flash_type=f_type, lat=f_lat, lon=f_lon,\n",
    "                                               true_time_s=t0, sample_idx=idx,\n",
    "                                               window_idx=idx//1024))\n",
    "\n",
    "        # ----- noise & ADC synthesis -------------------------------------\n",
    "        rfi_tones = [14_400, 20_100, 30_300]\n",
    "        b, a      = butter(4, 45_000/(FS/2), 'low')\n",
    "        chunk     = int(20*FS); tv_wave = tv40\n",
    "\n",
    "        for nm in self._STN:\n",
    "            bursts = [(i0, br) for st, i0, br in burst_book if st == nm]\n",
    "            cfg_noise = dict(\n",
    "                white=rng.uniform(0.010, 0.018),\n",
    "                hum=rng.uniform(0.006, 0.020) if flags['coloured_noise'] else 0.01,\n",
    "                tones=[] if not flags['rfi_tones'] else\n",
    "                      [(rng.choice(rfi_tones), rng.uniform(0.001, 0.004))],\n",
    "                gain_drift=rng.uniform(-0.05, 0.05) if flags['gain_drift'] else 0.0,\n",
    "                skew=rng.uniform(-25e-6, 25e-6) if flags['clock_skew'] else 0.0\n",
    "            )\n",
    "\n",
    "            drop = np.ones(N, bool)\n",
    "            if flags['dropouts'] and rng.random() < 0.1:\n",
    "                for _ in range(rng.integers(1, 3)):\n",
    "                    s = rng.integers(int(pre_sec*FS), N - int(0.4*FS))\n",
    "                    drop[s:s+int(0.4*FS)] = False\n",
    "\n",
    "            for s0 in range(0, N, chunk):\n",
    "                e0 = min(N, s0 + chunk)\n",
    "                L  = e0 - s0\n",
    "                t  = np.arange(s0, e0) / FS * (1 + cfg_noise['skew'])\n",
    "\n",
    "                seg = (cfg_noise['white'] * rng.standard_normal(L) +\n",
    "                       cfg_noise['hum']   * np.sin(2*math.pi*50*t))\n",
    "                for f, amp in cfg_noise['tones']:\n",
    "                    seg += amp * np.sin(2*math.pi*f*t + rng.uniform(0, 2*math.pi))\n",
    "                if flags['sferic_bed']:\n",
    "                    seg += 0.0008 * rng.standard_normal(L)\n",
    "\n",
    "                seg *= 1 + cfg_noise['gain_drift'] * (t - pre_sec) / (storm_sec + 1e-6)\n",
    "\n",
    "                for i0, br in bursts:\n",
    "                    if s0 <= i0 < e0:\n",
    "                        off = i0 - s0\n",
    "                        seg[off:off+wave_len] += br[:min(wave_len, L-off)]\n",
    "\n",
    "                if flags['impulsive_rfi'] and rng.random() < 0.002:\n",
    "                    p = rng.integers(0, L-200)\n",
    "                    seg[p:p+200] += rng.uniform(-0.9, 0.9) * np.hanning(200)\n",
    "                if flags['false_transient'] and rng.random() < 0.003:\n",
    "                    p = rng.integers(0, L-wave_len)\n",
    "                    seg[p:p+wave_len] += (0.6*np.sin(2*math.pi*5800*tv_wave) *\n",
    "                                          np.exp(-tv_wave/0.0009))\n",
    "\n",
    "                seg = filtfilt(b, a, seg)\n",
    "                if flags['clipping']:\n",
    "                    seg = np.clip(seg, -0.9*cfg.vref, 0.9*cfg.vref)\n",
    "                full = 2**(cfg.bits-1) - 1\n",
    "                adc  = np.clip(np.round(seg/cfg.vref*full), -full, full).astype(np.int16)\n",
    "                quantised[nm][s0:e0][drop[s0:e0]] = adc[drop[s0:e0]]\n",
    "\n",
    "        # ----- DataFrames & summary --------------------------------------\n",
    "        df_wave = pd.DataFrame({'time_s': np.arange(N)/FS})\n",
    "        for nm in self._STN:\n",
    "            df_wave[nm] = quantised[nm]\n",
    "        df_labels = pd.DataFrame(stroke_records)\n",
    "\n",
    "        print(f\"Tier-{cfg.difficulty}  SNR={cfg.snr_db:+.1f} dB  scenario={cfg.scenario}\"\n",
    "              f\"  cells={len(cells)}\")\n",
    "        print(f\"Flashes {len(events):3d} | strokes \"\n",
    "              f\"{len(df_labels)//len(self._STN):3d} | samples {N:,}\")\n",
    "\n",
    "        return StormBundle(quantised, events, stroke_records, df_wave, df_labels)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Example quick run (will execute when you run this cell)\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = StormConfig()          # default parameters\n",
    "    gen = StormGenerator(cfg)\n",
    "    storm_data = gen.generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  Lightning‑storm visual summary –   *updated for the modular notebook*\n",
    "# ---------------------------------------------------------------------\n",
    "#  • Works directly with `storm_data` (StormBundle) and `cfg`\n",
    "#  • Needs **NO** other globals except what earlier cells already created.\n",
    "# =====================================================================\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "plt.rcParams.update({'axes.grid': True, 'figure.dpi': 110})\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0)  Helper – tiny console table printer\n",
    "# ------------------------------------------------------------------\n",
    "def _tbl(rows, hdr=None, col_sep=\"  \"):\n",
    "    \"\"\"Print a plain text table to stdout.\n",
    "\n",
    "    Args:\n",
    "        rows: Iterable of row iterables (cells converted via `str()`).\n",
    "        hdr: Optional header row (iterable of column headings).\n",
    "        col_sep: Column separator string.\n",
    "\n",
    "    Returns:\n",
    "        None. Prints the table and a trailing newline.\n",
    "    \"\"\"\n",
    "    if hdr: print(col_sep.join(hdr))\n",
    "    for r in rows:\n",
    "        print(col_sep.join(str(c) for c in r))\n",
    "    print()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1)  Harmonise the key data objects\n",
    "# ------------------------------------------------------------------\n",
    "FS         = cfg.fs                              # sampling rate\n",
    "STN        = list(storm_data.quantised)          # station order\n",
    "quantised  = storm_data.quantised                # Dict[str, np.ndarray]\n",
    "events     = storm_data.events\n",
    "stroke_records = storm_data.stroke_records\n",
    "stations   = StormGenerator._stations            # static lat/lon table\n",
    "\n",
    "DIFFICULTY = cfg.difficulty                      # 1 … 9\n",
    "N          = quantised[STN[0]].size\n",
    "dur_sec    = N / FS\n",
    "\n",
    "print(f\"► Simulator difficulty tier : {DIFFICULTY}\")\n",
    "print(f\"► Sampling rate             : {FS:,.0f} Hz (Δt={1e6/FS:.2f} µs)\")\n",
    "print(f\"► Duration                  : {dur_sec:.2f} s  ({dur_sec/60:.2f} min)\")\n",
    "print(f\"► Total ADC samples         : {N:,}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2)  Per‑station basic ADC stats\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "for nm in STN:\n",
    "    q = quantised[nm].astype(float)\n",
    "    rows.append([nm, q.min().astype(int), q.max().astype(int),\n",
    "                 f\"{q.mean():.1f}\", f\"{q.std():.1f}\",\n",
    "                 f\"{100*np.count_nonzero(q)/len(q):.2f}%\"])\n",
    "_tbl(rows, hdr=[\"STN\",\"min\",\"max\",\"μ\",\"σ\",\"non‑zero %\"])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3)  Flash / stroke timing & location dataframe\n",
    "# ------------------------------------------------------------------\n",
    "stroke_times = np.hstack([ev[\"stroke_times\"] for ev in events])\n",
    "lat_rep      = np.hstack([[ev['lat']]*len(ev['stroke_times']) for ev in events])\n",
    "lon_rep      = np.hstack([[ev['lon']]*len(ev['stroke_times']) for ev in events])\n",
    "\n",
    "df_strokes = pd.DataFrame(dict(time_s=stroke_times, lat=lat_rep, lon=lon_rep))\n",
    "df_strokes.sort_values(\"time_s\", inplace=True, ignore_index=True)\n",
    "\n",
    "print(f\"Flashes : {len(events)}\")\n",
    "print(f\"Strokes : {len(df_strokes)}\\n\")\n",
    "display(df_strokes.head())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4)  Simple “did‑station‑see‑stroke?” heuristic\n",
    "#     (noise σ estimated from the *first three seconds*)\n",
    "# ------------------------------------------------------------------\n",
    "pre_samp = int(3 * FS)\n",
    "det_tbl  = []\n",
    "\n",
    "for nm in STN:\n",
    "    noiseσ = quantised[nm][:pre_samp].astype(float).std()\n",
    "    thr    = 3 * noiseσ\n",
    "    hits   = []\n",
    "    for t in df_strokes.time_s:\n",
    "        idx = int(t * FS)\n",
    "        hits.append(abs(quantised[nm][idx]) >= thr if idx < N else False)\n",
    "    df_strokes[f\"det_{nm}\"] = hits\n",
    "    det_tbl.append([nm, f\"{100*np.mean(hits):.1f}%\"])\n",
    "\n",
    "_tbl(det_tbl, hdr=[\"STN\", \"simple detection rate\"])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5)  Window‑level labels (quiet = 0, lightning = 1)\n",
    "# ------------------------------------------------------------------\n",
    "W      = 1024\n",
    "n_win  = N // W\n",
    "starts = (np.arange(n_win) * W) / FS\n",
    "\n",
    "stroke_idx  = (df_strokes.time_s.values * FS).astype(int)\n",
    "labels_win  = np.zeros(n_win, bool)\n",
    "labels_win[(stroke_idx // W).clip(max=n_win-1)] = True\n",
    "\n",
    "df_win = pd.DataFrame(dict(win_idx=np.arange(n_win, dtype=int),\n",
    "                           start_s=starts,\n",
    "                           label=labels_win.astype(int)))\n",
    "quiet_cnt, light_cnt = np.bincount(df_win.label, minlength=2)\n",
    "print(f\"\\nWindows: quiet={quiet_cnt}, lightning={light_cnt}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6)  Visual #1 – ADC histograms (log‑scale y)\n",
    "# ------------------------------------------------------------------\n",
    "for nm in STN:\n",
    "    plt.figure(figsize=(4, 2.5))\n",
    "    plt.hist(quantised[nm][::max(1, N//200_000)],\n",
    "             bins=120, log=True, color='#4682B4')\n",
    "    plt.title(f\"{nm} – ADC histogram\")\n",
    "    plt.xlabel(\"ADC counts\"); plt.ylabel(\"occurrences (log)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7)  Visual #2 – first lightning vs first quiet window\n",
    "# ------------------------------------------------------------------\n",
    "def _plot_window(win_row, title):\n",
    "    \"\"\"Plot all station traces for the given window row.\n",
    "\n",
    "    Args:\n",
    "        win_row: A row from `df_win` with fields `win_idx`, `start_s`, `label`.\n",
    "        title: Figure title.\n",
    "\n",
    "    Notes:\n",
    "        Uses outer‑scope globals: `W`, `FS`, `STN`, and `quantised`.\n",
    "        Assumes `win_row.win_idx` is a valid index (0 ≤ idx < n_win).\n",
    "    \"\"\"\n",
    "    idx = int(win_row.win_idx)\n",
    "    tms = (np.arange(W) / FS) * 1e3\n",
    "    plt.figure(figsize=(6, 2.4))\n",
    "    for nm in STN:\n",
    "        seg = quantised[nm][idx*W : (idx+1)*W]\n",
    "        plt.plot(tms, seg, label=nm, alpha=.8)\n",
    "    plt.title(title); plt.xlabel(\"time (ms)\"); plt.ylabel(\"ADC\")\n",
    "    plt.legend(ncol=len(STN)//2)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "_plot_window(df_win[df_win.label == 1].iloc[0], \"First lightning window\")\n",
    "_plot_window(df_win[df_win.label == 0].iloc[0], \"First quiet window\"   )\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8)  Visual #3 – Hilbert envelope of the lightning window\n",
    "# ------------------------------------------------------------------\n",
    "idx_lit = int(df_win[df_win.label == 1].iloc[0].win_idx)\n",
    "tms     = (np.arange(W) / FS) * 1e3\n",
    "plt.figure(figsize=(6, 2.6))\n",
    "for nm in STN:\n",
    "    seg = quantised[nm][idx_lit*W : (idx_lit+1)*W].astype(float)\n",
    "    env = np.abs(hilbert(seg))\n",
    "    plt.plot(tms, env, label=f\"{nm} env\")\n",
    "plt.title(f\"Envelope – window {idx_lit} (lightning)\")\n",
    "plt.xlabel(\"time (ms)\"); plt.ylabel(\"|hilbert|\")\n",
    "plt.legend(ncol=len(STN)//2); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9)  Visual #4 – stroke‑window heat‑map\n",
    "# ------------------------------------------------------------------\n",
    "station_truth = {nm: np.zeros(n_win, bool) for nm in STN}\n",
    "for r in stroke_records:\n",
    "    station_truth[r['station']][r['window_idx']] = True\n",
    "\n",
    "truth_mat = np.vstack([station_truth[nm][:n_win] for nm in STN])\n",
    "plt.figure(figsize=(10, 1.2 + 0.25*len(STN)))\n",
    "plt.imshow(truth_mat, aspect='auto',\n",
    "           cmap=plt.get_cmap(\"Reds\", 2), interpolation='nearest')\n",
    "plt.yticks(range(len(STN)), STN)\n",
    "plt.xlabel(f\"window index ({W} samples)\")\n",
    "plt.title(\"Ground‑truth stroke windows\")\n",
    "for w in np.where(truth_mat.any(0))[0]:\n",
    "    plt.axvline(w, color='red', lw=.5, alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10) Optional geographic snapshot (Cartopy)\n",
    "# ------------------------------------------------------------------\n",
    "try:\n",
    "    import cartopy.crs as ccrs, cartopy.feature as cfeature\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax  = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE, lw=.6)\n",
    "    ax.add_feature(cfeature.BORDERS, lw=.4)\n",
    "    ax.set_extent([-35, 30, 30, 65])\n",
    "    ax.scatter(df_strokes.lon, df_strokes.lat,\n",
    "               c='orange', s=10, lw=0, zorder=2)\n",
    "    for nm in STN:\n",
    "        ax.plot(stations[nm]['lon'], stations[nm]['lat'],\n",
    "                '^', ms=7, mfc='#1f77b4', mec='k', zorder=3)\n",
    "        ax.text(stations[nm]['lon'] + 0.4, stations[nm]['lat'] + 0.4,\n",
    "                nm, fontsize=8)\n",
    "    ax.set_title(\"Station & flash geography\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "except ImportError:\n",
    "    print(\"(Cartopy not installed – map skipped)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# MODULE TARGET : lightning_sim/features/basic.py\n",
    "# ----------------------------------------------------------------------\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "End‑to‑End Feature‑Engineering Toolkit   —  Lecture‑ready overview (UK English)\n",
    "===============================================================================\n",
    "\n",
    "What’s in this cell?\n",
    "--------------------\n",
    "A *mini‑library in a single file* that transforms raw one‑dimensional\n",
    "sensor traces into model‑ready feature matrices.  It contains:\n",
    "\n",
    "1. **`make_windows`** – the canonical sliding‑window helper.\n",
    "2. **Feature registry** – decorator‑driven (`register_feature`,\n",
    "   `list_features`) so new features can be added without editing core\n",
    "   code.\n",
    "3. **Atomic features** – four concise statistics:\n",
    "      • *hilbert_peak*      – peak amplitude of the analytic envelope\n",
    "      • *rms_db*            – RMS energy in decibels\n",
    "      • *spectral_entropy*  – Shannon entropy of Welch PSD\n",
    "      • *env_kurtosis*      – excess kurtosis of the envelope\n",
    "4. **Composite blocks** – higher‑dimensional bundles tuned to specific\n",
    "   models:\n",
    "      • *iso13*  – 13‑D vector for Isolation‑Forest v1\n",
    "      • *ae10*   – 10‑D, I/O‑light block for an Auto‑Encoder\n",
    "      • *iso16* – 16‑D extension adding spectral‑shape moments\n",
    "5. **`FeatureExtractor`** – a stateless façade that calls the requested\n",
    "   features, **aligns all stations to the same window count**, and\n",
    "   returns a tidy dictionary of matrices.\n",
    "\n",
    "How you will talk through it\n",
    "----------------------------\n",
    "*Start at the top and work down, reinforcing each design decision.*\n",
    "\n",
    "* **Sliding windows (5 min)**   Show an int16 trace, explain why copying\n",
    "  10⁸ samples is untenable, then reveal that `make_windows` produces a\n",
    "  *view* – no extra memory – via advanced indexing.  A quick `timeit`\n",
    "  demo hammers the point home.\n",
    "\n",
    "* **Decorator registry (3 min)**   Live‑code a toy feature:\n",
    "  `@register_feature(\"abs_mean\")`.  Run `list_features()`; the new name\n",
    "  appears instantly.  This drives home the *plug‑and‑play* philosophy.\n",
    "\n",
    "* **Atomic features (10 min)**   Walk through each statistic, linking to\n",
    "  real‑world use cases:\n",
    "\n",
    "  ─ *Peak envelope*   impulsive faults in bearings, AE burst counting\n",
    "  ─ *RMS in dB*       audio loudness, normalised energy metrics\n",
    "  ─ *Spectral entropy* distinguishing tonal whistles from broadband\n",
    "    grinding\n",
    "  ─ *Envelope kurtosis* spike detection in structural‑health monitoring\n",
    "\n",
    "* **Composite blocks (15 min)**   Explain why bundling orthogonal cues\n",
    "  (time, frequency, complexity) helps tree ensembles and why a lean\n",
    "  10‑D vector speeds GPU auto‑encoding.  Emphasise that\n",
    "  *iso16* adds centroid/bandwidth/entropy so the model “knows” where\n",
    "  power lives in the spectrum.\n",
    "\n",
    "* **FeatureExtractor (7 min)**   Run a live extract:\n",
    "  ```python\n",
    "  mats, n = fx.transform(raw, win=1024, hop=512, fs=44_100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "basic.py — Feature‑engineering primitives for 1‑D sensor traces.\n",
    "\n",
    "Public API\n",
    "----------\n",
    "- :func:`make_windows` — fixed‑length overlapping windows (copy via integer indexing).\n",
    "- :func:`register_feature`, :func:`list_features` — decorator‑driven registry.\n",
    "- Atomic features: ``hilbert_peak``, ``rms_db``, ``spectral_entropy``, ``env_kurtosis``.\n",
    "- Composite blocks: ``iso13``, ``ae10``, ``iso16``.\n",
    "- :class:`FeatureExtractor` — stateless façade that aligns stations and stacks features.\n",
    "\n",
    "Conventions\n",
    "-----------\n",
    "- Shapes: windows are (n, W), outputs are (n, d).\n",
    "- Units: RMS/levels in dB **amplitude**; frequencies in Hz; indices are zero‑based.\n",
    "- Context: frequency‑aware features expect ``ctx['fs']`` (sampling rate in Hz).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import hilbert, welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "from typing import Callable, Dict, List\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Sliding‑window helper  (STRICTLY NO DUPLICATES ANYWHERE ELSE)\n",
    "# ----------------------------------------------------------------------\n",
    "def make_windows(sig: np.ndarray, win: int, hop: int) -> np.ndarray:\n",
    "    \"\"\"Construct an overlapping window matrix from a 1‑D signal.\n",
    "\n",
    "    Args:\n",
    "        sig: 1‑D array of samples (any numeric dtype).\n",
    "        win: Window length (samples).\n",
    "        hop: Step between successive windows (samples).\n",
    "\n",
    "    Returns:\n",
    "        Array of shape ``(n_windows, win)`` assembled via **integer indexing**.\n",
    "\n",
    "    Notes:\n",
    "        - This implementation uses advanced (integer) indexing and therefore\n",
    "          returns a **copy**, not a view. Memory usage is ``O(n_windows · win)``.\n",
    "        - Number of windows ``n_windows = floor((len(sig) - win)/hop) + 1``; no padding.\n",
    "        - Caller is responsible for dtype conversion if needed (e.g., to float).\n",
    "    \"\"\"\n",
    "    n = (len(sig) - win) // hop + 1\n",
    "    idx = np.arange(0, n * hop, hop)[:, None] + np.arange(win)\n",
    "    return sig[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Tiny registry of feature functions\n",
    "# ----------------------------------------------------------------------\n",
    "_feature_funcs: Dict[str, Callable[[np.ndarray, Dict], np.ndarray]] = {}\n",
    "\"\"\"Mapping from feature name → callable(win_mat, ctx) -> (n, d) float array.\"\"\"\n",
    "\n",
    "def register_feature(name: str):\n",
    "    \"\"\"Decorator to register a feature function under ``name``.\n",
    "\n",
    "    Contract:\n",
    "        The decorated function must accept:\n",
    "            * ``win_mat``: ``np.ndarray`` with shape ``(n, W)`` (windows)\n",
    "            * ``ctx``: ``Dict`` providing any run‑time context (e.g., ``fs``)\n",
    "        and return a ``(n, d)`` float array.\n",
    "\n",
    "    Args:\n",
    "        name: Registry key used to refer to the feature.\n",
    "\n",
    "    Returns:\n",
    "        A decorator that inserts the function into the global registry.\n",
    "    \"\"\"\n",
    "    def _wrap(func):\n",
    "        _feature_funcs[name] = func\n",
    "        return func\n",
    "    return _wrap\n",
    "\n",
    "def list_features() -> List[str]:\n",
    "    \"\"\"List the currently registered feature names (sorted).\"\"\"\n",
    "    return sorted(_feature_funcs)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Example feature functions\n",
    "# ----------------------------------------------------------------------\n",
    "@register_feature(\"hilbert_peak\")\n",
    "def feat_hilbert_peak(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"Peak of the analytic envelope per window.\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows (any numeric dtype).\n",
    "        ctx: Unused.\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 1)`` array with max(|hilbert(x)|) per window (float).\n",
    "    \"\"\"\n",
    "    env = np.abs(hilbert(win_mat.astype(float), axis=1))\n",
    "    return env.max(axis=1, keepdims=True)\n",
    "\n",
    "@register_feature(\"rms_db\")\n",
    "def feat_rms_db(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"RMS level in dB (amplitude) per window.\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows.\n",
    "        ctx: Unused.\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 1)`` array of ``20·log10(rms + ε)`` (float).\n",
    "    \"\"\"\n",
    "    rms = np.sqrt((win_mat.astype(float)**2).mean(axis=1)) + 1e-9\n",
    "    return (20 * np.log10(rms)).reshape(-1, 1)\n",
    "\n",
    "@register_feature(\"spectral_entropy\")\n",
    "def feat_spec_entropy(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"Shannon entropy of the Welch PSD (natural log).\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows.\n",
    "        ctx: Requires ``ctx['fs']`` (sampling rate, Hz).\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 1)`` array with ``-Σ p·ln(p)`` where ``p`` is normalised PSD.\n",
    "    \"\"\"\n",
    "    fs = ctx[\"fs\"]\n",
    "    out = np.empty((win_mat.shape[0], 1))\n",
    "    for i, w in enumerate(win_mat):\n",
    "        f, P = welch(w, fs, nperseg=256)\n",
    "        P /= P.sum() + 1e-9\n",
    "        out[i, 0] = -np.sum(P * np.log(P + 1e-9))\n",
    "    return out\n",
    "\n",
    "@register_feature(\"env_kurtosis\")\n",
    "def feat_env_kurt(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"Excess kurtosis of the analytic envelope.\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows.\n",
    "        ctx: Unused.\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 1)`` array of kurtosis(|hilbert(x)|) (float).\n",
    "    \"\"\"\n",
    "    env = np.abs(hilbert(win_mat.astype(float), axis=1))\n",
    "    return kurtosis(env, axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Registry extension – 13‑dim feature vector for Isolation‑Forest\n",
    "# ----------------------------------------------------------------------\n",
    "# Adds \"iso13\" to the existing feature registry.\n",
    "# You only need to run this cell ONCE in the notebook.\n",
    "# ======================================================================\n",
    "\n",
    "import numpy as np, math, zlib, pywt\n",
    "from scipy.signal import hilbert\n",
    "from typing import Dict\n",
    "\n",
    "@register_feature(\"iso13\")\n",
    "def feat_iso13(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"Compute a 13‑D feature block per window.\n",
    "\n",
    "    Features (columns):\n",
    "      0: peak_env       — max of |hilbert(x)|\n",
    "      1: med_env        — median of |hilbert(x)|\n",
    "      2: ratio_env      — peak_env / med_env\n",
    "      3: energy         — Σ x²\n",
    "      4: sta_lta        — envelope STA/LTA (128 / 1024 centred)\n",
    "      5: crest_short    — crest factor on central ⅛ of window\n",
    "      6: crest_global   — peak_env / RMS\n",
    "      7–10: frac1–4     — FFT power quartiles (0–25, 25–50, 50–75, 75–100 %)\n",
    "     11: wave_hi        — db4 level‑split: high‑band energy ratio\n",
    "     12: comp_ratio     — zlib compression ratio (bytes)\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows (int or float).\n",
    "        ctx: Unused.\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 13)`` float array.\n",
    "    \"\"\"\n",
    "    n, W = win_mat.shape\n",
    "    env   = np.abs(hilbert(win_mat.astype(float), axis=1))\n",
    "    Nfft  = W//2 + 1\n",
    "    b25,b50,b75 = [int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "    out = np.empty((n, 13), float)\n",
    "\n",
    "    def _sta_lta(x, sta=128, lta=1024):\n",
    "        c=len(x)//2\n",
    "        return x[c-sta//2:c+sta//2].mean()/(x[c-lta//2:c+lta//2].mean()+1e-9)\n",
    "    def _crest(seg):\n",
    "        n=len(seg)//8; c=len(seg)//2\n",
    "        part=seg[c-n//2:c+n//2].astype(float)\n",
    "        return np.abs(part).max()/(math.sqrt((part**2).mean())+1e-9)\n",
    "    _comp = lambda v: len(zlib.compress(v.tobytes(),6))/len(v.tobytes())\n",
    "\n",
    "    for i,(seg_i16, env_seg) in enumerate(zip(win_mat,env)):\n",
    "        seg_f = seg_i16.astype(float)\n",
    "        peak  = env_seg.max(); med = np.median(env_seg)\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2; totP=P.sum()+1e-9\n",
    "        hi = pywt.wavedec(seg_f,'db4',level=3)[1]\n",
    "        lo = pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        out[i] = [\n",
    "            peak, med, peak/(med+1e-9),\n",
    "            (seg_f**2).sum(), _sta_lta(env_seg),\n",
    "            _crest(seg_i16), peak/(math.sqrt((seg_f**2).mean())+1e-9),\n",
    "            P[:b25].sum()/totP, P[b25:b50].sum()/totP,\n",
    "            P[b50:b75].sum()/totP, P[b75:].sum()/totP,\n",
    "            (hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9),\n",
    "            _comp(seg_i16)\n",
    "        ]\n",
    "    return out\n",
    "\n",
    "# ======================================================================\n",
    "# Registry extension – \"ae10\"  (10‑dim node‑level feature block)\n",
    "# ----------------------------------------------------------------------\n",
    "# Run once; makes the feature available to any future model.\n",
    "# ======================================================================\n",
    "\n",
    "import numpy as np, math\n",
    "from scipy.signal import hilbert\n",
    "from typing import Dict\n",
    "\n",
    "@register_feature(\"ae10\")\n",
    "def feat_ae10(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"Compute a compact 10‑D feature block per window.\n",
    "\n",
    "    Columns:\n",
    "      0: peak_env         5: sta/lta (first 256 / whole)\n",
    "      1: median_env       6: FFT band 0–25 %\n",
    "      2: peak/median      7: FFT band 25–50 %\n",
    "      3: RMS              8: FFT band 50–75 %\n",
    "      4: peak/RMS         9: FFT band 75–100 %\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows.\n",
    "        ctx: Unused.\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 10)`` float array.\n",
    "    \"\"\"\n",
    "    env = np.abs(hilbert(win_mat.astype(float), axis=1))\n",
    "    n, W = win_mat.shape\n",
    "    out  = np.empty((n, 10), float)\n",
    "    Nfft = W//2 + 1\n",
    "    b25,b50,b75 = [int(Nfft*r) for r in (0.25,0.50,0.75)]\n",
    "\n",
    "    def _sta_lta(e):\n",
    "        return e[:256].mean() / (e.mean() + 1e-9)\n",
    "\n",
    "    for i, (seg, e) in enumerate(zip(win_mat, env)):\n",
    "        peak, med = e.max(), np.median(e)\n",
    "        rms  = math.sqrt((seg.astype(float)**2).mean()+1e-9)\n",
    "        P    = np.abs(np.fft.rfft(seg.astype(float)))**2\n",
    "        totP = P.sum()+1e-9\n",
    "        frac = [P[:b25].sum()/totP, P[b25:b50].sum()/totP,\n",
    "                P[b50:b75].sum()/totP, P[b75:].sum()/totP]\n",
    "        out[i] = [peak, med, peak/(med+1e-9),\n",
    "                  rms, peak/(rms+1e-9), _sta_lta(e), *frac]\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# Registry extension – \"iso16\"  (16‑dim Isolation‑Forest features)\n",
    "# ----------------------------------------------------------------------\n",
    "# Run once; adds the feature function to the shared registry.\n",
    "# ======================================================================\n",
    "\n",
    "import numpy as np, math, zlib, pywt\n",
    "from scipy.signal import hilbert\n",
    "from typing import Dict\n",
    "\n",
    "@register_feature(\"iso16\")\n",
    "def feat_iso16(win_mat: np.ndarray, ctx: Dict) -> np.ndarray:\n",
    "    \"\"\"Compute a 16‑D feature block per window (extended spectral shape).\n",
    "\n",
    "    Columns:\n",
    "      0: peak_env         8: band2 (25–50 %)\n",
    "      1: med_env          9: band3 (50–75 %)\n",
    "      2: ratio_env       10: band4 (75–100 %)\n",
    "      3: energy          11: wave_hi (db4 detail vs approx ratio)\n",
    "      4: sta_lta         12: comp_ratio (zlib)\n",
    "      5: crest_short     13: spec_centroid (Hz)\n",
    "      6: crest_global    14: spec_bandwidth (Hz)\n",
    "      7: band1 (0–25 %)  15: spec_entropy (bits; log₂)\n",
    "\n",
    "    Args:\n",
    "        win_mat: ``(n, W)`` windows.\n",
    "        ctx: Requires ``ctx['fs']`` (sampling rate, Hz).\n",
    "\n",
    "    Returns:\n",
    "        ``(n, 16)`` float array.\n",
    "    \"\"\"\n",
    "    fs = ctx[\"fs\"]\n",
    "    n, W = win_mat.shape\n",
    "    env  = np.abs(hilbert(win_mat.astype(float), axis=1))\n",
    "    Nf   = W//2 + 1\n",
    "    b25,b50,b75 = [int(Nf*r) for r in (0.25,0.50,0.75)]\n",
    "    out = np.empty((n, 16), float)\n",
    "\n",
    "    def _sta_lta(x, s=128, l=1024):\n",
    "        c=len(x)//2\n",
    "        return x[c-s//2:c+s//2].mean()/(x[c-l//2:c+l//2].mean()+1e-9)\n",
    "    def _crest(seg):\n",
    "        return np.abs(seg).max()/(math.sqrt((seg.astype(float)**2).mean())+1e-9)\n",
    "    _comp = lambda v: len(zlib.compress(v.tobytes(),6))/len(v.tobytes())\n",
    "\n",
    "    for i,(seg_i16, env_seg) in enumerate(zip(win_mat, env)):\n",
    "        seg_f = seg_i16.astype(float)\n",
    "        peak, med = env_seg.max(), np.median(env_seg)\n",
    "        P = np.abs(np.fft.rfft(seg_f))**2\n",
    "        totP = P.sum()+1e-9\n",
    "        frac = [P[:b25].sum()/totP, P[b25:b50].sum()/totP,\n",
    "                P[b50:b75].sum()/totP, P[b75:].sum()/totP]\n",
    "        hi = pywt.wavedec(seg_f,'db4',level=3)[1]\n",
    "        lo = pywt.wavedec(seg_f,'db4',level=3)[-1]\n",
    "        wave_hi = (hi**2).sum()/((hi**2).sum()+(lo**2).sum()+1e-9)\n",
    "        freqs = np.fft.rfftfreq(len(seg_f), d=1/fs)\n",
    "        Pn = P/totP\n",
    "        centroid = (freqs*Pn).sum()\n",
    "        bandwidth= math.sqrt(((freqs-centroid)**2*Pn).sum())\n",
    "        entropy  = -(Pn*np.log2(Pn+1e-12)).sum()\n",
    "\n",
    "        out[i] = [\n",
    "            peak, med, peak/(med+1e-9),\n",
    "            (seg_f**2).sum(), _sta_lta(env_seg),\n",
    "            _crest(seg_i16[len(seg_i16)//2-W//16:len(seg_i16)//2+W//16]),\n",
    "            _crest(seg_i16),\n",
    "            *frac, wave_hi, _comp(seg_i16),\n",
    "            centroid, bandwidth, entropy\n",
    "        ]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. FeatureExtractor  (works for ANY model)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Stateless façade that runs selected features and aligns stations.\n",
    "\n",
    "    Behaviour:\n",
    "        - Validates requested feature names against the global registry.\n",
    "        - Applies each feature to each station’s window matrix.\n",
    "        - Truncates all station matrices to the **minimum** window count\n",
    "          so that every station has consistent ``n_windows``.\n",
    "\n",
    "    Attributes:\n",
    "        names: List of feature names applied in order (instance attribute).\n",
    "\n",
    "    Example:\n",
    "        >>> fx = FeatureExtractor([\"hilbert_peak\", \"rms_db\"])\n",
    "        >>> X_stn, n_win = fx.transform(raw_signals, win=1024, hop=512, fs=109_375)\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_names: List[str]):\n",
    "        \"\"\"Create an extractor for a fixed ordered list of features.\n",
    "\n",
    "        Args:\n",
    "            feat_names: Names present in :func:`list_features()`. Order determines\n",
    "                horizontal stacking order in the output matrices.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any requested feature is unknown.\n",
    "        \"\"\"\n",
    "        missing = [n for n in feat_names if n not in _feature_funcs]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Unknown features: {missing}\")\n",
    "        self.names = feat_names\n",
    "        \"\"\"Ordered list of feature names to evaluate (instance‑attribute).\"\"\"\n",
    "\n",
    "    def transform(self,\n",
    "                  raw_signals: Dict[str, np.ndarray],\n",
    "                  *,\n",
    "                  win: int,\n",
    "                  hop: int,\n",
    "                  fs: int) -> (Dict[str, np.ndarray], int):\n",
    "        \"\"\"Extract features for each station and align to a common window count.\n",
    "\n",
    "        Args:\n",
    "            raw_signals: Mapping ``station → 1‑D int/float array`` (same length).\n",
    "            win: Window length (samples).\n",
    "            hop: Hop size (samples).\n",
    "            fs: Sampling rate (Hz). Passed via ``ctx`` to frequency‑aware features.\n",
    "\n",
    "        Returns:\n",
    "            mats: Dict ``station → (n_windows_aligned, d_total)`` float arrays,\n",
    "                where ``d_total`` is the sum of the selected feature dimensions.\n",
    "            n_windows_aligned: Minimum number of windows across stations.\n",
    "\n",
    "        Notes:\n",
    "            - Uses :func:`make_windows`; no padding; trailing samples are dropped.\n",
    "            - Truncation to the minimum window count ensures shapes match across stations.\n",
    "        \"\"\"\n",
    "        ctx = dict(fs=fs)\n",
    "        mats = {}\n",
    "        n_win_min = None\n",
    "        for nm, sig in raw_signals.items():\n",
    "            win_mat = make_windows(sig, win, hop)\n",
    "            parts = [ _feature_funcs[n](win_mat, ctx) for n in self.names ]\n",
    "            mats[nm] = np.hstack(parts)\n",
    "            n_win_min = mats[nm].shape[0] if n_win_min is None \\\n",
    "                        else min(n_win_min, mats[nm].shape[0])\n",
    "        # ensure all stations have same number of windows\n",
    "        mats = {nm: X[:n_win_min] for nm, X in mats.items()}\n",
    "        return mats, n_win_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "#  SIDE‑BY‑SIDE  SIGNAL‑ANALYSIS DASHBOARD  (quiet  vs  lightning)\n",
    "#  • One row per concept, two columns:  left = quiet, right = lightning\n",
    "#  • Independent y‑scales so nothing is flattened\n",
    "#  • No “scalar bundle” panel – every scalar feature has its own row\n",
    "#  • Summary table at the end\n",
    "# ======================================================================\n",
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd, math\n",
    "from scipy.signal import hilbert, stft\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "STATION = \"GIB\"                    # best station in this run\n",
    "WIN, HOP, FS = 1024, 512, cfg.fs\n",
    "C_Q, C_L = \"#4682B4\", \"#D2691E\"    # steel‑blue / dark‑orange\n",
    "\n",
    "# ─────────── locate windows ───────────────────────────────────────────\n",
    "def _first_stroke_win(stn, records):\n",
    "    for rec in records:\n",
    "        if rec[\"station\"] == stn:\n",
    "            return rec[\"sample_idx\"] // HOP\n",
    "    raise RuntimeError(\"no stroke found\")\n",
    "\n",
    "sig      = storm_data.quantised[STATION]\n",
    "win_mat  = make_windows(sig, WIN, HOP)\n",
    "w_L      = _first_stroke_win(STATION, storm_data.stroke_records)\n",
    "w_Q      = 0 if w_L else 1\n",
    "\n",
    "ctx = dict(fs=FS)\n",
    "\n",
    "# ─────────── feature scalars we want standalone rows for ──────────────\n",
    "# crest factor ---------------------------------------------------------\n",
    "crest_Q = np.abs(win_mat[w_Q]).max() / (math.sqrt((win_mat[w_Q]**2).mean())+1e-9)\n",
    "crest_L = np.abs(win_mat[w_L]).max() / (math.sqrt((win_mat[w_L]**2).mean())+1e-9)\n",
    "# STA/LTA --------------------------------------------------------------\n",
    "env_Q = np.abs(hilbert(win_mat[w_Q].astype(float)))\n",
    "env_L = np.abs(hilbert(win_mat[w_L].astype(float)))\n",
    "sta_Q, lta_Q = env_Q[:256].mean(), env_Q.mean()\n",
    "sta_L, lta_L = env_L[:256].mean(), env_L.mean()\n",
    "sta_lta_Q, sta_lta_L = sta_Q/lta_Q, sta_L/lta_L\n",
    "# Envelope kurtosis ----------------------------------------------------\n",
    "from scipy.stats import kurtosis\n",
    "kurt_Q = kurtosis(env_Q, fisher=False)\n",
    "kurt_L = kurtosis(env_L, fisher=False)\n",
    "# Wavelet high‑band energy ratio ---------------------------------------\n",
    "iso_Q, iso_L = (feat_iso16(win_mat[i:i+1], ctx)[0] for i in (w_Q, w_L))\n",
    "wave_hi_Q, wave_hi_L = iso_Q[11], iso_L[11]\n",
    "\n",
    "# ─────────── plotting canvas ──────────────────────────────────────────\n",
    "fig = plt.figure(figsize=(12, 48), constrained_layout=True)\n",
    "gs  = fig.add_gridspec(9, 2, height_ratios=[1.2]*4 + [0.6]*5)\n",
    "\n",
    "def _axpair(row, title):\n",
    "    axL = fig.add_subplot(gs[row, 0]); axR = fig.add_subplot(gs[row, 1])\n",
    "    axL.set_title(f\"{title} – quiet\",      fontsize=11)\n",
    "    axR.set_title(f\"{title} – lightning\",  fontsize=11)\n",
    "    return axL, axR\n",
    "\n",
    "# 0 Raw waveform -------------------------------------------------------\n",
    "axq, axl = _axpair(0, \"Raw waveform\")\n",
    "t_ms = np.arange(WIN)/FS*1e3\n",
    "axq.plot(t_ms, win_mat[w_Q], color=C_Q); axq.set_ylabel(\"ADC\")\n",
    "axl.plot(t_ms, win_mat[w_L], color=C_L); axl.set_ylabel(\"ADC\")\n",
    "for ax in (axq, axl): ax.set_xlabel(\"time (ms)\")\n",
    "\n",
    "# 1 Hilbert envelope ---------------------------------------------------\n",
    "axq, axl = _axpair(1, \"Hilbert envelope\")\n",
    "axq.plot(t_ms, env_Q, color=C_Q); axl.plot(t_ms, env_L, color=C_L)\n",
    "for ax in (axq, axl): ax.set_xlabel(\"time (ms)\")\n",
    "\n",
    "# 2 Power spectrum (log) ----------------------------------------------\n",
    "axq, axl = _axpair(2, \"Power spectrum\")\n",
    "f_kHz = np.fft.rfftfreq(WIN, 1/FS)/1e3\n",
    "axq.semilogy(f_kHz, np.abs(np.fft.rfft(win_mat[w_Q]))**2, color=C_Q)\n",
    "axl.semilogy(f_kHz, np.abs(np.fft.rfft(win_mat[w_L]))**2, color=C_L)\n",
    "for ax in (axq, axl):\n",
    "    ax.set_xlim(0, FS/2/1e3)\n",
    "    ax.set_xlabel(\"freq (kHz)\"); ax.set_ylabel(\"power\")\n",
    "\n",
    "# 3 Spectrogram --------------------------------------------------------\n",
    "axq, axl = _axpair(3, \"STFT\")\n",
    "for ax, w, col in ((axq,w_Q,\"magma\"), (axl,w_L,\"magma\")):\n",
    "    f,t,Z = stft(win_mat[w], FS, nperseg=128, noverlap=64)\n",
    "    pcm = ax.pcolormesh(t*1e3, f/1e3, 20*np.log10(np.abs(Z)+1e-12),\n",
    "                        shading='auto', cmap=col)\n",
    "    ax.set_xlabel(\"time (ms)\"); ax.set_ylabel(\"freq (kHz)\")\n",
    "fig.colorbar(pcm, ax=[axq, axl], shrink=0.4, label=\"dB\")\n",
    "\n",
    "# 4 Crest factor -------------------------------------------------------\n",
    "axq, axl = _axpair(4, \"Global crest factor\")\n",
    "axq.barh([0], [crest_Q], color=C_Q); axl.barh([0], [crest_L], color=C_L)\n",
    "axq.set_yticks([]); axl.set_yticks([]);\n",
    "for ax,v in ((axq,crest_Q), (axl,crest_L)):\n",
    "    ax.set_xlim(0, max(crest_Q, crest_L)*1.1)\n",
    "    ax.text(v, 0, f\"{v:.2f}\", va='center', ha='left')\n",
    "\n",
    "# 5 STA / LTA ratio ----------------------------------------------------\n",
    "axq, axl = _axpair(5, \"STA/LTA ratio\")\n",
    "axq.barh([0], [sta_lta_Q], color=C_Q); axl.barh([0], [sta_lta_L], color=C_L)\n",
    "for ax,v in ((axq,sta_lta_Q), (axl,sta_lta_L)):\n",
    "    ax.set_xlim(0, max(sta_lta_Q, sta_lta_L)*1.1)\n",
    "    ax.text(v, 0, f\"{v:.2f}\", va='center', ha='left')\n",
    "axq.set_yticks([]); axl.set_yticks([])\n",
    "\n",
    "# 6 Envelope kurtosis --------------------------------------------------\n",
    "axq, axl = _axpair(6, \"Envelope kurtosis\")\n",
    "axq.barh([0], [kurt_Q], color=C_Q); axl.barh([0], [kurt_L], color=C_L)\n",
    "for ax,v in ((axq,kurt_Q), (axl,kurt_L)):\n",
    "    ax.set_xlim(0, max(kurt_Q, kurt_L)*1.1)\n",
    "    ax.text(v, 0, f\"{v:.1f}\", va='center', ha='left')\n",
    "axq.set_yticks([]); axl.set_yticks([])\n",
    "\n",
    "# 7 Wavelet high‑band energy ratio ------------------------------------\n",
    "axq, axl = _axpair(7, \"Wavelet high‑band energy\")\n",
    "axq.barh([0], [wave_hi_Q], color=C_Q); axl.barh([0], [wave_hi_L], color=C_L)\n",
    "for ax,v in ((axq,wave_hi_Q), (axl,wave_hi_L)):\n",
    "    ax.set_xlim(0, max(wave_hi_Q, wave_hi_L)*1.1)\n",
    "    ax.text(v, 0, f\"{v:.3f}\", va='center', ha='left')\n",
    "axq.set_yticks([]); axl.set_yticks([])\n",
    "\n",
    "# 8 iso16 log‑bars -----------------------------------------------------\n",
    "axq, axl = _axpair(8, \"iso16 features (log‑scale)\")\n",
    "names16 = [\"peak\",\"med\",\"ratio\",\"energy\",\"sta_lta\",\"crest_s\",\"crest_g\",\n",
    "           \"b1\",\"b2\",\"b3\",\"b4\",\"wave_hi\",\"comp\",\"centroid\",\"bw\",\"entropy\"]\n",
    "idx = np.arange(16)\n",
    "for ax, vals, col in ((axq, iso_Q, C_Q), (axl, iso_L, C_L)):\n",
    "    ax.bar(idx, vals, color=col)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xticks(idx); ax.set_xticklabels(names16, rotation=60, ha='right')\n",
    "    ax.set_ylabel(\"value (log10)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ────────── numeric summary table ────────────────────────────────────\n",
    "df = pd.DataFrame({\n",
    "    \"quiet\":      list(iso_Q) + [crest_Q, sta_lta_Q, kurt_Q, wave_hi_Q],\n",
    "    \"lightning\":  list(iso_L) + [crest_L, sta_lta_L, kurt_L, wave_hi_L]\n",
    "}, index = names16 + [\"crest_factor\",\"sta_lta\",\"env_kurtosis\",\"wave_hi\"])\n",
    "df[\"ratio (L/Q)\"] = df[\"lightning\"] / (df[\"quiet\"] + 1e-12)\n",
    "display(df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Evaluation utilities  (full‑feature version) **************************note this still needs make windows check this\n",
    "# ----------------------------------------------------------------------\n",
    "# Defines:\n",
    "#   • EvalConfig          – all timing & tolerance knobs, plus fs\n",
    "#   • evaluate_windowed_model()\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Timing / tolerance configuration\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    win: int        = 1024\n",
    "    hop: int        = 512\n",
    "    fs: int         = 109_375   # Hz\n",
    "    burst_len: int  = None      # default = 0.04 * fs (filled below)\n",
    "    min_stn: int    = 2\n",
    "    tol_win: int    = 0         # dilation (0 = strict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.burst_len is None:\n",
    "            object.__setattr__(self, \"burst_len\", int(0.04 * self.fs))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Main evaluation routine\n",
    "# ----------------------------------------------------------------------\n",
    "def evaluate_windowed_model(\n",
    "    *,\n",
    "    hot: Dict[str, np.ndarray],\n",
    "    stroke_records: List[dict],\n",
    "    quantized: Dict[str, np.ndarray],\n",
    "    station_order: List[str],\n",
    "    win: int | None = None,\n",
    "    hop: int | None = None,\n",
    "    fs:  int | None = None,\n",
    "    burst_len: int | None = None,\n",
    "    min_stn: int | None = None,\n",
    "    tol_win: int | None = None,\n",
    "    cfg: EvalConfig | None = None,\n",
    "    plot: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate per‑station *hot* window flags against ground‑truth strokes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    You may EITHER pass `cfg=EvalConfig(...)` **or** the individual knobs\n",
    "    (`win`, `hop`, `fs`, `burst_len`, …).  Mixing is fine; cfg overrides.\n",
    "    \"\"\"\n",
    "    # ---- resolve config -------------------------------------------------\n",
    "    if cfg is not None:\n",
    "        win       = cfg.win\n",
    "        hop       = cfg.hop\n",
    "        fs        = cfg.fs\n",
    "        burst_len = cfg.burst_len\n",
    "        min_stn   = cfg.min_stn\n",
    "        tol_win   = cfg.tol_win\n",
    "    else:   # legacy / ad‑hoc style\n",
    "        win       = win or 1024\n",
    "        hop       = hop or win // 2\n",
    "        fs        = fs  or 109_375\n",
    "        burst_len = burst_len or int(0.04 * fs)\n",
    "        min_stn   = min_stn or 2\n",
    "        tol_win   = tol_win or 0\n",
    "\n",
    "    n_win = min((len(quantized[s]) - win) // hop + 1 for s in station_order)\n",
    "    if n_win <= 0:\n",
    "        raise RuntimeError(\"No complete windows to score.\")\n",
    "\n",
    "    # ---------- 1) ground truth -----------------------------------------\n",
    "    station_truth = {s: np.zeros(n_win, bool) for s in station_order}\n",
    "    stroke_to_winset = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "    for rec in stroke_records:\n",
    "        s = rec[\"station\"]\n",
    "        if s not in station_order:\n",
    "            continue\n",
    "        s0 = rec[\"sample_idx\"]; s1 = s0 + burst_len - 1\n",
    "        w_first = max(0, (s0 - win + hop) // hop)\n",
    "        w_last  = min(n_win - 1,  s1 // hop)\n",
    "        station_truth[s][w_first:w_last+1] = True\n",
    "        key = (rec[\"event_id\"], rec.get(\"stroke_i\", 0))\n",
    "        stroke_to_winset[key][s].update(range(w_first, w_last+1))\n",
    "\n",
    "    # ---------- 2) predictions (+ optional dilation) --------------------\n",
    "    ker = np.ones(2*tol_win+1, int) if tol_win > 0 else None\n",
    "    hot_pred = {}\n",
    "    for s in station_order:\n",
    "        m = hot[s][:n_win].astype(bool)\n",
    "        if ker is not None:\n",
    "            m = np.convolve(m.astype(int), ker, mode=\"same\") > 0\n",
    "        hot_pred[s] = m\n",
    "\n",
    "    # ---------- 3) per‑station metrics ----------------------------------\n",
    "    def _metrics(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            y_true, y_pred, labels=[False, True]).ravel()\n",
    "        return dict(\n",
    "            TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn),\n",
    "            P=float(f\"{precision_score(y_true,y_pred,zero_division=0):.3f}\"),\n",
    "            R=float(f\"{recall_score   (y_true,y_pred,zero_division=0):.3f}\"),\n",
    "            F1=float(f\"{f1_score      (y_true,y_pred,zero_division=0):.3f}\")\n",
    "        )\n",
    "    station_metrics = {s: _metrics(station_truth[s], hot_pred[s])\n",
    "                       for s in station_order}\n",
    "\n",
    "    # ---------- 4) stroke‑level / network metrics -----------------------\n",
    "    tp = fn = 0; matched = set()\n",
    "    stroke_hits = {}\n",
    "    for key, per_stn in stroke_to_winset.items():\n",
    "        hits = sum(any(hot_pred[s][w] for w in ws)\n",
    "                   for s, ws in per_stn.items())\n",
    "        stroke_hits[key] = hits\n",
    "        if hits >= min_stn:\n",
    "            tp += 1; matched.update(*per_stn.values())\n",
    "        else:\n",
    "            fn += 1\n",
    "\n",
    "    counts = sum(hot_pred[s] for s in station_order)\n",
    "    fp_mask = counts >= min_stn\n",
    "    fp = 0; in_cl = False\n",
    "    fp_windows = []\n",
    "    for w, flag in enumerate(fp_mask):\n",
    "        if flag and w not in matched and not in_cl:\n",
    "            fp += 1; fp_windows.append(w); in_cl = True\n",
    "        elif not flag: in_cl = False\n",
    "\n",
    "    P_net = tp/(tp+fp) if tp+fp else 0\n",
    "    R_net = tp/(tp+fn) if tp+fn else 0\n",
    "    F1_net= 2*P_net*R_net/(P_net+R_net) if P_net+R_net else 0\n",
    "    network_metrics = dict(\n",
    "        TP=tp, FP=fp, FN=fn, TN=0,\n",
    "        P=float(f\"{P_net:.3f}\"), R=float(f\"{R_net:.3f}\"), F1=float(f\"{F1_net:.3f}\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- 5) visualisation ---------------------------------------\n",
    "    if plot:\n",
    "        # timeline\n",
    "        fig, ax = plt.subplots(figsize=(12, 2.5))\n",
    "        ax.set_facecolor(\"#202020\"); fig.patch.set_facecolor(\"#202020\")\n",
    "        for key, per_stn in stroke_to_winset.items():\n",
    "            x = min(min(ws) for ws in per_stn.values())\n",
    "            col = \"#ffffff\" if stroke_hits[key] >= min_stn else \"#ffa500\"\n",
    "            ax.axvline(x, color=col, lw=1.4)\n",
    "        for w in fp_windows:\n",
    "            ax.axvline(w, color=\"#ff1744\", lw=1.4)\n",
    "        ax.set_yticks([]); ax.set_xlabel(\"Window index\")\n",
    "        ax.set_title(\"Stroke timeline   –   white TP   |   amber FN   |   red FP\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        # waveform snippets (first TP / FP / FN)\n",
    "        def _first(cand): return min(cand, key=lambda t:t[1]) if cand else None\n",
    "        tp_cand = [(s, np.flatnonzero(station_truth[s] & hot_pred[s]))\n",
    "                   for s in station_order]\n",
    "        tp_win  = _first([(s, arr[0]) for s, arr in tp_cand if arr.size])\n",
    "\n",
    "        fp_cand = [(s, np.flatnonzero(~station_truth[s] & hot_pred[s]))\n",
    "                   for s in station_order]\n",
    "        fp_win  = _first([(s, arr[0]) for s, arr in fp_cand if arr.size])\n",
    "\n",
    "        fn_win = None\n",
    "        if fn:\n",
    "            for key, per_stn in stroke_to_winset.items():\n",
    "                if stroke_hits[key] < min_stn:\n",
    "                    st = next(iter(per_stn))\n",
    "                    fn_win = (st, min(per_stn[st])); break\n",
    "\n",
    "        if any([tp_win, fp_win, fn_win]):\n",
    "            fig2, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=True)\n",
    "            t_axis = np.arange(win) / fs * 1e3  # ms\n",
    "            def _panel(ax, win_tup, title_ok, title_none):\n",
    "                if win_tup:\n",
    "                    s,w = win_tup; beg=w*hop\n",
    "                    ax.plot(t_axis, quantized[s][beg:beg+win])\n",
    "                    ax.set_title(title_ok.format(s,w))\n",
    "                else:\n",
    "                    ax.set_title(title_none)\n",
    "            _panel(axes[0], tp_win, \"First TP — {} win#{}\", \"No true positives\")\n",
    "            _panel(axes[1], fp_win, \"First FP — {} win#{}\", \"No false positives\")\n",
    "            _panel(axes[2], fn_win, \"First FN — {} win#{}\", \"No false negatives\")\n",
    "            axes[-1].set_xlabel(\"Time in window (ms)\")\n",
    "            plt.tight_layout(); plt.show()\n",
    "\n",
    "    return station_metrics, network_metrics, n_win\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "#  evaluate_windowed_model  –  audited / corrected version -------------------checxk this also confused on versions but need make windows\n",
    "# ======================================================================\n",
    "\n",
    "\"\"\"\n",
    "================================================================================\n",
    "window_eval.py  — Explaining the window‑based lightning‑detection evaluation\n",
    "================================================================================\n",
    "This cell provides all the scaffolding needed to score a **window‑level**\n",
    "binary detector (e.g. an Isolation‑Forest over 1024‑sample frames) against\n",
    "*stroke‑level* ground truth from the synthetic storm generator.  Lightning\n",
    "bursts last ≈ 40 ms, far longer than a single 9.36 ms window\n",
    "(1024 / 109 375 Hz), so the evaluation must reconcile two time‑scales:\n",
    "\n",
    "* **Station scale**   Was each *station* “hot” in every window that overlaps\n",
    "  a true burst? → classic confusion‑matrix metrics.\n",
    "* **Network scale**   Did the *network* (≥ `min_stn` stations) fire within\n",
    "  the burst’s window set? → stroke‑level precision/recall.\n",
    "\n",
    "The cell therefore exposes **two layers of metrics** so research\n",
    "questions about *within‑station sensitivity* versus *network localisation\n",
    "success* can be answered independently.\n",
    "\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "1. Configuration — `EvalConfig`\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "| Field          | Meaning                                                      |\n",
    "| -------------- | ------------------------------------------------------------ |\n",
    "| `win`, `hop`   | Frame length/stride in samples; must match the detector.     |\n",
    "| `fs`           | Sampling rate (Hz) so burst length can be converted to samples.|\n",
    "| `burst_len`    | Samples occupied by an ideal 40 ms lightning burst. If left |\n",
    "|                | at `None` it is auto‑filled via `0.04 * fs`.                 |\n",
    "| `min_stn`      | Minimum number of stations that must fire for a stroke to   |\n",
    "|                | count as detected on the **network** level.                 |\n",
    "| `tol_win`      | Window *dilation* radius. Setting > 0 allows a prediction to|\n",
    "|                | drift ±`tol_win` frames yet still be credited (latency       |\n",
    "|                | tolerance).                                                 |\n",
    "\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "2. Main routine — `evaluate_windowed_model(...)`\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "Call signature is intentionally flexible:\n",
    "\n",
    "    evaluate_windowed_model(\n",
    "        hot=per_station_binary_flags,\n",
    "        stroke_records=list_of_dicts,\n",
    "        quantized=raw_int16_waveforms,\n",
    "        station_order=[...],\n",
    "        cfg=EvalConfig(...),   # OR supply win/hop/etc individually\n",
    "        plot=True\n",
    "    )\n",
    "\n",
    "Step‑by‑step logic\n",
    "------------------\n",
    "1. **Resolve parameters** – explicit kwargs override defaults unless a\n",
    "   complete `cfg` object is supplied.\n",
    "\n",
    "2. **Ground‑truth window mask (per station)**\n",
    "   * For every stroke record we convert its sample range\n",
    "     `[sample_idx : sample_idx + burst_len)` to **window indices**, mark\n",
    "     those windows `True`, and record which windows belong to which\n",
    "     stroke: `stroke_to_winset[event_id, stroke_i][station] = {…}`.\n",
    "\n",
    "3. **Prediction mask (with optional dilation)**\n",
    "   * `hot[s]` is your model’s Boolean output per window.\n",
    "   * If `tol_win > 0`, we convolve with a ones‑kernel to expand each hit\n",
    "     by ±`tol_win` windows – compensates for sub‑window mis‑alignment.\n",
    "\n",
    "4. **Station‑level metrics**\n",
    "   We compute a full `confusion_matrix` per station and derive\n",
    "   precision‑recall‑F1.  **Subtlety**: scores are *window counts*, not\n",
    "   stroke counts. A station may be perfect on short bursts yet accrue\n",
    "   false positives on long tails.\n",
    "\n",
    "5. **Network / stroke‑level metrics**\n",
    "   * A stroke is a **true positive** (TP) if ≥ `min_stn` distinct\n",
    "     stations fire in *any* overlapping window.\n",
    "   * Contiguous windows where ≥ `min_stn` stations fire **without**\n",
    "     overlapping any ground‑truth stroke constitute **false positives**\n",
    "     (FP). We count clusters, not individual windows, to avoid inflating\n",
    "     FP if a detector merely “buzzes” for a few frames.\n",
    "   * False negatives (FN) are strokes missed by the required quorum.\n",
    "   * TN\n",
    "   Precision/recall/F1 are derived from these stroke‑counts.\n",
    "\n",
    "6. **Visualisation (optional)**\n",
    "   * **Timeline plot** – vertical lines: white = TP, amber = FN,\n",
    "     red = FP window cluster. Helps eyeball temporal patterns.\n",
    "   * **Waveform triptych** – first TP / FP / FN window snippets with raw\n",
    "     ADC data, so you can diagnose why a detector mis‑triggered\n",
    "     (poor SNR, multipath echo, etc.).  Shows only if such cases exist.\n",
    "\n",
    "Return value\n",
    "------------\n",
    "``station_metrics``   Dict [station → {TP,FP,FN,TN,P,R,F1}]\n",
    "``network_metrics``  Dict {TP,FP,FN,TN,P,R,F1}\n",
    "``n_win``            Analysed window count\n",
    "\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "Metric subtleties to highlight in the lecture\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "* **Window granularity** – Station metrics treat every 9 ms frame\n",
    "  equally; a single burst spans ~4 windows, so TP counts can exceed the\n",
    "  number of physical strokes. This is fine when you want temporal\n",
    "  coverage but can over‑reward jittery detectors.\n",
    "\n",
    "* **Stroke granularity** – Network metrics collapse all windows belonging\n",
    "  to one stroke into a single decision, aligning with operational needs\n",
    "  (one alert per lightning discharge).  Discuss how `min_stn` trades\n",
    "  sensitivity for localisation confidence.\n",
    "\n",
    "* **Tolerance dilation** – Increasing `tol_win` rewards detectors that\n",
    "  fire “near” the burst but penalises ones that are precise in time yet\n",
    "  sometimes late; ideal when teaching about latency–accuracy trade‑offs.\n",
    "\n",
    "* **FP clustering** – A detector that locks high for 100 windows is\n",
    "  still only one FP at stroke‑level, preventing ridiculous precision\n",
    "  penalties.  Conversely, station metrics would mark **every** window FP,\n",
    "  illustrating why both layers are needed.\n",
    "\n",
    "* **Recall ceiling** – If only two stations have usable SNR at *far*\n",
    "  ranges, setting `min_stn = 3` makes 100 % recall impossible. Use this\n",
    "  knob live to show its impact on the ROC curve.\n",
    "\n",
    "Suggested demo narrative\n",
    "------------------------\n",
    "1. Run a detector on *near* vs *far* storms; compare station vs network\n",
    "   F1.\n",
    "2. Show how increasing `tol_win` from 0 to 1 boosts station recall but\n",
    "   barely moves network recall (bursts are already multi‑window).\n",
    "3. Lower `min_stn` from 3 to 2 → higher recall, lower precision: cue\n",
    "   discussion on deployment priorities (public safety vs false alarms).\n",
    "\n",
    "Take‑away\n",
    "---------\n",
    "This evaluator makes explicit the often‑hidden assumptions about\n",
    "*how* lightning is deemed “detected” in a windowed paradigm.  By\n",
    "exposing configurable tolerances and two granularities of truth, it\n",
    "serves both algorithm development (optimise window scores) and practical\n",
    "operations (will the network raise the right alerts?).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "window_eval.py — Window‑based lightning‑detection evaluation utilities.\n",
    "\n",
    "Provides:\n",
    "- :class:`EvalConfig` — parameters for windowing, sampling, tolerances, quorum.\n",
    "- :func:`pretty_metrics` — helper to round floats in metrics dicts.\n",
    "- :func:`evaluate_windowed_model` — compute station window metrics and\n",
    "  network stroke metrics from per‑station Boolean predictions.\n",
    "\n",
    "Notes:\n",
    "- Station metrics operate on **window counts** (TP/FP/FN/TN by frame).\n",
    "- Network metrics operate on **stroke counts** with a station quorum\n",
    "  (``min_stn``). FP are counted as clusters of predicted network activity.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing      import Dict, List\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from scipy.ndimage import binary_dilation\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Small, typed config object\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for window‑level evaluation.\n",
    "\n",
    "    Attributes:\n",
    "        win: Window length in samples (must match the detector).\n",
    "        hop: Hop size in samples (must match the detector).\n",
    "        fs: Sampling rate in Hz.\n",
    "        burst_len: Lightning burst length in samples. If ``None``, set to\n",
    "            ``int(0.04 * fs)`` (≈ 40 ms).\n",
    "        min_stn: Minimum number of stations required to count a stroke as a\n",
    "            *network‑level* detection.\n",
    "        tol_win: Integer tolerance for **prediction dilation** in windows. If\n",
    "            ``> 0``, hot predictions are expanded by ±``tol_win`` frames.\n",
    "    \"\"\"\n",
    "    win:        int = 1024\n",
    "    hop:        int = 512\n",
    "    fs:         int = 109_375\n",
    "    burst_len:  int | None = None   # default‑ed in __post_init__\n",
    "    min_stn:    int = 2\n",
    "    tol_win:    int = 0             # prediction dilation (0 = strict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Fill in derived defaults after initialisation.\"\"\"\n",
    "        if self.burst_len is None:\n",
    "            self.burst_len = int(0.04 * self.fs)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Pretty‑printer for metric dictionaries\n",
    "# ---------------------------------------------------------------\n",
    "def pretty_metrics(d: dict, ndigits: int = 3) -> dict:\n",
    "    \"\"\"Return a new dict with float values rounded to ``ndigits``.\n",
    "\n",
    "    Non‑float values are copied unchanged (e.g., integer counts).\n",
    "\n",
    "    Args:\n",
    "        d: Metrics dictionary (e.g., from evaluation).\n",
    "        ndigits: Number of decimal places for floats.\n",
    "\n",
    "    Returns:\n",
    "        A shallow copy with rounded float values.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        k: (round(v, ndigits) if isinstance(v, float) else v)\n",
    "        for k, v in d.items()\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Main routine\n",
    "# ----------------------------------------------------------------------\n",
    "def evaluate_windowed_model(\n",
    "    *,\n",
    "    hot: Dict[str, np.ndarray],\n",
    "    stroke_records: List[dict],\n",
    "    quantized: Dict[str, np.ndarray],\n",
    "    station_order: List[str],\n",
    "    cfg: EvalConfig | None = None,\n",
    "    # legacy individual args (optional)\n",
    "    win: int | None = None, hop: int | None = None, fs: int | None = None,\n",
    "    burst_len: int | None = None, min_stn: int | None = None,\n",
    "    tol_win: int | None = None,\n",
    "    plot: bool = True,\n",
    "):\n",
    "    \"\"\"Evaluate a windowed detector at station‑ and network‑level granularity.\n",
    "\n",
    "    Args:\n",
    "        hot: Mapping ``station → (n_win_pred,) bool`` model outputs (per window).\n",
    "        stroke_records: List of dicts with at least:\n",
    "            ``{'station', 'event_id', 'stroke_i', 'sample_idx', 'window_idx'}``.\n",
    "            Only ``station`` and ``sample_idx`` are required for window truth;\n",
    "            ``event_id``/``stroke_i`` are used to group windows per stroke.\n",
    "        quantized: Mapping ``station → 1‑D np.ndarray`` raw waveforms (int16).\n",
    "        station_order: Ordered list of station names to score/aggregate.\n",
    "        cfg: Optional :class:`EvalConfig`. If omitted, individual parameters\n",
    "            below are used to build one.\n",
    "        win, hop, fs, burst_len, min_stn, tol_win: Optional overrides used\n",
    "            only when ``cfg`` is ``None``.\n",
    "        plot: If ``True``, draw a compact timeline visual.\n",
    "\n",
    "    Returns:\n",
    "        station_metrics: ``dict(station → {TP,FP,FN,TN,P,R,F1})`` computed\n",
    "            over windows.\n",
    "        network_metrics: ``{TP,FP,FN,TN,P,R,F1}`` computed over strokes/clusters.\n",
    "        n_win: Number of analysed windows per station (after alignment).\n",
    "\n",
    "    Notes:\n",
    "        - **Ground‑truth to windows.** A stroke occupies samples\n",
    "          ``[s0, s1]``, where ``s1 = s0 + burst_len - 1``. Window ``k`` covers\n",
    "          samples ``[k·hop, k·hop + win - 1]``. Overlap occurs when\n",
    "          ``k·hop ≤ s1`` and ``k·hop + win - 1 ≥ s0``. Algebra yields:\n",
    "          ``w_first = ceil((s0 + 1 - win)/hop)``, ``w_last = floor(s1/hop)``.\n",
    "        - **Dilation.** If ``tol_win > 0``, predictions are expanded by\n",
    "          ±``tol_win`` windows using a Boolean convolution (binary dilation).\n",
    "        - **Network FP definition.** FP are counted as *clusters* of aggregated\n",
    "          windows (≥ ``min_stn`` stations hot) that do **not** overlap any\n",
    "          windows from **matched** strokes. By construction, aggregated activity\n",
    "          overlapping *missed* strokes is counted as FP (strict).\n",
    "    \"\"\"\n",
    "    # ---------- consolidate configuration -----------------------------\n",
    "    if cfg is None:\n",
    "        cfg = EvalConfig(\n",
    "            win        = win        or 1024,\n",
    "            hop        = hop        or (win or 1024)//2,\n",
    "            fs         = fs         or 109_375,\n",
    "            burst_len  = burst_len,\n",
    "            min_stn    = min_stn    or 2,\n",
    "            tol_win    = tol_win    or 0\n",
    "        )\n",
    "\n",
    "    win, hop, fs   = cfg.win, cfg.hop, cfg.fs\n",
    "    burst_len      = cfg.burst_len\n",
    "    min_stn        = cfg.min_stn\n",
    "    tol_win        = cfg.tol_win\n",
    "\n",
    "    # ---------- input‑sanity ------------------------------------------\n",
    "    n_win = min((len(quantized[s]) - win)//hop + 1 for s in station_order)\n",
    "    if n_win <= 0:\n",
    "        raise RuntimeError(\"No complete windows to score\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) ground‑truth window masks   (per station)\n",
    "    # ------------------------------------------------------------------\n",
    "    station_truth = {s: np.zeros(n_win, bool) for s in station_order}\n",
    "    stroke_to_winset = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "    for rec in stroke_records:\n",
    "        stn = rec[\"station\"]\n",
    "        if stn not in station_order:\n",
    "            continue\n",
    "        s0   = rec[\"sample_idx\"]\n",
    "        s1   = s0 + burst_len - 1\n",
    "        w_first = int(np.ceil( (s0+1 - win) / hop ))      # inclusive\n",
    "        w_first = max(0, w_first)\n",
    "        w_last  = int(np.floor( s1 / hop ))               # inclusive\n",
    "        w_last  = min(n_win - 1, w_last)\n",
    "\n",
    "        station_truth[stn][w_first:w_last+1] = True\n",
    "        key = (rec[\"event_id\"], rec.get(\"stroke_i\", 0))\n",
    "        stroke_to_winset[key][stn].update(range(w_first, w_last+1))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) predictions  (+ optional dilation)\n",
    "    # ------------------------------------------------------------------\n",
    "    hot_pred = {}\n",
    "    if tol_win > 0:\n",
    "        ker = np.ones(2*tol_win+1, bool)\n",
    "        for s in station_order:\n",
    "            hot_pred[s] = binary_dilation(hot[s][:n_win], ker)\n",
    "    else:\n",
    "        hot_pred = {s: hot[s][:n_win].astype(bool) for s in station_order}\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) per‑station window metrics\n",
    "    # ------------------------------------------------------------------\n",
    "    def _m(y_true, y_pred):\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            y_true, y_pred, labels=[False, True]).ravel()\n",
    "        return dict(TP=int(tp), FP=int(fp), FN=int(fn), TN=int(tn),\n",
    "                    P=float(precision_score(y_true,y_pred,zero_division=0)),\n",
    "                    R=float(recall_score   (y_true,y_pred,zero_division=0)),\n",
    "                    F1=float(f1_score      (y_true,y_pred,zero_division=0)))\n",
    "    station_metrics = {s: _m(station_truth[s], hot_pred[s]) for s in station_order}\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) stroke‑level / network metrics\n",
    "    # ------------------------------------------------------------------\n",
    "    tp_st, fn_st = 0, 0\n",
    "    matched_win  = set()\n",
    "\n",
    "    for key, per_stn in stroke_to_winset.items():\n",
    "        hit_cnt = sum(any(hot_pred[s][w] for w in ws) for s, ws in per_stn.items())\n",
    "        if hit_cnt >= min_stn:\n",
    "            tp_st += 1\n",
    "            for ws in per_stn.values(): matched_win.update(ws)\n",
    "        else:\n",
    "            fn_st += 1\n",
    "\n",
    "    # network FP = clusters of windows where ≥ min_stn stations fire\n",
    "    agg_mask = sum(hot_pred[s] for s in station_order) >= min_stn\n",
    "    fp_clust = []\n",
    "    in_cl = False\n",
    "    for w, flag in enumerate(agg_mask):\n",
    "        if flag and w not in matched_win and not in_cl:\n",
    "            fp_clust.append(w); in_cl = True\n",
    "        elif not flag: in_cl = False\n",
    "    fp_st = len(fp_clust)\n",
    "\n",
    "    P_net = tp_st/(tp_st+fp_st) if tp_st+fp_st else 0\n",
    "    R_net = tp_st/(tp_st+fn_st) if tp_st+fn_st else 0\n",
    "    F1_net= (2*P_net*R_net)/(P_net+R_net) if P_net+R_net else 0\n",
    "\n",
    "    # TN = windows with no truth & no prediction\n",
    "    truth_any = np.zeros(n_win, bool)\n",
    "    for s in station_order: truth_any |= station_truth[s]\n",
    "    pred_any  = agg_mask\n",
    "    TN_net    = int(np.sum(~truth_any & ~pred_any))\n",
    "\n",
    "    network_metrics = dict(TP=tp_st, FP=fp_st, FN=fn_st, TN=TN_net,\n",
    "                           P=float(P_net), R=float(R_net), F1=float(F1_net))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) minimal visuals\n",
    "    # ------------------------------------------------------------------\n",
    "    if plot:\n",
    "        # timeline\n",
    "        fig, ax = plt.subplots(figsize=(12, 2.4))\n",
    "        ax.set_facecolor(\"#202020\"); fig.patch.set_facecolor(\"#202020\")\n",
    "        for key, per_stn in stroke_to_winset.items():\n",
    "            x = min(min(ws) for ws in per_stn.values())\n",
    "            col = \"#ffffff\" if (len(per_stn)>=min_stn) else \"#ffff00\"\n",
    "            ax.axvline(x, color=col, lw=1.3)\n",
    "        for w in fp_clust: ax.axvline(w, color=\"#ff1744\", lw=1.3)\n",
    "        ax.set_yticks([]); ax.set_xlabel(\"Window index\")\n",
    "        ax.set_title(\"white TP  |  amber FN  |  red FP (network view)\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    return station_metrics, network_metrics, n_win\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Hilbert‑envelope baseline – refactored (fits + evaluates)\n",
    "# ----------------------------------------------------------------------\n",
    "# Uses:\n",
    "#   • storm_data         (already produced by StormGenerator.generate())\n",
    "#   • FeatureExtractor   (registry cell, with \"hilbert_peak\")\n",
    "#   • EvalConfig + evaluate_windowed_model  (evaluator cell)\n",
    "# ======================================================================\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "baseline_hilbert_peak.py  — The “hello‑world” lightning detector\n",
    "===============================================================================\n",
    "\n",
    "What this cell does\n",
    "-------------------\n",
    "Turns raw 14‑bit ADC traces from every station into a **single scalar\n",
    "feature per window** – the peak of the Hilbert envelope – then flags the\n",
    "top 0.1 % of windows at each station as “hot”.  A stroke is declared\n",
    "detected if at least two stations are hot in *any* of the windows that\n",
    "overlap the true burst.  Finally, we run the generic\n",
    "`evaluate_windowed_model` to obtain per‑station **and** network metrics.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "0  Shared parameters\n",
    "-------------------------------------------------------------------------------\n",
    "* **`FS`**                 Sampling rate (Hz) – imported from `StormConfig`\n",
    "  so simulation and feature extraction stay in sync.\n",
    "* **`WIN`, `HOP`**         1024 / 512 samples → 9.36 ms frames, 50 %\n",
    "  overlap.  Every future model re‑uses these for fair comparison.\n",
    "* **`PCT_THRESH`**         Station‑specific percentile (99.9 %) rather than\n",
    "  an absolute voltage threshold; adapts automatically to differing gains\n",
    "  and noise floors.\n",
    "* **`MIN_STN`, `TOL_WIN`** Network needs ≥ 2 stations, no timing slack\n",
    "  (`tol_win = 0`) – establishes an *upper bound* on latency precision.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "1  Feature extraction in one line\n",
    "-------------------------------------------------------------------------------\n",
    "```python\n",
    "fx = FeatureExtractor([\"hilbert_peak\"])\n",
    "peaks_dict, n_win = fx.transform(storm_data.quantised,\n",
    "                                 win=WIN, hop=HOP, fs=FS)\n",
    "Data structure   peaks_dict[station] → np.ndarray(shape=(n_win,1))\n",
    "This mapping is the contract every detector must meet, whether it uses\n",
    "one feature (here), sixteen (Isolation‑Forest), or thousands (CNN).\n",
    "\n",
    "Why the Hilbert envelope?\n",
    "Mathematical view   For a real signal x(t) the analytic signal\n",
    "x_a(t) = x(t) + j · H{x(t)} (where H{·} is the Hilbert transform)\n",
    "has a complex modulus\n",
    "|x_a(t)| = sqrt(x² + H{x}²) which tracks the instantaneous\n",
    "amplitude independent of phase. Taking the maximum over a 9 ms\n",
    "window gives a robust scalar that survives polarity flips and minimises\n",
    "smearing compared with plain RMS.\n",
    "\n",
    "Intuitive analogy   Imagine an AM radio: the envelope detector\n",
    "rectifies the RF waveform and low‑pass filters it, producing the audio\n",
    "loudness curve. Lightning sferics are similarly “buried” in a noisy\n",
    "carrier; the envelope rise during a stroke sticks out like a sore thumb.\n",
    "\n",
    "Key properties\n",
    "\n",
    "Polarity‑agnostic – Positive or negative spikes yield the same\n",
    "envelope peak.\n",
    "\n",
    "Scale‑aware – Peaks follow 1/r attenuation with distance, so a\n",
    "percentile threshold adapts per station.\n",
    "\n",
    "Computationally cheap – FFT‑based Hilbert on a 1024‑sample frame is\n",
    "negligible compared with model inference time.\n",
    "\n",
    "2  Per‑station thresholding\n",
    "For each station nm:\n",
    "\n",
    "python\n",
    "Copy\n",
    "thr  = np.percentile(pk, 99.9)\n",
    "mask = pk > thr          # Boolean window flags\n",
    "Percentile choice is arbitrary but repeatable; tweak during the lecture\n",
    "to illustrate precision‑recall trade‑offs.\n",
    "\n",
    "3  Evaluation\n",
    "We reuse the generic evaluation routine:\n",
    "\n",
    "python\n",
    "Copy\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                      burst_len=int(0.04*FS),\n",
    "                      min_stn=MIN_STN, tol_win=TOL_WIN)\n",
    "\n",
    "station_m, net_m, _ = evaluate_windowed_model(\n",
    "    hot=hot, stroke_records=storm_data.stroke_records,\n",
    "    quantized=storm_data.quantised, station_order=STN,\n",
    "    cfg=eval_cfg, plot=True)\n",
    "Because every future detector will output the same hot dictionary, no\n",
    "changes to the evaluator are needed when you swap in Isolation‑Forest or\n",
    "a neural net.\n",
    "\n",
    "4  Interpreting the output\n",
    "Per‑station metrics   Reveal whether certain stations (e.g. coastal,\n",
    "low SNR) cause most false alarms.\n",
    "Network metrics       Answer the operational question: “Will the\n",
    "system alert on a real stroke without crying wolf?”\n",
    "\n",
    "Stepping‑stone to richer models\n",
    "Isolation‑Forest – Call FeatureExtractor([\"iso16\"]), train a\n",
    "one‑class forest on normal windows, produce hot from its anomaly\n",
    "scores, pass to the same evaluator.\n",
    "\n",
    "Neural nets – Stack per‑station features into\n",
    "X.shape = (n_win, n_stn, n_feat) and train a CNN/LSTM to output\n",
    "window probabilities; threshold these to form hot.\n",
    "\n",
    "Take‑away\n",
    "By fixing data plumbing first – one mapping, one evaluator – we unlock\n",
    "rapid experimentation while ensuring every new model is directly\n",
    "comparable to this Hilbert‑peak baseline.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Baseline windowed detector using Hilbert‑envelope peak + percentile gate.\n",
    "\n",
    "    Parameters:\n",
    "        pct_thresh: Percentile in [0, 100] used per station to mark “hot” windows\n",
    "            (e.g., ``99.9`` means top 0.1 % of windows at each station are hot).\n",
    "\n",
    "    Instance attributes:\n",
    "        pct_thresh (float): Configured percentile threshold.\n",
    "        fx (FeatureExtractor): Pre‑configured extractor for ``\"hilbert_peak\"``.\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"Compute per‑station “hot” window masks using percentile thresholding.\n",
    "\n",
    "        Args:\n",
    "            quantised: Mapping ``station → 1‑D int/float array`` of equal length.\n",
    "            win: Window length in samples (e.g., 1024).\n",
    "            hop: Hop size in samples (e.g., 512).\n",
    "            fs: Sampling rate in Hz (passed through to the feature extractor).\n",
    "\n",
    "        Returns:\n",
    "            hot: ``dict(station → bool[n_win])`` where ``True`` marks a hot window.\n",
    "            n_win: Common number of analysed windows across stations.\n",
    "\n",
    "        Notes:\n",
    "            - The underlying :class:`FeatureExtractor` truncates all station\n",
    "              matrices to the **minimum** window count so shapes align.\n",
    "            - Thresholding is performed **independently per station** using\n",
    "              the configured percentile, providing gain/noise floor invariance.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ------------ 0.  Shared values ---------------------------------------\n",
    "FS   = cfg.fs                       # sample‑rate from the very same config\n",
    "STN  = list(storm_data.quantised)   # deterministic station order\n",
    "WIN, HOP = 1024, 512\n",
    "PCT_THRESH = 99.9\n",
    "MIN_STN    = 2\n",
    "TOL_WIN    = 0\n",
    "\n",
    "# ------------ 1.  Compute Hilbert‑peak per window ----------------------\n",
    "fx   = FeatureExtractor([\"hilbert_peak\"])\n",
    "peaks_dict, n_win = fx.transform(storm_data.quantised,\n",
    "                                 win=WIN, hop=HOP, fs=FS)\n",
    "\n",
    "# ------------ 2.  Per‑station thresholding ----------------------------\n",
    "hot = {}\n",
    "print(\"Per‑station thresholds & flagged windows:\")\n",
    "for nm in STN:\n",
    "    pk = peaks_dict[nm][:, 0]                     # single‑column feature\n",
    "    thr = np.percentile(pk, PCT_THRESH)\n",
    "    mask = pk > thr\n",
    "    hot[nm] = mask\n",
    "    print(f\" {nm}: thr={thr:7.2f}, flagged={mask.sum():5d} / {n_win}\")\n",
    "\n",
    "# ------------ 3.  Evaluation ------------------------------------------\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                      burst_len=int(0.04*FS),\n",
    "                      min_stn=MIN_STN, tol_win=TOL_WIN)\n",
    "\n",
    "station_m, net_m, _ = evaluate_windowed_model(\n",
    "    hot            = hot,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True          # timeline + waveform panels\n",
    ")\n",
    "\n",
    "# ------------ 4.  Summary printout ------------------------------------\n",
    "print(\"\\nPer‑station metrics (strict timeline):\")\n",
    "for nm, m in station_m.items():\n",
    "    print(f\" {nm}: TP={m['TP']:3d} FP={m['FP']:3d} \"\n",
    "          f\"FN={m['FN']:3d}  P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m, ndigits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "#  Normalised‑Compression‑Distance (NCD) detector – modular edition\n",
    "# ---------------------------------------------------------------------\n",
    "#  • No duplicate helpers: relies on `StormGenerator`, feature registry,\n",
    "#    `EvalConfig`, and `evaluate_windowed_model` that you already ran.\n",
    "#  • Supports four encoder variants ('bits', 'raw', 'norm', 'tanh').\n",
    "#  • Prints one‑line network scores for each variant; keeps full\n",
    "#    per‑station metrics in a dictionary you can inspect afterwards.\n",
    "# =====================================================================\n",
    "\"\"\"\n",
    "================================================================================\n",
    "ncd_detector.py  — Compression‑based anomaly detector with four encodings\n",
    "================================================================================\n",
    "This cell demonstrates how the **Normalised Compression Distance (NCD)**\n",
    "can be used as a *model‑free* lightning detector.  NCD treats each\n",
    "window as a “string”; if adding that string to a *baseline* string\n",
    "fails to compress well, the window is deemed anomalous.\n",
    "\n",
    "Pipeline overview\n",
    "-----------------\n",
    "    raw ADC → window view (stride trick) → encode to bytes →\n",
    "    bzip2 compress → NCD to baseline → adaptive threshold →\n",
    "    per‑station Boolean mask → network evaluation\n",
    "\n",
    "Key tunables (top of the script)\n",
    "--------------------------------\n",
    "* `WIN`, `HOP`      Window geometry – must match earlier evaluators.\n",
    "* `BASE_PCT`        Choose baseline among the *k* most compressible\n",
    "  windows (usually quiet background).\n",
    "* `PCT_THR`, `Z_SIGMA`  Two adaptive thresholds on NCD; we take the min\n",
    "  (robust yet sensitive).\n",
    "* `ENCODINGS`       Four data representations evaluated in one sweep:\n",
    "  `\"bits\"`, `\"raw\"`, `\"norm\"`, `\"tanh\"`.\n",
    "* `MIN_STN`         Network quorum (≥ 2 stations).\n",
    "\n",
    "What is NCD?\n",
    "------------\n",
    "For two byte‑strings *x* and *y*:\n",
    "``NCD(x,y) = (C(xy) − min(C(x),C(y))) / max(C(x),C(y))``\n",
    "where `C(·)` is compressed size (here **bzip2** at level 9).\n",
    "*Intuition*: identical windows compress together as well as alone →\n",
    "NCD ≈ 0; dissimilar windows inflate archive size → NCD → 1.\n",
    "\n",
    "Window encoding variants\n",
    "------------------------\n",
    "| Encoding | What it stores | Why it might help | Weaknesses |\n",
    "| -------- | -------------- | ----------------- | ----------- |\n",
    "| `\"bits\"` | **Sign of first differences**, packed 8‑to‑a‑byte | Removes amplitude, keeps *shape* and zero‑crossing rhythm; compresses extremely well, so small deviations stand out | Ignores magnitude – a low‑SNR burst with subtle shape change may be missed |\n",
    "| `\"raw\"`  | 16‑bit ADC samples verbatim | Retains full information | Compression dominated by Gaussian noise; gain drifts change size more than sferic bursts, causing false positives |\n",
    "| `\"norm\"` | Z‑score‑normalised samples, re‑quantised to ±32767 | Equalises gain across windows; compression focuses on waveform pattern | Still 16‑bit; if noise variance shifts mid‑storm, normalisation can over‑whiten bursts |\n",
    "| `\"tanh\"` | `tanh(sample/16384)` scaled to int16 | Soft‑clips extreme peaks, highlighting mid‑range oscillations | Harsh clipping may erase small pre‑cursor pulses important for early warning |\n",
    "\n",
    "Detector internals\n",
    "------------------\n",
    "1. **`_win_view` (stride trick)** – zero‑copy matrix view; mandatory\n",
    "   because NCD is CPU‑heavy.\n",
    "2. **Pass 1 – compressed size scan**\n",
    "   * Memoised `_c_size` prevents re‑compressing identical payloads.\n",
    "   * Picks a *baseline window* = median of the `BASE_PCT` % smallest\n",
    "     windows – usually atmospheric hum.\n",
    "3. **Pass 2 – NCD vector** against the fixed baseline.\n",
    "4. **Adaptive threshold** – window is *hot* if\n",
    "   `NCD > min(percentile, μ + Z·σ)`.\n",
    "5. **Meta cache** – Detector stores `thr`, `ncd`, and Boolean mask per\n",
    "   station for introspection.\n",
    "\n",
    "Efficiency hacks\n",
    "----------------\n",
    "* **Bzip2 compression** is pure‑Python but C‑optimised; still the slow\n",
    "  part, hence tqdm progress bars.\n",
    "* Memoisation with `@lru_cache` avoids recompressing duplicate blocks\n",
    "  (common with `\"bits\"` encoding).\n",
    "* `ENCODINGS` loop lets you benchmark variants *within* one run and pick\n",
    "  the best F1 via `results`.\n",
    "\n",
    "Integration with the common evaluator\n",
    "-------------------------------------\n",
    "The output of `.predict()` is the familiar\n",
    "`Dict[station → np.ndarray bool shape (n_win,)]`.  We therefore call\n",
    "`evaluate_windowed_model` unchanged, aligning this compression‑based\n",
    "detector with Hilbert‑threshold, Isolation‑Forest, and any forthcoming\n",
    "CNN model.\n",
    "\n",
    "When to use NCD\n",
    "---------------\n",
    "* **Pros**  Model‑free, no training data, robust to frequency drift.\n",
    "* **Cons**  CPU‑intensive; sensitive to choice of encoding and baseline;\n",
    "  raw encoding struggles under variable gain noise.\n",
    "\n",
    "Live‑demo ideas\n",
    "---------------\n",
    "1. Show how `\"bits\"` yields F1 ≈ 0.6 at SNR +0 dB while `\"raw\"` collapses\n",
    "   to < 0.2 (compression dominated by noise).\n",
    "2. Visualise NCD time‑series; observe bursts spike for `\"bits\"` but not\n",
    "   for `\"raw\"`.\n",
    "3. Toggle the `BASE_PCT` knob from 5 % to 1 %; demonstrate risk of\n",
    "   choosing a noisy window as baseline.\n",
    "\n",
    "Bottom line\n",
    "-----------\n",
    "NCD acts as a **strong unsupervised baseline** whose performance hinges\n",
    "on clever byte‑encodings.  The `\"bits\"` variant exemplifies how a tiny\n",
    "representation can amplify structural change, but its amplitude blindness\n",
    "makes it miss quiet sferics.  Normalised or tanh encodings trade off\n",
    "these extremes, yet the compression metric remains noisier than\n",
    "feature‑based ML.  Use NCD to sanity‑check ML detectors or as a\n",
    "stand‑in when labelled data are scarce.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, bz2, math\n",
    "from functools   import lru_cache\n",
    "from tqdm.auto   import trange\n",
    "from typing      import Dict, List\n",
    "\n",
    "# ---------------- user‑tunable knobs ---------------------------------\n",
    "WIN        = 1024               # window length  (samples)\n",
    "HOP        = 512                # hop size       (samples)\n",
    "BASE_PCT   = 5                  # % smallest windows  → baseline pool\n",
    "PCT_THR    = 98.5               # percentile threshold on NCD\n",
    "Z_SIGMA    = 3.5                # μ + Z·σ alternative threshold\n",
    "ENCODINGS  = (\"bits\", \"raw\", \"norm\", \"tanh\")  # which variants to evaluate\n",
    "MIN_STN    = 2                  # network requirement\n",
    "RAW        = storm_data.quantised              # from StormGenerator cell\n",
    "STN        = list(RAW)                           # station order\n",
    "FS         = cfg.fs\n",
    "BURST_LEN  = int(0.04 * FS)      # 40 ms burst for evaluator\n",
    "\n",
    "# ---------------- helpers (no duplicates elsewhere) ------------------\n",
    "@lru_cache(maxsize=None)\n",
    "def _c_size(payload: bytes) -> int:\n",
    "    \"\"\"Compressed size (bzip2, level 9) with memoisation.\"\"\"\n",
    "    return len(bz2.compress(payload, 9))\n",
    "\n",
    "def _win_view(sig: np.ndarray, W=WIN, H=HOP) -> np.ndarray:\n",
    "    \"\"\"Zero-copy, overlap-add window view using strides.\n",
    "\n",
    "    Args:\n",
    "        sig: 1-D array of samples (e.g., int16).\n",
    "        W:   Window length (samples).\n",
    "        H:   Hop size (samples).\n",
    "\n",
    "    Returns:\n",
    "        View of shape (n_windows, W). Caller must not write into it.\n",
    "    \"\"\"\n",
    "    n = (len(sig) - W) // H + 1\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        sig,\n",
    "        shape=(n, W),\n",
    "        strides=(sig.strides[0]*H, sig.strides[0])\n",
    "    )\n",
    "\n",
    "def _enc_bits(arr):\n",
    "    \"\"\"Sign of first-differences, packed 8-to-a-byte.\"\"\"\n",
    "    diff = np.diff(arr.astype(np.int16), prepend=arr[0])\n",
    "    return np.packbits((diff > 0).astype(np.uint8)).tobytes()\n",
    "\n",
    "def _enc_raw(arr):\n",
    "    \"\"\"Verbatim int16 payload.\"\"\"\n",
    "    return arr.astype(np.int16).tobytes()\n",
    "\n",
    "def _enc_norm(arr):\n",
    "    \"\"\"Z-score normalised to int16 range.\"\"\"\n",
    "    a = (arr.astype(np.float32) - arr.mean()) / (arr.std(ddof=0) + 1e-9)\n",
    "    return np.clip(a * 32767, -32767, 32767).astype(np.int16).tobytes()\n",
    "\n",
    "def _enc_tanh(arr):\n",
    "    \"\"\"Soft-clipped via tanh, then quantised to int16.\"\"\"\n",
    "    return (np.tanh(arr.astype(np.float32) / 16384) * 32767)\\\n",
    "             .astype(np.int16).tobytes()\n",
    "\n",
    "ENC_FUN = dict(bits=_enc_bits, raw=_enc_raw,\n",
    "               norm=_enc_norm, tanh=_enc_tanh)\n",
    "\n",
    "# ---------------- detector (stateless, one encoder) ------------------\n",
    "class NcdDetector:\n",
    "    \"\"\"Normalised-Compression-Distance (NCD) windowed detector.\n",
    "\n",
    "    Initialises with a fixed window geometry and encoding. The **fit** step\n",
    "    selects a per-station baseline window (among the most compressible) and\n",
    "    stores an adaptive threshold; **predict** then scores windows against the\n",
    "    stored baseline and returns Boolean “hot” masks.\n",
    "\n",
    "    Parameters:\n",
    "        encoding: One of {'bits', 'raw', 'norm', 'tanh'}.\n",
    "        win: Window length in samples (e.g., 1024).\n",
    "        hop: Hop size in samples (e.g., 512).\n",
    "        base_pct: Percent of *smallest compressed* windows to consider when\n",
    "            choosing the baseline; the median within this pool is used.\n",
    "        pct_thr: Percentile threshold applied to the NCD vector.\n",
    "        z_sigma: Z for μ+Z·σ alternative threshold.\n",
    "\n",
    "    Instance attributes:\n",
    "        encoding (str): Selected encoding variant.\n",
    "        win (int): Window length in samples.\n",
    "        hop (int): Hop size in samples.\n",
    "        base_pct (float): Baseline pool percentage.\n",
    "        pct_thr (float): NCD percentile threshold.\n",
    "        z_sigma (float): Z for μ+Z·σ.\n",
    "        meta (dict): Per-station metadata captured at fit-time:\n",
    "            {station → {'thr': float, 'base_bytes': bytes, 'Cb': int, 'n_win': int}}\n",
    "    \"\"\"\n",
    "    def __init__(self, *, encoding: str):\n",
    "        if encoding not in ENC_FUN:\n",
    "            raise ValueError(f\"Unknown encoding '{encoding}'\")\n",
    "        self.enc = encoding\n",
    "        \"\"\"Selected window encoding ('bits', 'raw', 'norm', 'tanh').\"\"\"\n",
    "        \"\"\"Window length in samples.\"\"\"\n",
    "        \"\"\"Hop size in samples.\"\"\"\n",
    "        \"\"\"Percent of smallest-compressed windows used as baseline pool.\"\"\"\n",
    "        \"\"\"Percentile threshold applied to per-station NCD vectors.\"\"\"\n",
    "        \"\"\"Z for the μ+Z·σ alternative threshold.\"\"\"\n",
    "        \"\"\"Per-station fit-time artefacts: baseline bytes, Cb, threshold, n_win.\"\"\"\n",
    "\n",
    "        self.meta: Dict[str, Dict] = {}   # station → info\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    def fit(self, raw: Dict[str, np.ndarray]):\n",
    "        \"\"\"Learn a per-station baseline and threshold from raw ADC traces.\n",
    "\n",
    "        For each station:\n",
    "            1) Build a stride-based window view (win, hop).\n",
    "            2) Compute compressed sizes C(w) for all windows under the chosen encoding.\n",
    "            3) Select baseline window as the median of the lowest `base_pct` % by C(w).\n",
    "            4) Compute NCD to the baseline for each window:\n",
    "               NCD = (C(w ⊕ b) - min(C(w), C(b))) / max(C(w), C(b))\n",
    "            5) Set threshold `thr = min(percentile(NCD), μ + z_sigma·σ)`.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping station → 1-D arrays (same length per station is typical).\n",
    "            verbose: If True, show progress bars per station; otherwise, silent.\n",
    "\n",
    "        Returns:\n",
    "            self (for chaining).\n",
    "        \"\"\"\n",
    "        enc_fun = ENC_FUN[self.enc]\n",
    "        for nm, sig in raw.items():\n",
    "            wmat   = _win_view(sig)           # (n_win, WIN)\n",
    "            n_win  = len(wmat)\n",
    "            # pass‑1: compressed sizes\n",
    "            Cs = np.empty(n_win, np.uint32)\n",
    "            for i in trange(n_win, desc=f\"{nm} size\", leave=False):\n",
    "                Cs[i] = _c_size(enc_fun(wmat[i]))\n",
    "\n",
    "            # choose baseline window  =  median of lowest BASE_PCT %\n",
    "            k         = max(1, int(BASE_PCT/100 * n_win))\n",
    "            low_k     = np.argpartition(Cs, k)[:k]\n",
    "            base_idx  = low_k[np.argsort(Cs[low_k])[k//2]]\n",
    "            base_b    = enc_fun(wmat[base_idx])\n",
    "            Cb        = _c_size(base_b)\n",
    "\n",
    "            # pass‑2: NCD to baseline\n",
    "            ncd = np.empty(n_win, np.float32)\n",
    "            for i in trange(n_win, desc=f\"{nm} NCD\", leave=False):\n",
    "                Cw = Cs[i]\n",
    "                ncd[i] = (_c_size(enc_fun(wmat[i]) + base_b) -\n",
    "                          min(Cw, Cb)) / max(Cw, Cb)\n",
    "\n",
    "            # adaptive threshold (percentile  ∧  μ+Zσ)\n",
    "            mu, var = ncd.mean(), ncd.var()\n",
    "            thr = min(np.percentile(ncd, PCT_THR), mu + Z_SIGMA * math.sqrt(var))\n",
    "            self.meta[nm] = dict(hot=(ncd > thr),\n",
    "                                 thr=thr, ncd=ncd)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    def predict(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Score windows against stored baselines and return Boolean hot masks.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping station → 1-D arrays to score (may differ from fit data).\n",
    "\n",
    "        Returns:\n",
    "            hot: dict(station → bool[n_win]) with `True` where NCD > stored `thr`.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: if called before `fit()` or if a station was not seen in fit.\n",
    "        \"\"\"\n",
    "        if not self.meta:\n",
    "            raise RuntimeError(\"call .fit() first\")\n",
    "        return {nm: info[\"hot\"] for nm, info in self.meta.items()}\n",
    "\n",
    "# ---------------- run all variants & evaluate ------------------------\n",
    "results = {}\n",
    "print(f\"\\nAnalysing {len(_win_view(next(iter(RAW.values())))):,} windows per station\")\n",
    "for enc in ENCODINGS:\n",
    "    print(f\"\\n================ variant: {enc} ================\")\n",
    "    det = NcdDetector(encoding=enc)\n",
    "    det.fit(RAW)\n",
    "    hot = det.predict()\n",
    "\n",
    "    station_m, net_m, _ = evaluate_windowed_model(\n",
    "        hot            = hot,\n",
    "        stroke_records = storm_data.stroke_records,\n",
    "        quantized      = RAW,\n",
    "        station_order  = STN,\n",
    "        cfg            = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                                    burst_len=BURST_LEN,\n",
    "                                    min_stn=MIN_STN, tol_win=0),\n",
    "        plot           = True                   # change to True if desired\n",
    "    )\n",
    "    results[enc] = dict(station=station_m, network=net_m)\n",
    "    print(f\"Network  P={net_m['P']:.3f}  R={net_m['R']:.3f}  F1={net_m['F1']:.3f}\")\n",
    "\n",
    "best = max(results, key=lambda e: results[e]['network']['F1'])\n",
    "print(f\"\\n>>> Best encoder on this run: {best}  \"\n",
    "      f\"(F1={results[best]['network']['F1']:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Isolation‑Forest detector – fit, predict, evaluate\n",
    "# ----------------------------------------------------------------------\n",
    "# Depends on:\n",
    "#   • storm_data, cfg              (from StormGenerator cell)\n",
    "#   • FeatureExtractor, make_windows (registry cell, now incl. \"iso13\")\n",
    "#   • EvalConfig, evaluate_windowed_model (evaluator cell)\n",
    "# ======================================================================\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "iso_forest_model.py  — Per‑station Isolation‑Forest lightning detector\n",
    "===============================================================================\n",
    "\n",
    "Goal\n",
    "----\n",
    "Move beyond single‑threshold baselines by training an **unsupervised\n",
    "tree ensemble** on a 13‑dimension feature vector (`iso13`) and flagging\n",
    "windows that lie in sparse regions of feature space.\n",
    "\n",
    "Data plumbing recap\n",
    "-------------------\n",
    "* **Input from generator**     `Dict[station → 1‑D int16 ADC]`\n",
    "* **Windowing**                `WIN = 1024`, `HOP = 512`\n",
    "* **Feature extraction**       `FeatureExtractor([\"iso13\"])` returns\n",
    "  `Dict[station → np.ndarray (n_win, 13)]`\n",
    "* **Output mask**              `hot[station] → np.ndarray bool (n_win,)`\n",
    "\n",
    "`IsoForestModel` respects this contract so it can plug straight into\n",
    "`evaluate_windowed_model` without code changes.\n",
    "\n",
    "iso13 feature vector (per window)\n",
    "---------------------------------\n",
    "| Id | Name            | Intuition                                                |\n",
    "| -- | --------------- | -------------------------------------------------------- |\n",
    "| 0  | **peak_env**    | Largest Hilbert envelope value; spikes from sferics      |\n",
    "| 1  | **med_env**     | Envelope median; robust background level                 |\n",
    "| 2  | **ratio_env**   | Crest factor (peak/median)                               |\n",
    "| 3  | **energy**      | Σ x², overall energy                                     |\n",
    "| 4  | **sta_lta**     | Short‑/long envelope ratio; seismology staple            |\n",
    "| 5  | **crest_short** | Crest factor in centre ⅛; local sharpness                |\n",
    "| 6  | **crest_global**| peak_env / RMS; classic vibration metric                 |\n",
    "| 7‑10| **FFT quartiles** | Fractional power 0‑25 %, … 75‑100 %                   |\n",
    "| 11 | **wave_hi**     | db4 wavelet high‑band energy ratio                       |\n",
    "| 12 | **comp_ratio**  | bzip2 compression ratio; proxy for complexity            |\n",
    "\n",
    "Tree ensembles love orthogonal cues; iso13 mixes *time*, *frequency*,\n",
    "and *complexity* so anomalies are isolated quickly.\n",
    "\n",
    "Isolation‑Forest refresher\n",
    "--------------------------\n",
    "* Builds **n binary trees** by recursively *randomly* selecting a\n",
    "  feature and split‑value until each sample is isolated.\n",
    "* **Path length** = number of splits to isolate a point.  Anomalies\n",
    "  (rare, in sparse regions) require *fewer* splits → shorter paths.\n",
    "* Average path length across the forest is converted to an *anomaly\n",
    "  score*; here we deem a window *hot* if the score is in the worst\n",
    "  `contamination` fraction (`0.001` = 0.1 %).\n",
    "* Completely unsupervised – no burst labels needed.\n",
    "\n",
    "Why `StandardScaler`?\n",
    "---------------------\n",
    "Isolation‑Forest is scale‑invariant to axis re‑ordering **but not to\n",
    "feature scaling**: a high‑dynamic‑range feature may dominate split\n",
    "selection.  We therefore z‑score each station’s feature matrix before\n",
    "fitting and transform again at predict time.\n",
    "\n",
    "Training & prediction flow\n",
    "--------------------------\n",
    "```text\n",
    "raw ADC (per station)\n",
    "      ↓ window + iso13\n",
    "X ∊ ℝ^{n_win × 13}\n",
    "      ↓ StandardScaler.fit_transform\n",
    "      ↓ IsolationForest.fit\n",
    "      ↓ predict → {-1, +1}\n",
    "      ↓ equality check (-1 ⇒ anomaly) → Boolean hot mask\n",
    "\n",
    "Design decisions & intuition\n",
    "Per‑station models   Lightning amplitude varies with range and\n",
    "ground conductivity; station‑specific forests learn local background\n",
    "manifolds.\n",
    "\n",
    "contamination = 0.001   Matches the idea that true strokes are\n",
    "rare (< 0.1 % of windows). Tune during the lecture to show precision–\n",
    "recall curves.\n",
    "\n",
    "n_trees = 150   Enough diversity for stable path‑length estimates;\n",
    "beyond ~200 returns diminish while training time grows.\n",
    "\n",
    "Evaluating results\n",
    "After .fit() we immediately apply the model to the same data – a\n",
    "sanity check phase. Downstream:\n",
    "\n",
    "python\n",
    "Copy\n",
    "station_m3, net_m3, _ = evaluate_windowed_model(...)\n",
    "Because the evaluator expects the same Boolean masks, the only code that\n",
    "changed from the Hilbert baseline is inside the model.\n",
    "\n",
    "Typical observations (demo suggestions)\n",
    "Station precision increases versus percentile threshold because the\n",
    "forest jointly considers 13 axes; false positives caused by loud hum\n",
    "but flat spectrum are down‑voted.\n",
    "\n",
    "Recall can drop at very high SNR – bursts that look “too clean” may\n",
    "resemble baseline windows in iso13 space; illustrate with feature\n",
    "scatter plots.\n",
    "\n",
    "Network F1 often beats NCD but lags a tuned CNN; positions this model\n",
    "as a solid middle ground: no labels, decent performance.\n",
    "\n",
    "Take‑away\n",
    "Isolation‑Forest gives us a data‑driven detector that honours the\n",
    "established IO pipeline. By decoupling feature engineering (iso13)\n",
    "from model logic, we can iterate separately: swap in iso16 or AE10\n",
    "features, or raise contamination in harsher storms, all while keeping\n",
    "evaluation code frozen.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict\n",
    "\n",
    "class IsoForestModel:\n",
    "    def __init__(self, *, win=1024, hop=512,\n",
    "                 contamination=0.001, n_trees=150, random_state=42):\n",
    "        self.win = win; self.hop = hop\n",
    "        self.contam = contamination\n",
    "        self.n_trees = n_trees\n",
    "        self.rs = random_state\n",
    "        self.fx = FeatureExtractor([\"iso13\"])\n",
    "        self.models: Dict[str, tuple] = {}  # station → (scaler, iso)\n",
    "\n",
    "    # ---------------- fit per‑station models --------------------------\n",
    "    def fit(self, raw: Dict[str, np.ndarray], fs: int, verbose=True):\n",
    "        feats, _ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        for nm, X in feats.items():\n",
    "            scaler = StandardScaler().fit(X)\n",
    "            iso = IsolationForest(\n",
    "                n_estimators = self.n_trees,\n",
    "                contamination= self.contam,\n",
    "                random_state = self.rs\n",
    "            ).fit(scaler.transform(X))\n",
    "            self.models[nm] = (scaler, iso)\n",
    "            if verbose:\n",
    "                n_hot = int((iso.predict(scaler.transform(X)) == -1).sum())\n",
    "                print(f\"{nm}: windows flagged = {n_hot:5d} / {len(X)} \"\n",
    "                      f\"(contam={self.contam:.3%})\")\n",
    "\n",
    "    # ---------------- predict hot masks -------------------------------\n",
    "    def predict(self, raw: Dict[str, np.ndarray], fs: int) -> Dict[str, np.ndarray]:\n",
    "        feats, _ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        hot = {}\n",
    "        for nm, X in feats.items():\n",
    "            scaler, iso = self.models[nm]\n",
    "            hot[nm] = iso.predict(scaler.transform(X)) == -1\n",
    "        return hot\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Train & predict\n",
    "# ----------------------------------------------------------------------\n",
    "WIN, HOP = 1024, 512\n",
    "model3 = IsoForestModel(win=WIN, hop=HOP,\n",
    "                        contamination=0.001, n_trees=150)\n",
    "\n",
    "print(\"\\n▶ Fitting Isolation‑Forest detectors …\")\n",
    "model3.fit(storm_data.quantised, fs=cfg.fs)\n",
    "\n",
    "print(\"\\n▶ Predicting hot windows …\")\n",
    "hot3 = model3.predict(storm_data.quantised, fs=cfg.fs)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Evaluate\n",
    "# ----------------------------------------------------------------------\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=cfg.fs,\n",
    "                      burst_len=int(0.04*cfg.fs),\n",
    "                      min_stn=2, tol_win=0)\n",
    "\n",
    "station_m3, net_m3, n_win3 = evaluate_windowed_model(\n",
    "    hot            = hot3,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = list(storm_data.quantised),\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Summary\n",
    "# ----------------------------------------------------------------------\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win3:,}) ——\")\n",
    "for nm, m in station_m3.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "#print(net_m3)\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m3, ndigits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Isolation‑Forest detector – clean notebook cell\n",
    "# ----------------------------------------------------------------------\n",
    "# Needs:\n",
    "#   • storm_data, cfg                         (StormGenerator cell)\n",
    "#   • FeatureExtractor, make_windows, \"iso16\" (registry now extended)\n",
    "#   • EvalConfig, evaluate_windowed_model     (evaluator cell)\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Example (for reference):\n",
    "#\n",
    "#   from lightning_sim.features.basic import FeatureExtractor\n",
    "#   fx = FeatureExtractor([\"iso16\"])\n",
    "#   model = IsoForestModel(fx, win=1024, hop=512,\n",
    "#                          contamination=0.0015, n_trees=150, random_state=42)\n",
    "#   model.fit(storm_data.quantised, fs=cfg.fs, verbose=True)\n",
    "#   hot = model.predict(storm_data.quantised, fs=cfg.fs)\n",
    "#\n",
    "#   # Evaluate with the common evaluator:\n",
    "#   from lightning_sim.eval.window_eval import EvalConfig, evaluate_windowed_model\n",
    "#   station_m, net_m, _ = evaluate_windowed_model(\n",
    "#       hot=hot,\n",
    "#       stroke_records=storm_data.stroke_records,\n",
    "#       quantized=storm_data.quantised,\n",
    "#       station_order=list(storm_data.quantised),\n",
    "#       cfg=EvalConfig(win=model.win, hop=model.hop, fs=cfg.fs, min_stn=2, tol_win=0),\n",
    "#       plot=True,\n",
    "#   )\n",
    "#   print(\"Network:\", net_m)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from typing import Dict\n",
    "\n",
    "class IsoForestModel:\n",
    "    \"\"\"Per‑station Isolation‑Forest detector over windowed feature blocks.\n",
    "\n",
    "    Initialisation injects a pre‑configured :class:`FeatureExtractor` (`fx`)\n",
    "    so the model does not hard‑code feature selection. For the baseline,\n",
    "    pass an extractor created as: ``FeatureExtractor([\"iso16\"])``.\n",
    "\n",
    "    Parameters:\n",
    "        fx: Feature extractor with a ``transform(raw, win, hop, fs)`` method\n",
    "            returning ``(features_by_station: Dict[str, np.ndarray], n_win)``.\n",
    "        win: Window length in samples (e.g., 1024).\n",
    "        hop: Hop size in samples (e.g., 512).\n",
    "        contamination: Expected fraction of anomalies (Isolation‑Forest `contamination`).\n",
    "        n_trees: Number of trees (`n_estimators`) in the forest.\n",
    "        random_state: RNG seed for reproducibility.\n",
    "\n",
    "    Instance attributes:\n",
    "        fx: Injected feature extractor (used in both fit and predict).\n",
    "        win, hop: Window geometry in samples.\n",
    "        contam: Global contamination fraction applied to all stations.\n",
    "        n_trees, rs: Model capacity / randomness controls.\n",
    "        models: Mapping ``station → (scaler, forest)`` learned in :meth:`fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, win=1024, hop=512,\n",
    "                 contamination=0.001, n_trees=150,\n",
    "                 contam_map: Dict[str,float]|None=None,\n",
    "                 random_state=42):\n",
    "        self.win = win; self.hop = hop\n",
    "        self.contam = contamination\n",
    "        self.contam_map = contam_map or {}\n",
    "        self.n_trees = n_trees; self.rs = random_state\n",
    "        self.fx = FeatureExtractor([\"iso16\"])\n",
    "        self.models: Dict[str, tuple] = {}  # station → (scaler, forest)\n",
    "\n",
    "    # ---- training -----------------------------------------------------\n",
    "    def fit(self, raw: Dict[str,np.ndarray], fs:int, verbose=True):\n",
    "        \"\"\"Fit one scaled Isolation‑Forest per station.\n",
    "\n",
    "        Steps per station:\n",
    "            1) Extract features via the injected `fx` using ``win/hop/fs``.\n",
    "            2) Fit a :class:`RobustScaler` (median/IQR) to mitigate heavy tails.\n",
    "            3) Fit an :class:`IsolationForest` on the scaled features.\n",
    "            4) Cache ``(scaler, forest)`` in :attr:`models`.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping ``station → 1‑D array`` of equal length (int or float).\n",
    "            fs: Sampling rate in Hz (forwarded to the feature extractor).\n",
    "            verbose: If ``True``, print per‑station flagged‑window counts.\n",
    "\n",
    "        Returns:\n",
    "            self (to allow chaining).\n",
    "        \"\"\"\n",
    "        feats, _ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        for nm, X in feats.items():\n",
    "            scaler = RobustScaler().fit(X)\n",
    "            contam = self.contam_map.get(nm, self.contam)\n",
    "            iso = IsolationForest(\n",
    "                    n_estimators = self.n_trees,\n",
    "                    contamination= contam,\n",
    "                    random_state = self.rs\n",
    "                 ).fit(scaler.transform(X))\n",
    "            self.models[nm] = (scaler, iso, contam)\n",
    "            if verbose:\n",
    "                n_hot = int((iso.predict(scaler.transform(X))==-1).sum())\n",
    "                print(f\"{nm}: windows flagged = {n_hot:5d} / {len(X)}  \"\n",
    "                      f\"(contam {contam:.3%})\")\n",
    "\n",
    "    # ---- inference ----------------------------------------------------\n",
    "    def predict(self, raw: Dict[str,np.ndarray], fs:int) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"Infer per‑window anomaly flags per station (Boolean masks).\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping ``station → 1‑D array`` to score.\n",
    "            fs: Sampling rate in Hz (forwarded to the feature extractor).\n",
    "\n",
    "        Returns:\n",
    "            Dict ``station → (n_windows_aligned,) bool`` where ``True`` marks\n",
    "            a window predicted as anomalous (Isolation‑Forest label ``-1``).\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If a station has not been fitted yet.\n",
    "        \"\"\"\n",
    "        feats, _ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        out = {}\n",
    "        for nm, X in feats.items():\n",
    "            scaler, iso, _ = self.models[nm]\n",
    "            out[nm] = iso.predict(scaler.transform(X)) == -1\n",
    "        return out\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Instantiate & fit\n",
    "# ----------------------------------------------------------------------\n",
    "WIN, HOP = 1024, 512\n",
    "contam_override = {\"LON\": 0.001, \"LER\": 0.001,  # replicate inline logic\n",
    "                   # others default to 0.0015 (=1.5×BASE_CONT)\n",
    "                  }\n",
    "model_iso = IsoForestModel(win=WIN, hop=HOP,\n",
    "                           contamination=0.0015,\n",
    "                           contam_map=contam_override,\n",
    "                           n_trees=150)\n",
    "\n",
    "print(\"\\n▶ Fitting Isolation‑Forest detectors …\")\n",
    "model_iso.fit(storm_data.quantised, fs=cfg.fs)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Predict\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"\\n▶ Predicting hot windows …\")\n",
    "hot_iso = model_iso.predict(storm_data.quantised, fs=cfg.fs)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Evaluate\n",
    "# ----------------------------------------------------------------------\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=cfg.fs,\n",
    "                      burst_len=int(0.04*cfg.fs),\n",
    "                      min_stn=2, tol_win=0)\n",
    "\n",
    "station_m_iso, net_m_iso, n_win_iso = evaluate_windowed_model(\n",
    "    hot            = hot_iso,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = list(storm_data.quantised),\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Summary\n",
    "# ----------------------------------------------------------------------\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_iso:,}) ——\")\n",
    "for nm,m in station_m_iso.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "#print(net_m_iso)\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m_iso, ndigits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "#  Model‑5 · Extended Isolation‑Forest (isotree) – recall‑boosted\n",
    "# ----------------------------------------------------------------------\n",
    "# Needs (already defined in previous cells):\n",
    "#   • storm_data, cfg\n",
    "#   • FeatureExtractor with \"iso16\"\n",
    "#   • EvalConfig, evaluate_windowed_model\n",
    "# ======================================================================\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "extended_iso_forest.py  — Adaptive, depth‑aware Isolation‑Forest detector\n",
    "===============================================================================\n",
    "\n",
    "Why another Isolation‑Forest?\n",
    "-----------------------------\n",
    "The earlier `IsoForestModel` was a **quick baseline**: 13‑D feature\n",
    "vector, fixed contamination, scikit‑learn implementation.  This cell\n",
    "raises the bar by introducing:\n",
    "\n",
    "1. **Richer features**     `iso16` adds three spectral‑shape moments\n",
    "   (centroid, bandwidth, entropy).\n",
    "2. **`isotree` backend**   C++ implementation exposes *average depth*\n",
    "   directly and scales to hundreds of trees in seconds.\n",
    "3. **Station‑wise contamination grid search**   Auto‑tunes the anomaly\n",
    "   fraction between 0.1 % and 0.7 % per station.\n",
    "4. **Extreme‑depth rescue**   After grid tuning we *still* flag the\n",
    "   deepest 0.05 % of windows (`EXTREME_Q`) in case rare events were\n",
    "   hidden by noise during training.\n",
    "5. **RobustScaler** instead of StandardScaler — immune to outliers\n",
    "   when storms vary wildly in amplitude.\n",
    "\n",
    "Data flow (unchanged plumbing)\n",
    "------------------------------\n",
    "``raw ADC → windows (1024/512) → iso16 features (n_win, 16)\n",
    "        → RobustScaler → isotree Isolation‑Forest\n",
    "        → average_depth vector  → depth < thr ⇒ hot mask\n",
    "\n",
    "Output mask shape and evaluator call are identical to every previous\n",
    "model, preserving end‑to‑end comparability.\n",
    "\n",
    "Key hyper‑parameters\n",
    "GRID_CONT = np.linspace(0.001, 0.007, 5)   Five candidate anomaly\n",
    "rates per station; we pick the first whose mask retains at least\n",
    "0.1 % of windows.\n",
    "\n",
    "EXTREME_Q = 99.95   Rescues the deepest 0.05 % of windows even if the\n",
    "grid threshold missed them (“red‑button” catch‑all).\n",
    "\n",
    "ndim = X.shape[1]‑1   Feature‑subsampling in isotree — roughly one\n",
    "less than total dims — proven to improve diversity without over‑fitting.\n",
    "\n",
    "prob_pick_avg_gain = prob_pick_pooled_gain = 0   Disable greedy\n",
    "gain heuristics so each tree remains truly random; helps anomaly\n",
    "generalisation.\n",
    "\n",
    "Handling isotree API drift\n",
    "_avg_depth() tries multiple keyword syntaxes\n",
    "(type=\"avg_depth\" vs output_type=\"avg_depth\") and falls back to\n",
    "-predict() for older versions. Ensures the notebook survives library\n",
    "upgrades without edits – highlight this as defensive coding.\n",
    "\n",
    "Differences versus the basic Isolation‑Forest\n",
    "Aspect\tBasic IsoForestModel (sklearn)\tExtendedIsoForest\n",
    "Feature set\tiso13 (13 dims)\tiso16 (adds spectral shape)\n",
    "Scaling\tStandardScaler (μ, σ)\tRobustScaler (median, IQR)\n",
    "Backend\tscikit‑learn\tisotree (faster, depth API)\n",
    "Contamination\tFixed 0.1 %\tGrid‑search 0.1–0.7 % + rescue\n",
    "Threshold basis\tForest’s internal flag\tAverage depth quantile\n",
    "Multi‑threading\tLimited\tAll CPU cores − 1\n",
    "Extra safeguard\t–\tExtreme‑depth rescue\n",
    "\n",
    "Intuition behind “average depth”\n",
    "In Isolation‑Forest theory, short path = anomaly. isotree returns\n",
    "average depth where larger means more normal (trees are\n",
    "inverted). We therefore flag windows whose depth falls below a\n",
    "station‑specific threshold.\n",
    "\n",
    "Grid search reasoning\n",
    "Station noise floors differ; a fixed contamination either over‑flags\n",
    "quiet stations or under‑flags noisy ones. A simple five‑point grid is\n",
    "cheap (< 1 ms per station) and finds a sweet spot automatically.\n",
    "\n",
    "Extreme‑depth rescue\n",
    "Rare, high‑amplitude sferics can shift feature medians so far that\n",
    "the RobustScaler dampens their anomaly depth. By always adding the\n",
    "deepest 0.05 % windows we catch these “edge‑of‑world” bursts without\n",
    "hurting precision much.\n",
    "\n",
    "Live demo talking points\n",
    "Display depth histogram before and after rescue; rescued windows form\n",
    "a tiny left‑hand tail.\n",
    "\n",
    "Compare per‑station F1 against the basic model; typically improves\n",
    "on low‑SNR coastal stations.\n",
    "\n",
    "Turn off EXTREME_Q (set to 0) and show the network recall drop,\n",
    "illustrating its safety‑net role.\n",
    "\n",
    "Performance note\n",
    "With 11 stations × ~30 000 windows each, 200 isotree trees train in\n",
    "≈ 1 s on a modern laptop thanks to C++ multi‑threading; far quicker than\n",
    "scikit‑learn’s Python loop.\n",
    "\n",
    "Take‑away\n",
    "ExtendedIsoForest exemplifies incremental sophistication: richer\n",
    "features, smarter thresholds, and low‑level performance tweaks – all\n",
    "while honouring the single‑responsibility, plug‑compatible design of the\n",
    "pipeline established from day 1.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "import os, numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from isotree import IsolationForest\n",
    "\n",
    "# ───────────────────────── configuration ──────────────────────────────\n",
    "WIN, HOP   = 1024, 512\n",
    "BASE_CONT  = 0.0015                  # default contamination\n",
    "GRID_CONT  = np.linspace(0.001, 0.007, 5)   # per‑station grid search\n",
    "EXTREME_Q  = 99.95                   # extra 0.05 % “depth rescue”\n",
    "NTREES     = 200\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "\n",
    "# ─────────────────── helper: robust avg‑depth getter ──────────────────\n",
    "def _avg_depth(iso: IsolationForest, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return isotree **average depth** with compatibility across versions.\n",
    "\n",
    "    Tries multiple keyword variants, then falls back to signed score.\n",
    "\n",
    "    Args:\n",
    "        model: Fitted `isotree.IsolationForest`.\n",
    "        X:     2‑D feature matrix.\n",
    "\n",
    "    Returns:\n",
    "        1‑D array where **larger means more normal** (deeper in the trees).\n",
    "    \"\"\"\n",
    "    for kw in ({\"type\": \"avg_depth\"}, {\"output_type\": \"avg_depth\"}):\n",
    "        try:    return iso.predict(X, **kw)          # ≥ 0.6  or  0.4‑0.5\n",
    "        except TypeError:    continue\n",
    "    return -iso.predict(X)                           # plain score (flip sign)\n",
    "\n",
    "# ─────────────────────── model wrapper class ──────────────────────────\n",
    "class ExtendedIsoForest:\n",
    "    \"\"\"Depth‑aware Isolation‑Forest with contamination grid + recall rescue.\n",
    "\n",
    "    Initialises with fixed window geometry and isotree knobs; extracts features\n",
    "    using the provided `fx` (usually `FeatureExtractor([\"iso16\"])`).\n",
    "\n",
    "    Parameters:\n",
    "        fx: Feature extractor with `transform(raw, win, hop, fs)` → `(by_station, n_win)`.\n",
    "            If omitted, the caller must inject one before calling `fit()`.\n",
    "        win: Window length in samples (e.g., 1024).\n",
    "        hop: Hop size in samples (e.g., 512).\n",
    "        n_trees: Number of trees in isotree forest.\n",
    "        grid_cont: Iterable of small fractions (e.g., 0.001..0.007) used as\n",
    "            candidate **depth quantiles** per station; the first that retains\n",
    "            at least `min_flag_frac` of windows is selected.\n",
    "        min_flag_frac: Minimum fraction of windows a station must flag at grid\n",
    "            selection time (guards against thresholds that flag nothing).\n",
    "        extreme_q: Percentile in (0, 100]; we always add windows below the\n",
    "            `(100 - extreme_q)` percentile of **training depth** as a recall\n",
    "            safety‑net. Set to `0` to disable.\n",
    "        ndim_sub: Feature sub‑sampling for isotree (`None` to use `X.shape[1]-1`).\n",
    "        sample_size: Isotree sample size per tree (e.g., `\"auto\"`).\n",
    "        prob_pick_avg_gain / prob_pick_pooled_gain: Set to 0 to keep trees random.\n",
    "        nthreads: Worker threads; default `max(os.cpu_count()-1, 1)`.\n",
    "        random_seed: RNG seed for isotree.\n",
    "\n",
    "    Instance attributes:\n",
    "        fx: Feature extractor (instance attribute).\n",
    "        win, hop, n_trees, grid_cont, min_flag_frac, extreme_q, … : configuration.\n",
    "        mods: `{station -> (scaler, iso_model, depth_train)}`.\n",
    "        thr:  `{station -> float}` station‑specific **depth** threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, win:int=WIN, hop:int=HOP,\n",
    "                 base_cont:float=BASE_CONT, n_trees:int=NTREES,\n",
    "                 extreme_q:float=EXTREME_Q):\n",
    "        self.win=win; self.hop=hop\n",
    "        self.base_cont=base_cont; self.n_trees=n_trees\n",
    "        self.extreme_q=extreme_q\n",
    "        self.fx   = FeatureExtractor([\"iso16\"])\n",
    "        self.mods : Dict[str, tuple] = {}   # nm → (scaler, iso, depth)\n",
    "        self.thr  : Dict[str, float] = {}   # chosen depth threshold per stn\n",
    "\n",
    "    # ---------- train --------------------------------------------------\n",
    "    def fit(self, raw:Dict[str,np.ndarray], fs:int, verbose:bool=True):\n",
    "        \"\"\"Fit one isotree model per station and select depth thresholds.\n",
    "\n",
    "        Steps per station:\n",
    "            1) Extract features via `fx.transform(raw, win, hop, fs)`.\n",
    "            2) Fit :class:`RobustScaler` and transform features.\n",
    "            3) Fit `isotree.IsolationForest` with configured parameters.\n",
    "            4) Compute **average depth** on training windows (larger = normal).\n",
    "            5) Choose depth threshold via the first grid fraction whose mask\n",
    "               retains at least `min_flag_frac` of windows; otherwise use the\n",
    "               last grid value.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping `station → 1‑D array` (equal length per station recommended).\n",
    "            fs: Sampling rate in Hz (passed to the feature extractor).\n",
    "            verbose: If True, print per‑station threshold and flagged counts.\n",
    "\n",
    "        Returns:\n",
    "            self (to allow chaining).\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: if `fx` is missing.\n",
    "        \"\"\"\n",
    "        feats,_ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        for nm,X in feats.items():\n",
    "            scaler = RobustScaler().fit(X)\n",
    "            Xs     = scaler.transform(X)\n",
    "            iso    = IsolationForest(\n",
    "                         ntrees       = self.n_trees,\n",
    "                         sample_size  = 'auto',\n",
    "                         ndim         = X.shape[1]-1,\n",
    "                         prob_pick_avg_gain   = 0,\n",
    "                         prob_pick_pooled_gain= 0,\n",
    "                         nthreads     = max(os.cpu_count()-1,1),\n",
    "                         random_seed  = 42\n",
    "                     ).fit(Xs)\n",
    "            depth  = _avg_depth(iso, Xs)    # larger → normal\n",
    "            # ---- grid search contamination ---------------------------\n",
    "            for c in GRID_CONT:\n",
    "                thr = np.quantile(depth, c)     # smaller = more anomalous\n",
    "                mask= depth < thr\n",
    "                if mask.sum() >= 0.001*len(depth):\n",
    "                    self.thr[nm] = thr; break   # first acceptable\n",
    "            else:\n",
    "                self.thr[nm] = np.quantile(depth, GRID_CONT[-1])\n",
    "            self.mods[nm] = (scaler, iso, depth)\n",
    "            if verbose:\n",
    "                n_hot = int((depth < self.thr[nm]).sum())\n",
    "                print(f\"{nm}: thr depth {self.thr[nm]:.4f}  \"\n",
    "                      f\"flagged {n_hot}/{len(depth)}  \"\n",
    "                      f\"(grid c≈{(depth<self.thr[nm]).mean():.3%})\")\n",
    "\n",
    "    # ---------- inference ----------------------------------------------\n",
    "    def predict(self, raw:Dict[str,np.ndarray], fs:int) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"Predict Boolean “hot window” masks per station.\n",
    "\n",
    "        Combines the grid‑selected depth threshold with an optional\n",
    "        **extreme‑depth rescue** tail from training to boost recall.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping `station → 1‑D array` to score.\n",
    "            fs: Sampling rate in Hz (passed to the feature extractor).\n",
    "\n",
    "        Returns:\n",
    "            Dict `station → (n_windows_aligned,) bool`.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: if a station has not been fitted.\n",
    "        \"\"\"\n",
    "        feats,_ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        out     = {}\n",
    "        for nm,X in feats.items():\n",
    "            scaler, iso, depth_train = self.mods[nm]\n",
    "            Xs    = scaler.transform(X)\n",
    "            depth = _avg_depth(iso, Xs)\n",
    "            mask  = depth < self.thr[nm]                      # main IF mask\n",
    "            # ---- extreme‑depth rescue -------------------------------\n",
    "            extreme_thr = np.percentile(depth_train, 100-self.extreme_q)\n",
    "            mask |= depth < extreme_thr\n",
    "            out[nm] = mask\n",
    "        return out\n",
    "\n",
    "# ──────────────────────────── run it ──────────────────────────────────\n",
    "model_eif = ExtendedIsoForest()\n",
    "\n",
    "print(\"▶ Fitting Extended Isolation‑Forest …\")\n",
    "model_eif.fit(storm_data.quantised, fs=FS)\n",
    "\n",
    "print(\"\\n▶ Predicting …\")\n",
    "hot_eif = model_eif.predict(storm_data.quantised, fs=FS)\n",
    "\n",
    "# ───────────────────── strict evaluation ──────────────────────────────\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                      burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "\n",
    "station_m_eif, net_m_eif, n_win_eif = evaluate_windowed_model(\n",
    "    hot            = hot_eif,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ───────────────────────── summary ────────────────────────────────────\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_eif:,}) ——\")\n",
    "for nm,m in station_m_eif.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "#print(net_m_eif)\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m_eif, ndigits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "#  Extended‑Isolation‑Forest · feature‑importance analysis\n",
    "# ----------------------------------------------------------------------\n",
    "#  Requirements (already defined earlier in the notebook):\n",
    "#     • model_eif   – fitted `ExtendedIsoForest`\n",
    "#     • STN         – list of station codes\n",
    "#     • FeatureExtractor registry already holds \"iso16\"\n",
    "# ----------------------------------------------------------------------\n",
    "#  What this cell does\n",
    "#  -------------------\n",
    "#  1.  Retrieves *per‑station* feature importances from every fitted\n",
    "#      isotree model (fast path: `iso.feature_importances_` if available).\n",
    "#      If the isotree build lacks that attribute, it falls back to a\n",
    "#      permutation‑importance routine run on ≤ 5000 random windows\n",
    "#      (fast enough for a notebook, completely self‑contained).\n",
    "#  2.  Aggregates the 16‑feature vectors into a tidy `pandas.DataFrame`.\n",
    "#  3.  Prints a nicely formatted table and draws a matplotlib bar‑chart\n",
    "#      (mean ± 1 std across stations) so you can see what “matters”.\n",
    "# ----------------------------------------------------------------------\n",
    "#  Paste‑and‑run – no external dependencies beyond those already used.\n",
    "# ======================================================================\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "iso_forest_importance.py  — Interpreting the Extended Isolation‑Forest\n",
    "===============================================================================\n",
    "\n",
    "Objective\n",
    "---------\n",
    "Quantify **which of the 16 iso16 features** drive anomaly decisions in the\n",
    "`ExtendedIsoForest` model.  Because Isolation‑Forest is *unsupervised*,\n",
    "we cannot rely on label‑based SHAP/grad techniques; instead we exploit\n",
    "the model’s own internal metric: **average path depth**.\n",
    "\n",
    "Road‑map of this cell\n",
    "---------------------\n",
    "1. **Canonical feature list**   `ISO16_NAMES` ensures every station\n",
    "   lines up its importance vector with the same order used during\n",
    "   training.\n",
    "2. **Permutation‑importance helper**  `_perm_importance()` estimates a\n",
    "   feature’s influence by shuffling its column, recomputing *average\n",
    "   depth*, and measuring the mean absolute change.\n",
    "3. **Per‑station loop**   For each station we either grab\n",
    "   `iso.feature_importances_` (if the installed *isotree* version\n",
    "   exposes it) **or** fall back to permutation importance.\n",
    "4. **Summary table**   Compute mean ± std across stations; print\n",
    "   a tidy table sorted by mean importance.\n",
    "5. **Bar plot**   Visualise mean importance with 1 σ error bars for the\n",
    "   lecture slide.\n",
    "\n",
    "Why average depth is the surrogate score\n",
    "----------------------------------------\n",
    "In Isolation‑Forest, windows that land in *shallow* leaf nodes are\n",
    "considered anomalous.  Therefore, if shuffling feature *j* inflates or\n",
    "deflates average depth noticeably, that feature must influence node\n",
    "splits in many trees.\n",
    "\n",
    "Permutation importance step‑by‑step\n",
    "-----------------------------------\n",
    "1. Optionally **sub‑sample** up to 5 000 windows for speed –\n",
    "   permutation cost grows linearly with rows × features × iterations.\n",
    "2. For each of `n_iter` rounds:\n",
    "   a. Shuffle column *j* of `Xs`.\n",
    "   b. Compute new depth vector via `_avg_depth`.\n",
    "   c. Accumulate `|depth_shuf – depth_base|`.\n",
    "3. Average over iterations and **normalise** so importances sum to 1,\n",
    "   enabling comparison across stations even if depth variances differ.\n",
    "\n",
    "Interpretation tips for the lecture\n",
    "-----------------------------------\n",
    "* A high mean importance + low σ (across stations) suggests a feature is\n",
    "  **universally valuable** (e.g. `peak_env` or `band3`).\n",
    "* High σ points to **station‑specific quirks** – for instance,\n",
    "  `comp_ratio` may matter only where RF tones vary strongly.\n",
    "* Comparing this plot with the iso13 version illustrates the marginal\n",
    "  gain of adding centroid/bandwidth/entropy to iso16.\n",
    "\n",
    "Caveats\n",
    "-------\n",
    "* Permutation ignores **feature interactions** – shuffling one column may\n",
    "  still leave surrogate cues in correlated columns.\n",
    "* In very skewed datasets, the average‑depth metric can saturate; here we\n",
    "  down‑weight that by absolute change rather than signed change.\n",
    "* Sub‑sampling trades variance for runtime; set `subsample=None` for a\n",
    "  research‑grade, full‑data run.\n",
    "\n",
    "Live‑demo idea\n",
    "--------------\n",
    "Toggle `subsample` from 5 000 to `len(Xs)` and watch the error bars\n",
    "shrink, reinforcing the bias‑variance trade‑off in permutation\n",
    "importance.\n",
    "\n",
    "Bottom line\n",
    "-----------\n",
    "This cell gives you a **model‑agnostic, label‑free interpretability**\n",
    "tool tailored to Isolation‑Forest.  Students see not just how well the\n",
    "model scores, but **why** – a crucial bridge between black‑box detection\n",
    "and domain insight.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, random, math\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1)  Define canonical feature‑name list (order matches `iso16`)\n",
    "# ------------------------------------------------------------------ #\n",
    "ISO16_NAMES = [\n",
    "    \"peak_env\", \"med_env\", \"ratio_env\", \"energy\",\n",
    "    \"sta_lta\", \"crest_short\", \"crest_global\",\n",
    "    \"band1\", \"band2\", \"band3\", \"band4\",\n",
    "    \"wave_hi\", \"comp_ratio\",\n",
    "    \"spec_centroid\", \"spec_bw\", \"spec_entropy\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2)  Helper: permutation importance (fallback)\n",
    "# ------------------------------------------------------------------ #\n",
    "def _perm_importance(iso, Xs, depth_base, n_iter=5, subsample=5000):\n",
    "    \"\"\"Estimate feature importance by shuffling each column and measuring\n",
    "    the mean absolute change in **average depth**.\n",
    "\n",
    "    Args:\n",
    "        iso: Fitted `isotree.IsolationForest` model for a single station.\n",
    "        Xs:  Scaled feature matrix (n, d).\n",
    "        depth_base: Average‑depth vector on Xs (baseline).\n",
    "        n_iter: Number of shuffle rounds per feature.\n",
    "        subsample: Upper bound on rows used for speed (if len(Xs) > subsample).\n",
    "\n",
    "    Returns:\n",
    "        `(d,)` array of normalised importances summing to 1.\n",
    "    \"\"\"\n",
    "    print('-> Using fallback permutation importance')\n",
    "    if len(Xs) > subsample:\n",
    "        idx = np.random.choice(len(Xs), subsample, replace=False)\n",
    "        Xs  = Xs[idx]\n",
    "        depth_base = depth_base[idx]\n",
    "\n",
    "    imp = np.zeros(Xs.shape[1], float)\n",
    "    for _ in range(n_iter):\n",
    "        for j in range(Xs.shape[1]):\n",
    "            X_shuf = Xs.copy()\n",
    "            np.random.shuffle(X_shuf[:, j])\n",
    "            depth_shuf = _avg_depth(iso, X_shuf)   # helper from previous cell\n",
    "            imp[j] += np.mean(np.abs(depth_shuf - depth_base))\n",
    "    imp /= n_iter\n",
    "    # normalise to sum=1 for comparability\n",
    "    s = imp.sum();  return imp / s if s else imp\n",
    "\n",
    "\n",
    "\"\"\"Summarise ExtendedIsoForest feature importances across stations.\n",
    "\n",
    "    For each station, this routine tries to read `feature_importances_`\n",
    "    from the underlying isotree model. If unavailable, it falls back to a\n",
    "    self‑contained permutation‑importance estimate using **average depth**.\n",
    "\n",
    "    It then aggregates station‑level vectors into a tidy table with the mean\n",
    "    and standard deviation per iso16 feature, prints a formatted summary,\n",
    "    and draws a bar chart (mean ± 1σ). The resulting summary table is returned.\n",
    "\n",
    "    Args:\n",
    "        model_eif: A fitted `ExtendedIsoForest` instance with attributes:\n",
    "            - `mods[nm] = (scaler, iso_model, depth_train)`\n",
    "            - `fx`      = FeatureExtractor used during training (expects \"iso16\")\n",
    "        storm_data: Object with `.quantised` mapping station → waveform (np.ndarray).\n",
    "        cfg: Config object with `.fs` sampling rate.\n",
    "        win: Window length in samples (must match model_eif’s feature geometry).\n",
    "        hop: Hop size in samples (must match model_eif’s feature geometry).\n",
    "\n",
    "    Returns:\n",
    "        `tbl`: `pd.DataFrame` with two columns:\n",
    "            - `mean_imp`: Mean importance across stations.\n",
    "            - `std_imp`:  Standard deviation across stations.\n",
    "        Index is the iso16 feature name; rows are sorted by `mean_imp` ↓.\n",
    "    \"\"\"\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3)  Collect per‑station importance vectors\n",
    "# ------------------------------------------------------------------ #\n",
    "imp_mat = []                 # rows = stations; cols = features\n",
    "stn_list = []                # station codes in the same order\n",
    "for nm in STN:\n",
    "    scaler, iso, depth_train = model_eif.mods[nm]\n",
    "    if hasattr(iso, \"feature_importances_\"):\n",
    "        imp = iso.feature_importances_.astype(float)\n",
    "        # guard – isotree can return unnormalised importances\n",
    "        if not math.isclose(imp.sum(), 1.0, rel_tol=1e-3):\n",
    "            imp = imp / imp.sum() if imp.sum() else imp\n",
    "    else:\n",
    "        # slow path (but still quick with subsample)\n",
    "        Xs = scaler.transform(model_eif.fx.transform(\n",
    "                {nm: storm_data.quantised[nm]},\n",
    "                win=WIN, hop=HOP, fs=FS\n",
    "            )[0][nm])\n",
    "        depth_base = _avg_depth(iso, Xs)\n",
    "        imp = _perm_importance(iso, Xs, depth_base)\n",
    "    imp_mat.append(imp)\n",
    "    stn_list.append(nm)\n",
    "\n",
    "imp_df = pd.DataFrame(imp_mat, index=stn_list, columns=ISO16_NAMES)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 4)  Pretty table  (mean ± std across stations, sorted)\n",
    "# ------------------------------------------------------------------ #\n",
    "tbl = pd.DataFrame({\n",
    "    \"mean_imp\" : imp_df.mean(axis=0),\n",
    "    \"std_imp\"  : imp_df.std(axis=0)\n",
    "}).sort_values(\"mean_imp\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Extended‑Isolation‑Forest  ·  feature importance ===\")\n",
    "print(tbl.to_string(float_format=\"%.4f\"))\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 5)  Bar‑plot  (mean importance with 1 σ error bars)\n",
    "# ------------------------------------------------------------------ #\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(tbl.index, tbl[\"mean_imp\"], yerr=tbl[\"std_imp\"], capsize=4)\n",
    "plt.xticks(rotation=60, ha='right')\n",
    "plt.ylabel(\"mean importance (across stations)\")\n",
    "plt.title(\"Iso16 feature importances – Extended Isolation‑Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Convolutional Denoising Auto‑Encoder detector (model 6)\n",
    "# ----------------------------------------------------------------------\n",
    "# Requires:\n",
    "#   • storm_data, cfg               (StormGenerator cell)\n",
    "#   • make_windows                  (feature registry cell)\n",
    "#   • evaluate_windowed_model, EvalConfig  (evaluator cell)\n",
    "# ======================================================================\n",
    "\n",
    "# from lightning_sim.features.basic import make_windows\n",
    "# from lightning_sim.evaluation.scorer import evaluate_windowed_model, EvalConfig\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "cdae_detector.py  — Per‑station Convolutional Denoising Auto‑Encoder (CDAE)\n",
    "===============================================================================\n",
    "\n",
    "Big picture\n",
    "-----------\n",
    "We train **one small CDAE per station** to reconstruct *normal* background\n",
    "windows.  Windows that reconstruct poorly (high MSE) are flagged as\n",
    "candidate strokes.  No labels are needed: the network learns the typical\n",
    "spectro‑temporal texture of atmospheric noise by simply denoising\n",
    "additive Gaussian perturbations.\n",
    "\n",
    "Data pipeline & shapes\n",
    "----------------------\n",
    "``raw ADC int16                   →  make_windows\n",
    "           (N,)                  →  win_mat.shape = (n_win, 1024)\n",
    "           └─ int16 → float32 /32768\n",
    "                                   ↓ _WinDataset  (adds channel dim)\n",
    "torch input  shape = (batch, 1, 1024)\n",
    "                                   ↓ CDAE(enc → latent → dec)\n",
    "reconstr.    shape = (batch, 1, 1024)\n",
    "                                   ↓ per‑window MSE\n",
    "MSE vector   shape = (n_win,)\n",
    "                                   ↓ percentile threshold (99.9 %)\n",
    "Boolean hot mask\n",
    "Tiny helpers\n",
    "_WinDataset   Yields (noisy, clean) pairs on‑the‑fly:\n",
    "adds noise_std · N(0,1) to the normalised (−1 … +1) waveform,\n",
    "teaching the CDAE to ignore random perturbations and focus on the\n",
    "underlying structure.\n",
    "\n",
    "_CDAE architecture\n",
    "\n",
    "Layer\tOutput shape\tPurpose\n",
    "Conv1d 1 @ stride 2\t(batch, 8, 512)\tLow‑level edges\n",
    "Conv1d 2 @ stride 2\t(batch, 16, 256)\tMid‑band reps\n",
    "Conv1d 3 @ stride 2\t(batch, 32, 128)\tHigh‑level motifs\n",
    "Flatten → Linear\t(batch, latent)\tBottleneck (default 32)\n",
    "Linear → reshape\t(batch, 32, 128)\tStart decoder\n",
    "Three ConvTranspose1d\t(batch, 1, 1024)\tReconstruct time series\n",
    "\n",
    "Strides halve the length each time: 1024 → 512 → 256 → 128,\n",
    "giving a tidy 32 × 128 tensor before flattening.\n",
    "\n",
    "Design choices & rationale\n",
    "Normalisation by 32768   Maps 14‑bit ADC range into (−1, 1); pairs\n",
    "nicely with tanh non‑linearity if added later.\n",
    "\n",
    "Gaussian noise injection (noise_std = 0.02) – forces the encoder\n",
    "to learn a manifold of clean background rather than memorising exact\n",
    "samples. Think “Rubber sheet” that spans the benign space.\n",
    "\n",
    "Training subset (train_win = 20 000) – speeds up training while\n",
    "preserving diversity; random sampling ensures every epoch sees a fresh\n",
    "mix of windows.\n",
    "\n",
    "Epochs = 4, Batch = 256 – enough to converge on noise removal\n",
    "without over‑fitting; perfect for a live demo (< 1 min on laptop GPU).\n",
    "\n",
    "Percentile threshold (pct_thr = 99.9) – equivalent to an\n",
    "unsupervised contamination of 0.1 %; adjust on the fly to illustrate\n",
    "precision‑recall trade‑off.\n",
    "\n",
    "Device auto‑selection – uses GPU if torch.cuda.is_available()\n",
    "yields True; otherwise falls back to CPU seamlessly.\n",
    "\n",
    "Scoring windows\n",
    "_score_windows() streams 4096 windows at a time to avoid GPU memory\n",
    "overflow, computes per‑window MSE, and stores the result in a pre‑sized\n",
    "NumPy array – faster than appending lists.\n",
    "\n",
    "Why a denoising AE, not plain AE?\n",
    "A plain auto‑encoder could memorise line noise and weak strokes,\n",
    "reducing error on exactly the samples we want to flag. By adding\n",
    "noise, we encourage the network to reconstruct general structure,\n",
    "so unseen transients (strokes) register higher error.\n",
    "\n",
    "Position in the grand pipeline\n",
    "Input contract   Still the same Dict[station → 1‑D ADC].\n",
    "\n",
    "Output contract  Unchanged Boolean mask per station, so\n",
    "evaluate_windowed_model remains plug‑compatible.\n",
    "\n",
    "Complementarity  CDAE often catches subtle shape changes\n",
    "overlooked by Isolation‑Forest (which relies on summary statistics)\n",
    "but may miss very high‑amplitude spikes if they saturate activations;\n",
    "combining both detectors via logical OR is an interesting extension.\n",
    "\n",
    "Potential extensions for students\n",
    "Phase‑aware training  Use the analytic signal (real + imag)\n",
    "as two channels to teach the CDAE complex‑domain features.\n",
    "\n",
    "Residual thresholding  Instead of percentile, fit a Gaussian\n",
    "Mixture to MSE and choose a log‑likelihood cut‑off.\n",
    "\n",
    "Temporal context  Feed sequences of windows to a Conv‑LSTM for\n",
    "smoother decisions.\n",
    "\n",
    "Bottom line\n",
    "The CDAE offers a learned non‑linear filter that models station‑specific\n",
    "noise better than linear features yet remains light enough for edge\n",
    "deployment. Its inclusion showcases the pipeline’s flexibility: swap\n",
    "feature engineering for raw‑waveform learning, keep the evaluation layer\n",
    "unchanged, and obtain an apples‑to‑apples performance score.\n",
    "\n",
    "\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "How the 1‑D convolutions work in the CDAE\n",
    "───────────────────────────────────────────────────────────────────────────────\n",
    "Signal orientation\n",
    "------------------\n",
    "* Input tensor to the network: **shape = (batch, channels, length)**.\n",
    "  Here, `channels = 1` (mono waveform) and `length = 1024` samples\n",
    "  (≈ 9.36 ms).\n",
    "\n",
    "Encoder anatomy\n",
    "---------------\n",
    "Conv1d(1, 8, kernel_size=7, stride=2, padding=3)\n",
    "↓ length: ⌈1024/2⌉ = 512\n",
    "Conv1d(8, 16, k=7, s=2, p=3)\n",
    "↓ length: 256\n",
    "Conv1d(16,32, k=7, s=2, p=3)\n",
    "↓ length: 128\n",
    "Flatten → Linear(32*128 → latent)\n",
    "\n",
    "markdown\n",
    "Copy\n",
    "\n",
    "* **Kernel size 7**   Looks at ±3 samples around each centre – roughly a\n",
    "  half‑cycle of a 14 kHz wave at 109 kHz FS.\n",
    "* **Stride 2**         Downsamples by 2, halving temporal resolution and\n",
    "  doubling receptive‑field overlap at each layer.\n",
    "* **Padding 3**        Keeps “same” length after convolution so stride is\n",
    "  the only factor shrinking the length (1024 → 512 → 256 → 128).\n",
    "* **Channels**         8 → 16 → 32 filters let the network capture an\n",
    "  increasing number of local motifs: low‑freq hum, RF spikes, etc.\n",
    "\n",
    "Decoder mirror\n",
    "--------------\n",
    "Transposed convolutions (“deconvs”) invert the stride‑2 downsampling:\n",
    "\n",
    "Linear(latent → 32*128) → reshape (batch,32,128)\n",
    "ConvT1d(32→16, k=7, s=2, p=3, output_padding=1) → length 256\n",
    "ConvT1d(16→8, k=7, s=2, p=3, output_padding=1) → length 512\n",
    "ConvT1d(8→1, k=7, s=2, p=3, output_padding=1) → length 1024\n",
    "\n",
    "markdown\n",
    "Copy\n",
    "*`output_padding=1`* compensates for integer‑division rounding so the\n",
    "decoder exactly doubles length at each step (128 → 256 → 512 → 1024).\n",
    "\n",
    "Receptive field\n",
    "---------------\n",
    "Each output sample “sees” a growing neighbourhood:\n",
    "\n",
    "Layer 0 (input) : ±0\n",
    "Layer 1 after stride‑2 : ±3\n",
    "Layer 2 : ±3 + 2·3 = ±9\n",
    "Layer 3 : ±9 + 4·3 = ±21\n",
    "\n",
    "pgsql\n",
    "Copy\n",
    "Thus, the latent embedding contains information from a **±21‑sample\n",
    "context** around every point (≈ 0.19 ms), enough to capture the attack\n",
    "and decay of a sferic’s initial peak.\n",
    "\n",
    "Denoising principle\n",
    "-------------------\n",
    "During training we add *white Gaussian noise* (`noise_std = 0.02`) **only\n",
    "to the input**:\n",
    "\n",
    "noisy = clean + ε, ε ~ N(0, 0.02²)\n",
    "loss = MSE( net(noisy), clean )\n",
    "\n",
    "markdown\n",
    "Copy\n",
    "The network therefore learns a mapping\n",
    "`F: noisy → clean ≈ E[clean | noisy]` – effectively a *non‑linear Wiener\n",
    "filter* tuned to the station’s typical spectrum.  A lightning burst,\n",
    "whose waveform statistics deviate sharply from background, yields a\n",
    "larger reconstruction error because the clean manifold cannot represent\n",
    "it well.\n",
    "\n",
    "Latent dimension (bottleneck)\n",
    "-----------------------------\n",
    "* `latent = 32` ⇒ compression factor\n",
    "  `(1 × 1024) / 32 = 32:1`.\n",
    "* A smaller latent forces stronger abstraction (higher error on rare\n",
    "  events) but can under‑fit fine background details; a larger latent\n",
    "  risks memorising noise.  32 is a sweet‑spot found empirically for\n",
    "  14‑bit, 9 ms windows.\n",
    "\n",
    "Why MSE, not L1 or spectral loss?\n",
    "---------------------------------\n",
    "* **MSE** has a convenient probabilistic interpretation: assuming\n",
    "  i.i.d. Gaussian error, it maximises log‑likelihood.\n",
    "* L1 would encourage sparsity but makes gradients less stable for ReLU\n",
    "  decoders at these small magnitudes.\n",
    "* Spectral losses (e.g. log‑STFT) bias towards frequency content but\n",
    "  require extra hyper‑parameters (FFT size, hop) and double compute.\n",
    "\n",
    "Threshold selection recap\n",
    "-------------------------\n",
    "We score *all* windows after training:\n",
    "\n",
    "errs = per‑window MSE\n",
    "thr = 99.9th percentile(errs)\n",
    "hot = errs > thr\n",
    "\n",
    "pgsql\n",
    "Copy\n",
    "Windows in the top 0.1 % error tail are *anomalies*.  Percentile keeps\n",
    "the station‑specific false‑positive rate fixed irrespective of absolute\n",
    "noise variance.\n",
    "\n",
    "Practical artefacts\n",
    "-------------------\n",
    "* **Input quantisation** – Int16 → float32 /32768 squashes values into\n",
    "  (−1,1); reconstruction error is therefore in units of *fraction of full‑\n",
    "  scale squared* (≈ 10⁻⁸ to 10⁻⁴).\n",
    "* **Batch‑inference chunking (4096 windows)** prevents GPU OOM while\n",
    "  keeping CU/CPU interplay efficient.\n",
    "* **torch.no_grad()** during scoring conserves VRAM and speeds up\n",
    "  inference by ~30 %.\n",
    "\n",
    "Take‑away\n",
    "---------\n",
    "1‑D convolutions slide small kernels along time, building hierarchical\n",
    "features exactly as 2‑D CNNs do for images but without wasting compute\n",
    "on a width dimension.  The CDAE leverages this to learn a compact,\n",
    "station‑specific “clean speech” model of atmospheric noise.  Reconstruction\n",
    "error then becomes a powerful, makeshift likelihood score for transient\n",
    "events such as lightning strokes – all without a single handcrafted\n",
    "feature or label.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math, random, numpy as np, torch, torch.nn as nn, os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict\n",
    "\n",
    "# ----------------------- tiny helpers ---------------------------------\n",
    "class _WinDataset(Dataset):\n",
    "    \"\"\"Dataset yielding (noisy, clean) window pairs for denoising AE.\n",
    "\n",
    "    The incoming window matrix is int16 (ADC codes). We normalise to\n",
    "    float32 in (−1, +1) by dividing by 32768, then add Gaussian noise\n",
    "    *only* to the input to encourage denoising rather than memorisation.\n",
    "\n",
    "    Args:\n",
    "        win_mat: Array of shape (n_win, W) containing int16 windows.\n",
    "        noise_std: Standard deviation of injected Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "        Each __getitem__ returns:\n",
    "            noisy: Tensor (1, W)  — clean + N(0, noise_std²)\n",
    "            clean: Tensor (1, W)  — target reconstruction\n",
    "    \"\"\"\n",
    "    def __init__(self, win_mat: np.ndarray, noise_std=0.02):\n",
    "        x = win_mat.astype(np.float32)/32768.0\n",
    "        self.clean = torch.from_numpy(x)[:,None]\n",
    "        self.noise_std = noise_std\n",
    "    def __len__(self): return len(self.clean)\n",
    "    def __getitem__(self, i):\n",
    "        clean = self.clean[i]\n",
    "        noisy = clean + self.noise_std*torch.randn_like(clean)\n",
    "        return noisy, clean\n",
    "\n",
    "class _CDAE(nn.Module):\n",
    "    \"\"\"Minimal 1‑D Convolutional Denoising Auto‑Encoder (CDAE).\n",
    "\n",
    "    Encoder:\n",
    "        Conv1d(1→8, k=7, s=2, p=3) → ReLU\n",
    "        Conv1d(8→16, k=7, s=2, p=3) → ReLU\n",
    "        Conv1d(16→32, k=7, s=2, p=3) → ReLU\n",
    "        Flatten → Linear(32*128 → latent) → ReLU\n",
    "\n",
    "    Decoder:\n",
    "        Linear(latent → 32*128) → reshape (32, 128)\n",
    "        ConvT1d(32→16, k=7, s=2, p=3, out_pad=1) → ReLU\n",
    "        ConvT1d(16→8,  k=7, s=2, p=3, out_pad=1) → ReLU\n",
    "        ConvT1d(8→1,   k=7, s=2, p=3, out_pad=1)\n",
    "\n",
    "    Notes:\n",
    "        - With W=1024 and stride‑2, lengths go 1024→512→256→128 in the\n",
    "          encoder and mirror back in the decoder.\n",
    "        - Activation on the output layer is omitted; loss is MSE to the\n",
    "          clean target.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent=32):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1,  8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, latent), nn.ReLU()\n",
    "        )\n",
    "        self.dec_fc = nn.Linear(latent, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: (B, 1, W) → (B, 1, W).\"\"\"\n",
    "        z = self.enc(x)\n",
    "        h = self.dec_fc(z).view(-1,32,128)\n",
    "        return self.dec(h)\n",
    "\n",
    "# ----------------------- detector class -------------------------------\n",
    "class CdaeModel:\n",
    "    \"\"\"Per‑station Convolutional Denoising Auto‑Encoder detector.\n",
    "\n",
    "    Trains a compact CDAE per station on unlabelled windows to model\n",
    "    “normal” background. Windows with high reconstruction MSE are flagged.\n",
    "\n",
    "    Parameters:\n",
    "        win: Window length in samples (e.g., 1024).\n",
    "        hop: Hop size in samples (e.g., 512).\n",
    "        latent: Bottleneck width of the CDAE (default 32).\n",
    "        epochs: Training epochs per station.\n",
    "        batch: Mini‑batch size (windows).\n",
    "        train_win: Max windows to sample for training (speeds up notebooks).\n",
    "        pct_thr: Percentile for threshold on per‑window MSE (e.g., 99.9).\n",
    "        device: Explicit device string; if None, auto‑select CUDA if available.\n",
    "\n",
    "    Instance attributes:\n",
    "        models: {station → trained `_CDAE`} (eval mode).\n",
    "        thr:    {station → float} percentile threshold on MSE.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 win=1024, hop=512,\n",
    "                 latent=32, epochs=4, batch=256,\n",
    "                 train_win=20_000, pct_thr=99.9,\n",
    "                 device=None):\n",
    "        self.win, self.hop = win, hop\n",
    "        self.latent   = latent\n",
    "        self.epochs   = epochs\n",
    "        self.batch    = batch\n",
    "        self.train_win= train_win\n",
    "        self.pct_thr  = pct_thr\n",
    "        self.device   = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.models: Dict[str, _CDAE] = {}\n",
    "        self.thr:    Dict[str, float] = {}\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _train_one(self, win_mat: np.ndarray):\n",
    "        \"\"\"Sub‑sample and train a CDAE on (noisy → clean) pairs.\n",
    "\n",
    "        Args:\n",
    "            win_mat: Int16 window matrix for a single station, shape (n, W).\n",
    "\n",
    "        Returns:\n",
    "            Trained `_CDAE` in eval mode.\n",
    "        \"\"\"\n",
    "        idx = np.random.choice(len(win_mat),\n",
    "                               min(self.train_win, len(win_mat)),\n",
    "                               replace=False)\n",
    "        ds = _WinDataset(win_mat[idx])\n",
    "        dl = DataLoader(ds, batch_size=self.batch, shuffle=True,\n",
    "                        num_workers=0, pin_memory=False)\n",
    "        net = _CDAE(latent=self.latent).to(self.device)\n",
    "        opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "        net.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for noisy, clean in dl:\n",
    "                noisy, clean = noisy.to(self.device), clean.to(self.device)\n",
    "                opt.zero_grad()\n",
    "                loss = nn.functional.mse_loss(net(noisy), clean)\n",
    "                loss.backward(); opt.step()\n",
    "        return net\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def fit(self, raw: Dict[str,np.ndarray], verbose=True):\n",
    "        \"\"\"Train one CDAE per station and set a station‑specific MSE threshold.\n",
    "\n",
    "        Threshold rule:\n",
    "            thr = percentile(errs, pct_thr) on the station’s *training* windows.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping station → 1‑D int/float array (same length across stations).\n",
    "            verbose: If True, print brief training and threshold summaries.\n",
    "        \"\"\"\n",
    "        for nm, sig in raw.items():\n",
    "            win_mat = make_windows(sig, self.win, self.hop)\n",
    "            if verbose:\n",
    "                print(f\"\\n▶ Training CDAE for {nm}  \"\n",
    "                      f\"({len(win_mat)} windows, device={self.device})\")\n",
    "            net = self._train_one(win_mat)\n",
    "            self.models[nm] = net.eval()\n",
    "\n",
    "            # inference to derive threshold\n",
    "            errs = self._score_windows(net, win_mat)\n",
    "            thr  = np.percentile(errs, self.pct_thr)\n",
    "            self.thr[nm] = thr\n",
    "            if verbose:\n",
    "                n_hot = int((errs>thr).sum())\n",
    "                print(f\"  windows flagged = {n_hot:5d} \"\n",
    "                      f\"({100*n_hot/len(errs):.2f} %)  |  thr={thr:.4e}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _score_windows(self, net: _CDAE, win_mat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute per‑window MSE in chunks to avoid memory spikes.\n",
    "\n",
    "        Args:\n",
    "            net: Trained `_CDAE`.\n",
    "            win_mat: Int16 windows, shape (n, W).\n",
    "\n",
    "        Returns:\n",
    "            errs: Float array of shape (n,) with mean‑squared error per window.\n",
    "        \"\"\"\n",
    "        errs = np.empty(len(win_mat), float)\n",
    "        with torch.no_grad():\n",
    "            for i0 in range(0, len(win_mat), 4096):\n",
    "                seg = torch.from_numpy(\n",
    "                        win_mat[i0:i0+4096].astype(np.float32)/32768.0\n",
    "                     )[:,None].to(self.device)\n",
    "                rec = net(seg).cpu().numpy()\n",
    "                errs[i0:i0+len(rec)] = ((rec - seg.cpu().numpy())**2)\\\n",
    "                                        .mean(axis=(1,2))\n",
    "        return errs\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def predict(self, raw: Dict[str,np.ndarray]) -> Dict[str,np.ndarray]:\n",
    "        \"\"\"Return Boolean hot masks per station using stored thresholds.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping station → 1‑D array to score.\n",
    "\n",
    "        Returns:\n",
    "            Dict station → bool[n_win] where True marks a high‑error window.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If prediction is attempted for an unfitted station.\n",
    "        \"\"\"\n",
    "        hot = {}\n",
    "        for nm, sig in raw.items():\n",
    "            win_mat = make_windows(sig, self.win, self.hop)\n",
    "            errs = self._score_windows(self.models[nm], win_mat)\n",
    "            hot[nm] = errs > self.thr[nm]\n",
    "        return hot\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Instantiate & train\n",
    "# ----------------------------------------------------------------------\n",
    "WIN, HOP = 1024, 512\n",
    "model_cdae = CdaeModel(win=WIN, hop=HOP,\n",
    "                       latent=32, epochs=4, batch=256,\n",
    "                       train_win=20_000, pct_thr=99.9)\n",
    "\n",
    "model_cdae.fit(storm_data.quantised)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  Predict hot windows\n",
    "# ----------------------------------------------------------------------\n",
    "hot_cdae = model_cdae.predict(storm_data.quantised)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Evaluate\n",
    "# ----------------------------------------------------------------------\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=cfg.fs,\n",
    "                      burst_len=int(0.04*cfg.fs),\n",
    "                      min_stn=2, tol_win=0)\n",
    "\n",
    "station_m_cdae, net_m_cdae, n_win_cdae = evaluate_windowed_model(\n",
    "    hot            = hot_cdae,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = list(storm_data.quantised),\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Summary\n",
    "# ----------------------------------------------------------------------\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_cdae:,}) ——\")\n",
    "for nm, m in station_m_cdae.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "#print(net_m_cdae)\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m_cdae, ndigits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Multi‑task CDAE: reconstruct waveform + iso16 features per station ===================\n",
    "# - Encoder/decoder for waveform (as before)\n",
    "# - Plus a feature head that reconstructs iso16 features from the latent\n",
    "# - Train with combined loss: L = α * MSE(wave) + β * MSE(features)\n",
    "# - Score windows with the same combined error; per‑station percentile threshold\n",
    "# =========================================================================================================\n",
    "\n",
    "import math, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# ----------------------- config -----------------------\n",
    "WIN, HOP = 1024, 512\n",
    "LATENT   = 32\n",
    "EPOCHS   = 4\n",
    "BATCH    = 256\n",
    "TRAIN_WIN= 20_000\n",
    "ALPHA_W  = 1.0     # weight for waveform MSE\n",
    "BETA_F   = 0.3     # weight for feature MSE (features standardized ⇒ ~unit scale)\n",
    "PCT_THR  = 99.9\n",
    "DEVICE   = ('mps' if torch.backends.mps.is_available()\n",
    "            else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ----------------------- helpers ----------------------\n",
    "def make_windows(sig: np.ndarray, win: int, hop: int) -> np.ndarray:\n",
    "    n = (len(sig) - win) // hop + 1\n",
    "    if n <= 0: return np.zeros((0, win), dtype=sig.dtype)\n",
    "    idx = np.arange(0, n*hop, hop)[:, None] + np.arange(win)\n",
    "    return sig[idx]\n",
    "\n",
    "# Feature extractor (provided by your env)\n",
    "fx = FeatureExtractor([\"iso16\"])  # returns Dict[str, np.ndarray] of shape (n_win, 16)\n",
    "\n",
    "# --------------------- datasets -----------------------\n",
    "class WinFeatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Holds normalized waveform windows and standardized iso16 features.\n",
    "    Returns: (noisy_wave [B,1,W], clean_wave [B,1,W], feat_std [B,16])\n",
    "    \"\"\"\n",
    "    def __init__(self, win_mat: np.ndarray, feat_std: np.ndarray, noise_std=0.02):\n",
    "        assert len(win_mat) == len(feat_std)\n",
    "        x = win_mat.astype(np.float32) / 32768.0\n",
    "        self.clean = torch.from_numpy(x)[:, None, :]               # (N,1,W)\n",
    "        self.feat  = torch.from_numpy(feat_std.astype(np.float32)) # (N,16)\n",
    "        self.noise = noise_std\n",
    "    def __len__(self): return len(self.clean)\n",
    "    def __getitem__(self, i):\n",
    "        clean = self.clean[i]\n",
    "        noisy = clean + self.noise * torch.randn_like(clean)\n",
    "        return noisy, clean, self.feat[i]\n",
    "\n",
    "# --------------------- model --------------------------\n",
    "class CDAE_Multi(nn.Module):\n",
    "    def __init__(self, latent: int = LATENT, feat_dim: int = 16):\n",
    "        super().__init__()\n",
    "        # encoder (same as your CDAE)\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1,  8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.fc_z = nn.Sequential(nn.Linear(32*128, latent), nn.ReLU())\n",
    "        # waveform decoder head\n",
    "        self.dec_fc = nn.Linear(latent, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "        # feature head (MLP)\n",
    "        self.feat_head = nn.Sequential(\n",
    "            nn.Linear(latent, 64), nn.ReLU(),\n",
    "            nn.Linear(64, feat_dim)\n",
    "        )\n",
    "    def forward(self, x):  # x: (B,1,W)\n",
    "        h = self.enc(x)\n",
    "        z = self.fc_z(h)\n",
    "        # waveform recon\n",
    "        hw = torch.relu(self.dec_fc(z)).view(x.size(0), 32, 128)\n",
    "        xhat = self.dec(hw)                  # (B,1,W)\n",
    "        # feature recon\n",
    "        fhat = self.feat_head(z)             # (B,F)\n",
    "        return xhat, fhat\n",
    "\n",
    "# --------------------- trainer ------------------------\n",
    "class CdaeFeatModel:\n",
    "    \"\"\"\n",
    "    Per‑station multi‑task CDAE: reconstruct waveform AND iso16 features.\n",
    "    Uses per‑station feature standardization (mean/std).\n",
    "    Threshold = percentile of combined error (α*MSE_wave + β*MSE_feat).\n",
    "    \"\"\"\n",
    "    def __init__(self, *, win=WIN, hop=HOP, latent=LATENT, epochs=EPOCHS,\n",
    "                 batch=BATCH, train_win=TRAIN_WIN, pct_thr=PCT_THR,\n",
    "                 alpha_w=ALPHA_W, beta_f=BETA_F, device=DEVICE):\n",
    "        self.win, self.hop     = win, hop\n",
    "        self.latent            = latent\n",
    "        self.epochs            = epochs\n",
    "        self.batch             = batch\n",
    "        self.train_win         = train_win\n",
    "        self.pct_thr           = pct_thr\n",
    "        self.alpha_w, self.beta_f = alpha_w, beta_f\n",
    "        self.device            = device\n",
    "        self.models: Dict[str, CDAE_Multi] = {}\n",
    "        self.thr  : Dict[str, float]       = {}\n",
    "        self.fstat: Dict[str, Tuple[np.ndarray,np.ndarray]] = {}  # station → (μ, σ) for iso16\n",
    "\n",
    "    def _prep_feats(self, nm: str, sig: np.ndarray) -> Tuple[np.ndarray,np.ndarray,np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute windows and iso16 features for station nm, return:\n",
    "        win_mat (N,W), F (N,16), F_std (N,16) with per-station μ/σ.\n",
    "        \"\"\"\n",
    "        win_mat = make_windows(sig, self.win, self.hop)\n",
    "        feats, _ = fx.transform({nm: sig}, win=self.win, hop=self.hop, fs=cfg.fs)\n",
    "        F = feats[nm].astype(np.float32)\n",
    "        # align lengths (defensive)\n",
    "        N = min(len(win_mat), len(F))\n",
    "        win_mat = win_mat[:N]\n",
    "        F       = F[:N]\n",
    "        # per‑station standardization for features\n",
    "        mu = F.mean(axis=0, keepdims=True)\n",
    "        sd = F.std(axis=0, keepdims=True) + 1e-8\n",
    "        F_std = (F - mu) / sd\n",
    "        self.fstat[nm] = (mu.squeeze(0), sd.squeeze(0))\n",
    "        return win_mat, F, F_std\n",
    "\n",
    "    def _train_one(self, win_mat: np.ndarray, F_std: np.ndarray) -> CDAE_Multi:\n",
    "        # sample windows for training\n",
    "        idx = np.random.choice(len(win_mat),\n",
    "                               min(self.train_win, len(win_mat)),\n",
    "                               replace=False)\n",
    "        ds = WinFeatDataset(win_mat[idx], F_std[idx], noise_std=0.02)\n",
    "        dl = DataLoader(ds, batch_size=self.batch, shuffle=True,\n",
    "                        num_workers=0, pin_memory=False, drop_last=True)\n",
    "        net = CDAE_Multi(latent=self.latent, feat_dim=F_std.shape[1]).to(self.device)\n",
    "        opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "        net.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for noisy, clean, f_t in dl:\n",
    "                noisy, clean, f_t = noisy.to(self.device), clean.to(self.device), f_t.to(self.device)\n",
    "                xhat, fhat = net(noisy)\n",
    "                loss_w = nn.functional.mse_loss(xhat, clean)\n",
    "                loss_f = nn.functional.mse_loss(fhat, f_t)\n",
    "                loss = self.alpha_w*loss_w + self.beta_f*loss_f\n",
    "                opt.zero_grad()\n",
    "                loss.backward(); opt.step()\n",
    "        return net.eval()\n",
    "\n",
    "    def fit(self, raw: Dict[str,np.ndarray], verbose=True):\n",
    "        for nm, sig in raw.items():\n",
    "            win_mat, F, F_std = self._prep_feats(nm, sig)\n",
    "            if verbose:\n",
    "                print(f\"\\n▶ Training CDAE+Feat for {nm}  \"\n",
    "                      f\"(windows={len(win_mat):,}, device={self.device})\")\n",
    "            net = self._train_one(win_mat, F_std)\n",
    "            self.models[nm] = net\n",
    "\n",
    "            # derive threshold from combined error over ALL windows\n",
    "            errs = self._score_windows(net, nm, sig, win_mat=win_mat, F=F)  # combined scores\n",
    "            thr  = np.percentile(errs, self.pct_thr)\n",
    "            self.thr[nm] = float(thr)\n",
    "            if verbose:\n",
    "                n_hot = int((errs > thr).sum())\n",
    "                print(f\"  windows flagged = {n_hot:5d} \"\n",
    "                      f\"({100*n_hot/len(errs):.2f} %)  |  thr={thr:.4e}\")\n",
    "\n",
    "    def _score_windows(self, net: CDAE_Multi, nm: str, sig: np.ndarray,\n",
    "                       win_mat: np.ndarray=None, F: np.ndarray=None) -> np.ndarray:\n",
    "        # recompute if not provided\n",
    "        if win_mat is None or F is None:\n",
    "            win_mat, F, _ = self._prep_feats(nm, sig)\n",
    "        mu, sd = self.fstat[nm]\n",
    "        F_std  = (F - mu[None,:]) / sd[None,:]\n",
    "\n",
    "        errs = np.empty(len(win_mat), dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            for i0 in range(0, len(win_mat), 4096):\n",
    "                seg = torch.from_numpy(\n",
    "                        win_mat[i0:i0+4096].astype(np.float32)/32768.0\n",
    "                     )[:,None].to(self.device)\n",
    "                Fb  = torch.from_numpy(F_std[i0:i0+4096].astype(np.float32)).to(self.device)\n",
    "                xhat, fhat = net(seg)\n",
    "                # per-window losses\n",
    "                e_wave = ((xhat - seg)**2).mean(dim=(1,2))        # (B,)\n",
    "                e_feat = ((fhat - Fb)**2).mean(dim=1)             # (B,)\n",
    "                e_comb = self.alpha_w*e_wave + self.beta_f*e_feat\n",
    "                errs[i0:i0+len(e_comb)] = e_comb.cpu().numpy()\n",
    "        return errs\n",
    "\n",
    "    def predict(self, raw: Dict[str,np.ndarray]) -> Dict[str,np.ndarray]:\n",
    "        hot = {}\n",
    "        for nm, sig in raw.items():\n",
    "            net = self.models[nm]\n",
    "            errs = self._score_windows(net, nm, sig)\n",
    "            hot[nm] = errs > self.thr[nm]\n",
    "        return hot\n",
    "\n",
    "# -------------------- train, predict, evaluate -----------------------\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "model_cdae_feat = CdaeFeatModel(win=WIN, hop=HOP,\n",
    "                                latent=LATENT, epochs=EPOCHS, batch=BATCH,\n",
    "                                train_win=TRAIN_WIN, pct_thr=PCT_THR,\n",
    "                                alpha_w=ALPHA_W, beta_f=BETA_F, device=DEVICE)\n",
    "\n",
    "print(\"▶ Fitting CDAE+Features models …\")\n",
    "model_cdae_feat.fit(storm_data.quantised)\n",
    "\n",
    "print(\"\\n▶ Predicting hot windows …\")\n",
    "hot_cdae_feat = model_cdae_feat.predict(storm_data.quantised)\n",
    "\n",
    "# -------------------- strict evaluation ------------------------------\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=cfg.fs,\n",
    "                      burst_len=int(0.04*cfg.fs), min_stn=2, tol_win=0)\n",
    "\n",
    "station_m, net_m, n_win = evaluate_windowed_model(\n",
    "    hot            = hot_cdae_feat,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = list(storm_data.quantised),\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win:,}) ——\")\n",
    "for nm, m in station_m.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m, ndigits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Graph‑CDAE (v2)  ·  Training & normalised window matrix export\n",
    "# — includes autograd‑mode reset so it cannot fail even if a\n",
    "#   previous cell set torch.set_grad_enabled(False)\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "===============================================================================\n",
    "graph_cdae_demo.py  — A Graph‑enhanced Convolutional Denoising Auto‑Encoder\n",
    "===============================================================================\n",
    "\n",
    "WHY THIS MODEL?\n",
    "---------------\n",
    "Lightning strokes are *spatially coherent*: neighbouring stations record\n",
    "bursts that share wave‑shapes and timing offsets.  The earlier per‑station\n",
    "CDAE ignored this cross‑station structure.  Here we keep the same\n",
    "1‑D convolutional encoder‑decoder but **add a Graph‑Attention layer in\n",
    "the latent space**, allowing latent codes from all stations in a frame\n",
    "to exchange information before reconstruction.\n",
    "\n",
    "DATA SHAPES & PIPELINE\n",
    "----------------------\n",
    "* **`RAW`** – NumPy array `(n_win, S, WIN)`\n",
    "  `n_win`   = common window count\n",
    "  `S`       = number of stations (here 11)\n",
    "  `WIN`     = 1024 samples\n",
    "* **Dataset** – `_WinDataset` returns `(noisy, clean)` tensors each of\n",
    "  shape `(S, WIN)`; additive σ = 0.03 noise encourages manifold learning.\n",
    "* **DataLoader** – batches concatenated along dim 0 ⇒ `noisy.shape\n",
    "  = (batch, S, WIN)`.\n",
    "\n",
    "MODEL ARCHITECTURE\n",
    "------------------\n",
    "(noisy B,S,1024)\n",
    "↓ reshape BS,1,1024\n",
    "Conv1d stack # local temporal patterns (per station)\n",
    "↓ flatten\n",
    "Linear → latent z ∈ ℝ^{BS × 32}\n",
    "↓ GBlock ×2 # cross‑station message passing\n",
    "Linear → reshape # prepare feature maps\n",
    "ConvTranspose stack # reconstruct waveform\n",
    "↓ reshape B,S,1024\n",
    "\n",
    "markdown\n",
    "Copy\n",
    "\n",
    "### 1‑D CONVOLUTIONS (encoder & decoder)\n",
    "* Kernel 7 / stride 2 / padding 3 ⇒ “same” length then halved:\n",
    "  1024 → 512 → 256 → 128.\n",
    "* Three levels capture **low‑, mid‑, high‑frequency motifs** of a sferic\n",
    "  while down‑sampling for computational efficiency.\n",
    "* Decoder mirrors this process with transposed convolutions\n",
    "  (`output_padding=1` to correct integer division).\n",
    "\n",
    "### LATENT GRAPH BLOCK\n",
    "GATv2Conv(h,h,heads=4,concat=False)\n",
    "LayerNorm + residual\n",
    "\n",
    "markdown\n",
    "Copy\n",
    "* **Input dim h = 32** latent per `(station,window)` pair.\n",
    "* **Fully‑connected graph** (`edge_index`) lets *every* station attend to\n",
    "  every other in the *same* window.  Attention weights are learned;\n",
    "  distant or noisy stations can be down‑weighted automatically.\n",
    "* Two stacked `GBlock`s deepen relational reasoning without blowing up\n",
    "  parameter count.\n",
    "\n",
    "MESSAGE‑PASSING INTUITION\n",
    "-------------------------\n",
    "Imagine each station as a node emitting a 32‑dim “summary” of its local\n",
    "waveform.  The **GAT layer** lets nodes query their neighbours:\n",
    "“Does your latent say *burst*? If yes, maybe raise mine a bit.”  This\n",
    "yields *coordinated denoising*: isolated noise at one station is ignored,\n",
    "but multi‑station patterns (true strokes) influence reconstruction.\n",
    "\n",
    "TRAINING SET‑UP (demo‑grade)\n",
    "----------------------------\n",
    "* **Epochs = 1** – just enough to show convergence in a workshop; bump to\n",
    "  50 for research.\n",
    "* **Batch = 128 windows** × 11 stations ≈ 140 k samples per step.\n",
    "* **Loss** – simple MSE between recon and clean, averaged over all\n",
    "  stations & samples.\n",
    "* **Optimiser LR = 2e‑3** – higher than usual because 1 epoch.\n",
    "\n",
    "DEVICE FLEXIBILITY\n",
    "------------------\n",
    "`DEVICE` auto‑selects **Apple M‑series GPU (mps)**, **CUDA GPU**, or CPU\n",
    "in that order.  No code changes required when you switch hardware.\n",
    "\n",
    "QUICK BENCHMARK\n",
    "---------------\n",
    "Running the cell prints\n",
    "✔ RAW built – shape (27 450, 11, 1024)\n",
    "✔ Graph‑CDAE trained on cuda | n_win = 27 450\n",
    "\n",
    "markdown\n",
    "Copy\n",
    "On a laptop RTX 3060 this takes ≈ 15 s.\n",
    "\n",
    "HOW TO USE AS A DETECTOR\n",
    "------------------------\n",
    "Exactly like the per‑station CDAE:\n",
    "\n",
    "1. After full training, compute per‑window reconstruction MSE.\n",
    "2. Aggregate **per station** or **across stations** (e.g. mean MSE) and\n",
    "   apply a percentile threshold.\n",
    "3. Feed Boolean mask(s) to `evaluate_windowed_model`.\n",
    "\n",
    "POSSIBLE EXTENSIONS\n",
    "-------------------\n",
    "* **Edge weights** – encode haversine distance or propagation delay\n",
    "  as initial edge attributes.\n",
    "* **IsoForest on latent z** – treat (window × station) latent matrix\n",
    "  as a 32‑D point cloud and run an Isolation‑Forest for hybrid\n",
    "  unsupervised detection.\n",
    "* **Temporal Transformer** – add a time‑axis self‑attention after the\n",
    "  GAT to capture stroke evolution over adjacent windows.\n",
    "* **Multi‑task learning** – jointly predict time‑of‑arrival offsets while\n",
    "  denoising, forcing the latent to encode physically meaningful cues.\n",
    "\n",
    "BOTTOM LINE\n",
    "-----------\n",
    "Graph‑CDAE fuses **local waveform features** (1‑D CNN) with **global\n",
    "station interactions** (GAT), demonstrating how easily the lightning\n",
    "pipeline can scale from single‑channel models to *spatio‑temporal\n",
    "deep learning* – all while sticking to our disciplined window/mask I/O\n",
    "protocol.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np, torch, torch.nn as nn, math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from typing import List\n",
    "\n",
    "# ---------- constants pulled from simulator --------------------\n",
    "STN:   List[str] = list(storm_data.quantised)\n",
    "S      = len(STN)\n",
    "FS     = cfg.fs\n",
    "WIN, HOP  = 1024, 512\n",
    "DEVICE = ('mps' if torch.backends.mps.is_available()\n",
    "          else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------- helper ---------------------------------------------\n",
    "def make_windows(sig: np.ndarray, win: int, hop: int) -> np.ndarray:\n",
    "    n = (len(sig) - win)//hop + 1\n",
    "    idx = np.arange(0, n*hop, hop)[:, None] + np.arange(win)\n",
    "    return sig[idx]\n",
    "\n",
    "# ---------- build RAW  (normalised float32) --------------------\n",
    "wins_per = [make_windows(storm_data.quantised[nm], WIN, HOP).astype(np.float32)\n",
    "            / 32768.0 for nm in STN]\n",
    "n_win    = min(w.shape[0] for w in wins_per)\n",
    "RAW      = np.stack([w[:n_win] for w in wins_per], axis=1)   # (n_win,S,WIN)\n",
    "print(f\"✔ RAW built – shape {RAW.shape}  dtype={RAW.dtype}\")\n",
    "\n",
    "# ---------- station graph (fully‑connected) --------------------\n",
    "edge_src, edge_dst = zip(*[(i,j) for i in range(S) for j in range(S) if i!=j])\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# ---------- dataset -------------------------------------------\n",
    "class WinDS(Dataset):\n",
    "    def __init__(self, arr: np.ndarray, sigma=0.03):\n",
    "        self.clean = torch.from_numpy(arr)\n",
    "        self.sigma = sigma\n",
    "    def __len__(self):  return len(self.clean)\n",
    "    def __getitem__(self, i):\n",
    "        clean = self.clean[i]\n",
    "        noisy = clean + self.sigma * torch.randn_like(clean)\n",
    "        return noisy, clean\n",
    "\n",
    "# ---------- model ---------------------------------------------\n",
    "LAT = 32\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.gat = GATv2Conv(h, h, heads=4, concat=False)\n",
    "        self.ln  = nn.LayerNorm(h)\n",
    "    def forward(self, x, ei):\n",
    "        return self.ln(x + torch.relu(self.gat(x, ei)))\n",
    "\n",
    "class GraphCDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1,8,7,2,3),  nn.ReLU(),\n",
    "            nn.Conv1d(8,16,7,2,3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7,2,3),nn.ReLU()\n",
    "        )\n",
    "        self.fc_z = nn.Linear(32*128, LAT)\n",
    "        self.g1 = GBlock(LAT); self.g2 = GBlock(LAT)\n",
    "        self.dec_fc = nn.Linear(LAT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "        )\n",
    "    def forward(self, x):               # x: (B,S,WIN)\n",
    "        B,S,W = x.shape\n",
    "        h = self.enc(x.view(B*S,1,W))\n",
    "        z = torch.relu(self.fc_z(h.flatten(1)))\n",
    "        z = self.g1(z, edge_index)\n",
    "        z = self.g2(z, edge_index)\n",
    "        h = torch.relu(self.dec_fc(z)).view(B*S,32,128)\n",
    "        return self.dec(h).view(B,S,W)\n",
    "\n",
    "# ---------- quick training (1 epoch) ----------------------------\n",
    "torch.set_grad_enabled(True)            # ← ensure Autograd ON\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "\n",
    "dl     = DataLoader(WinDS(RAW, 0.03), batch_size=128, shuffle=True)\n",
    "model  = GraphCDAE().to(DEVICE)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "model.train()\n",
    "for noisy, clean in dl:\n",
    "    noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "    opt.zero_grad()\n",
    "    loss = torch.mean((model(noisy) - clean) ** 2)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "print(f\"✔ Graph‑CDAE trained on {DEVICE}   |   n_win = {n_win:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"build fully connected edges\n",
    "Return a PyG ``edge_index`` for a fully connected directed graph.\n",
    "\n",
    "    Args:\n",
    "        num_nodes: Number of graph nodes (e.g., stations).\n",
    "        self_loops: If ``True``, include i→i edges; default excludes them.\n",
    "        device: Optional torch device for the returned tensor.\n",
    "\n",
    "    Returns:\n",
    "        ``edge_index`` of shape ``(2, E)`` with dtype ``long``.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Graph‑CDAE (v3‑fix)  ·  end‑to‑end cell\n",
    "# ================================================================\n",
    "import math, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from scipy.signal import hilbert, convolve\n",
    "from typing import Dict, List\n",
    "\n",
    "# ---------- constants & bookkeeping ---------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = list(storm_data.quantised)          # station order\n",
    "S         = len(STN)\n",
    "FS        = cfg.fs\n",
    "BURST_LEN = int(0.04 * FS)\n",
    "DEVICE    = ('mps' if torch.backends.mps.is_available()\n",
    "             else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------- build RAW  (n_win, S, WIN) ------------------------------\n",
    "def make_windows(sig: np.ndarray, win: int, hop: int) -> np.ndarray:\n",
    "    n = (len(sig) - win) // hop + 1\n",
    "    idx = np.arange(0, n*hop, hop)[:, None] + np.arange(win)\n",
    "    return sig[idx]\n",
    "\n",
    "win_mats = [make_windows(storm_data.quantised[nm], WIN, HOP)\n",
    "            for nm in STN]\n",
    "n_win    = win_mats[0].shape[0]\n",
    "RAW      = np.stack(win_mats, axis=1).astype(np.float32) / 32768.0  # [-1,1]\n",
    "\n",
    "# ---------- fully‑connected graph -----------------------------------\n",
    "edge_src, edge_dst = zip(*[(i, j) for i in range(S) for j in range(S) if i != j])\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "# ---------- dataset -------------------------------------------------\n",
    "class WinDS(Dataset):\n",
    "    \"\"\"Window dataset yielding (noisy, clean) pairs for denoising.\n",
    "\n",
    "    Args:\n",
    "        arr: Float32 array, shape ``(n_win, S, W)`` in the range [-1, 1].\n",
    "        noise_std: Standard deviation of injected Gaussian noise.\n",
    "\n",
    "    Each sample:\n",
    "        Returns a tuple ``(noisy, clean)`` with shape ``(S, W)`` each.\n",
    "    \"\"\"\n",
    "    def __init__(self, arr: np.ndarray, noise_std=0.03):\n",
    "        self.clean = torch.from_numpy(arr)\n",
    "        self.noise = noise_std\n",
    "    def __len__(self):            return len(self.clean)\n",
    "    def __getitem__(self, i):\n",
    "        c = self.clean[i]\n",
    "        n = c + self.noise * torch.randn_like(c)\n",
    "        return n, c\n",
    "\n",
    "# ---------- Graph‑conditioned CDAE ----------------------------------\n",
    "LAT = 32\n",
    "class _GBlock(nn.Module):\n",
    "    \"\"\"GATv2 block with residual connection and LayerNorm.\n",
    "\n",
    "    Args:\n",
    "        h: Hidden dimensionality (input = output = ``h``).\n",
    "\n",
    "    Forward:\n",
    "        x: Node features, shape ``(N, h)``.\n",
    "        edge_index: Graph connectivity, shape ``(2, E)``.\n",
    "    \"\"\"\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.g  = GATv2Conv(h, h, heads=4, concat=False)\n",
    "        self.ln = nn.LayerNorm(h)\n",
    "    def forward(self, x, ei):     # x: (B*S, LAT)\n",
    "        return self.ln(x + torch.relu(self.g(x, ei)))\n",
    "\n",
    "class GraphCDAE(nn.Module):\n",
    "    \"\"\"Graph‑conditioned CDAE over station windows.\n",
    "\n",
    "    Encoder:\n",
    "        3× Conv1d (stride=2) per (station, window) → Linear to latent.\n",
    "\n",
    "    Graph blocks:\n",
    "        Two GATv2 residual blocks operate on the per‑station latent\n",
    "        vectors, using a fully‑connected station graph.\n",
    "\n",
    "    Decoder:\n",
    "        Linear → 3× ConvTranspose1d (stride=2) back to original length.\n",
    "\n",
    "    Notes:\n",
    "        - Input is batched across windows; the graph is *replicated per batch*\n",
    "          via :meth:`_batch_graph`.\n",
    "        - Convolutional encoder/decoder run independently per station; only\n",
    "          the latent vectors are mixed via the graph attention.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1,  8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32, 7, 2, 3), nn.ReLU())\n",
    "        self.fc_z  = nn.Linear(32*128, LAT)\n",
    "        self.g1, self.g2 = _GBlock(LAT), _GBlock(LAT)\n",
    "        self.dec_fc = nn.Linear(LAT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1))\n",
    "    def forward(self, x):                      # x : (B, S, WIN)\n",
    "        B, S, W = x.shape\n",
    "        h = self.enc(x.view(B*S, 1, W))\n",
    "        z = torch.relu(self.fc_z(h.view(B*S, -1)))\n",
    "        z = self.g2(self.g1(z, edge_index), edge_index)\n",
    "        h = torch.relu(self.dec_fc(z)).view(B*S, 32, 128)\n",
    "        return self.dec(h).view(B, S, W)\n",
    "\n",
    "\"\"\"Graph‑conditioned CDAE over station windows.\n",
    "\n",
    "    Encoder:\n",
    "        3× Conv1d (stride=2) per (station, window) → Linear to latent.\n",
    "\n",
    "    Graph blocks:\n",
    "        Two GATv2 residual blocks operate on the per‑station latent\n",
    "        vectors, using a fully‑connected station graph.\n",
    "\n",
    "    Decoder:\n",
    "        Linear → 3× ConvTranspose1d (stride=2) back to original length.\n",
    "\n",
    "    Notes:\n",
    "        - Input is batched across windows; the graph is *replicated per batch*\n",
    "          via :meth:`_batch_graph`.\n",
    "        - Convolutional encoder/decoder run independently per station; only\n",
    "          the latent vectors are mixed via the graph attention.\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"Replicate a base ``edge_index`` across a batch of disjoint graphs.\n",
    "\n",
    "        Args:\n",
    "            ei: Base edge_index over ``S`` nodes, shape ``(2, E)``.\n",
    "            B:  Batch size (number of graphs to replicate).\n",
    "            S:  Nodes per graph (stations).\n",
    "\n",
    "        Returns:\n",
    "            Batched edge_index over ``B*S`` nodes, shape ``(2, B*E)``.\n",
    "        \"\"\"\n",
    "\n",
    "\"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Float tensor of shape ``(B, S, W)`` in [-1, 1].\n",
    "            base_edge_index: Fully‑connected graph over ``S`` nodes (2, E).\n",
    "\n",
    "        Returns:\n",
    "            Reconstructed tensor of the same shape as ``x``.\n",
    "        \"\"\"\n",
    "\n",
    "\"\"\"Graph‑CDAE detector with robust post‑processing.\n",
    "\n",
    "    Trains a single :class:`GraphCDAE` across all stations jointly, then\n",
    "    flags windows with high reconstruction error. A Hilbert‑envelope gate\n",
    "    and a 1‑D majority smoother reduce spurious triggers.\n",
    "\n",
    "    Parameters:\n",
    "        win, hop: Window geometry (samples).\n",
    "        latent: CDAE bottleneck width.\n",
    "        noise_std: Training noise injected into inputs (denoising objective).\n",
    "        epochs: Training epochs.\n",
    "        batch: Mini‑batch size (windows).\n",
    "        z_mad: Robust z‑score threshold (MAD units) on |reconstruction error|.\n",
    "        pct_env: Envelope percentile gate per station (0–100).\n",
    "        smooth_kernel: Sequence of ones for majority smoothing (e.g., (1,1,1)).\n",
    "        device: Override compute device; defaults to 'mps'→'cuda'→'cpu'.\n",
    "\n",
    "    Instance attributes:\n",
    "        station_order: List[str] defining the consistent station order.\n",
    "        edge_index: Base fully‑connected station graph (no self‑loops).\n",
    "        model: Trained :class:`GraphCDAE` in eval mode.\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"Build normalised window stack ``(n_win, S, W)`` from raw ADC.\n",
    "\n",
    "        - Preserves the mapping order from the input dict on first use and\n",
    "          reuses it on subsequent calls to keep station alignment stable.\n",
    "        - Truncates to the minimum window count across stations.\n",
    "\n",
    "        Returns:\n",
    "            Float32 array in [-1, 1], shape ``(n_win, S, self.win)``.\n",
    "        \"\"\"\n",
    "\n",
    "\"\"\"Compute per‑window MAE per station.\n",
    "\n",
    "        Args:\n",
    "            arr: Float32 array, shape ``(n_win, S, W)``.\n",
    "\n",
    "        Returns:\n",
    "            ``err``: Float32 array ``(n_win, S)`` with mean‑absolute error.\n",
    "        \"\"\"\n",
    "\"\"\"Train the Graph‑CDAE on stacked station windows.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping ``station → 1‑D int/float array`` (same length).\n",
    "\n",
    "        Side effects:\n",
    "            - Sets :attr:`station_order`, :attr:`edge_index`, and :attr:`model`.\n",
    "        \"\"\"\n",
    "\n",
    "\"\"\"Infer Boolean “hot” window masks per station.\n",
    "\n",
    "        Post‑processing per station:\n",
    "            1) Robust z‑score on MAE using median/MAD (``> z_mad``).\n",
    "            2) Envelope gate: Hilbert envelope peak per window must exceed\n",
    "               the ``pct_env`` percentile.\n",
    "            3) Majority smoothing via 1‑D convolution with ``smooth_kernel``.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping ``station → 1‑D array`` to score.\n",
    "\n",
    "        Returns:\n",
    "            Dict ``station → (n_win,) bool`` aligned to :attr:`station_order`.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: if called before :meth:`fit`.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- train ----------------------------------------------------\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "dl = DataLoader(WinDS(RAW, noise_std=0.03),\n",
    "                batch_size=256, shuffle=True, drop_last=True)\n",
    "model = GraphCDAE().to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "\n",
    "with torch.enable_grad():                       # ← **fix**\n",
    "    model.train()\n",
    "    for ep in range(1, 4):                      # 3 epochs\n",
    "        for noisy, clean in dl:\n",
    "            noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            out  = model(noisy)\n",
    "            loss = torch.mean((out - clean)**2)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f\"epoch {ep}/3   mse={loss.item():.4e}\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"✔ trained Graph‑CDAE on {DEVICE}  (n_win={n_win:,})\")\n",
    "\n",
    "# ---------- reconstruction error ------------------------------------\n",
    "err = np.empty((n_win, S), np.float32)\n",
    "with torch.no_grad():\n",
    "    for s in range(0, n_win, 256):\n",
    "        e   = min(n_win, s+256)\n",
    "        x   = torch.from_numpy(RAW[s:e]).to(DEVICE)\n",
    "        rec = model(x).cpu().numpy()\n",
    "        err[s:e] = np.mean(np.abs(rec - x.cpu().numpy()), axis=2)\n",
    "\n",
    "# ---------- robust z‑score + envelope gate --------------------------\n",
    "hot: Dict[str, np.ndarray] = {}\n",
    "for i, nm in enumerate(STN):\n",
    "    med = np.median(err[:, i])\n",
    "    mad = np.median(np.abs(err[:, i] - med)) + 1e-9\n",
    "    mask = (err[:, i] - med) / mad > 2.5            # 2.5 MAD\n",
    "    env  = np.abs(hilbert(storm_data.quantised[nm].astype(np.float32)))\n",
    "    pk   = np.max(make_windows(env, WIN, HOP)[:n_win], axis=1)\n",
    "    mask &= pk > np.percentile(pk, 95)              # 95‑% envelope\n",
    "    # one‑step majority smoothing\n",
    "    mask = convolve(mask.astype(int), [1,1,1], mode='same') >= 2\n",
    "    hot[nm] = mask\n",
    "\n",
    "# ---------- strict evaluation ---------------------------------------\n",
    "station_m, net_m, _ = evaluate_windowed_model(\n",
    "    hot            = hot,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                                burst_len=BURST_LEN, min_stn=2, tol_win=0),\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "print(\"\\n—— Station‑level window metrics ——\")\n",
    "for nm, m in station_m.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network‑level stroke metrics ——\")\n",
    "#print(net_m)\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m, ndigits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Graph-CDAE (v3-fix)  ·  end-to-end cell  — with correct batched graph\n",
    "\n",
    "#########################use this\n",
    "# fixed....\n",
    "# ================================================================\n",
    "import math, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from scipy.signal import hilbert, convolve\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ---------- constants & bookkeeping ---------------------------------\n",
    "WIN, HOP  = 1024, 512\n",
    "STN       = list(storm_data.quantised)          # station order\n",
    "S         = len(STN)\n",
    "FS        = cfg.fs\n",
    "BURST_LEN = int(0.04 * FS)\n",
    "DEVICE    = ('mps' if torch.backends.mps.is_available()\n",
    "             else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------- build RAW  (n_win, S, WIN) ------------------------------\n",
    "def make_windows(sig: np.ndarray, win: int, hop: int) -> np.ndarray:\n",
    "    n = (len(sig) - win) // hop + 1\n",
    "    idx = np.arange(0, n*hop, hop)[:, None] + np.arange(win)\n",
    "    return sig[idx]\n",
    "\n",
    "win_mats = [make_windows(storm_data.quantised[nm], WIN, HOP) for nm in STN]\n",
    "n_win    = min(w.shape[0] for w in win_mats)                          # <- safe min\n",
    "win_mats = [w[:n_win] for w in win_mats]\n",
    "RAW      = np.stack(win_mats, axis=1).astype(np.float32) / 32768.0    # [-1,1]\n",
    "\n",
    "# ---------- fully-connected base graph (S nodes, CPU) ----------------\n",
    "edge_src, edge_dst = zip(*[(i, j) for i in range(S) for j in range(S) if i != j])\n",
    "BASE_EDGE_INDEX = torch.tensor([edge_src, edge_dst], dtype=torch.long)  # [2,E] on CPU\n",
    "\n",
    "def batch_graph(ei: torch.Tensor, B: int, S: int, device=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tile an S-node graph B times (block-diagonal) for (B*S) nodes.\n",
    "    Returns [2, B*E] edge_index on the requested device.\n",
    "    \"\"\"\n",
    "    if device is None: device = ei.device\n",
    "    ei = ei.to(device)                         # [2,E]\n",
    "    ei_b = ei.unsqueeze(0).repeat(B, 1, 1)     # [B,2,E]\n",
    "    offsets = (torch.arange(B, device=device) * S).view(B, 1, 1)\n",
    "    ei_b = ei_b + offsets                      # add batch offsets to both rows\n",
    "    ei_b = ei_b.permute(1, 0, 2).reshape(2, -1)\n",
    "    return ei_b\n",
    "\n",
    "# ---------- dataset -------------------------------------------------\n",
    "class WinDS(Dataset):\n",
    "    def __init__(self, arr: np.ndarray, noise_std=0.03):\n",
    "        self.clean = torch.from_numpy(arr)\n",
    "        self.noise = noise_std\n",
    "    def __len__(self):            return len(self.clean)\n",
    "    def __getitem__(self, i):\n",
    "        c = self.clean[i]\n",
    "        n = c + self.noise * torch.randn_like(c)\n",
    "        return n, c\n",
    "\n",
    "# ---------- Graph-conditioned CDAE ----------------------------------\n",
    "LAT = 32\n",
    "class _GBlock(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        self.g  = GATv2Conv(h, h, heads=4, concat=False)\n",
    "        self.ln = nn.LayerNorm(h)\n",
    "    def forward(self, x, ei):     # x: (B*S, LAT)\n",
    "        return self.ln(x + torch.relu(self.g(x, ei)))\n",
    "\n",
    "class GraphCDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1,  8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32, 7, 2, 3), nn.ReLU())\n",
    "        self.fc_z  = nn.Linear(32*128, LAT)\n",
    "        self.g1, self.g2 = _GBlock(LAT), _GBlock(LAT)\n",
    "        self.dec_fc = nn.Linear(LAT, 32*128)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8,1,7,2,3,output_padding=1))\n",
    "    def forward(self, x):                      # x : (B, S, WIN)\n",
    "        B, S, W = x.shape\n",
    "        h = self.enc(x.view(B*S, 1, W))\n",
    "        z = torch.relu(self.fc_z(h.view(B*S, -1)))           # (B*S, LAT)\n",
    "        # -------- batched graph FIX: tile the S-node graph B times ------\n",
    "        ei_b = batch_graph(BASE_EDGE_INDEX, B, S, device=z.device)\n",
    "        z = self.g2(self.g1(z, ei_b), ei_b)\n",
    "        h = torch.relu(self.dec_fc(z)).view(B*S, 32, 128)\n",
    "        return self.dec(h).view(B, S, W)\n",
    "\n",
    "# ---------- train ----------------------------------------------------\n",
    "torch.manual_seed(0); np.random.seed(0)\n",
    "pin_mem = (DEVICE == 'cuda')  # MPS can't pin efficiently\n",
    "dl = DataLoader(WinDS(RAW, noise_std=0.03),\n",
    "                batch_size=256, shuffle=True, drop_last=True, pin_memory=pin_mem)\n",
    "model = GraphCDAE().to(DEVICE)\n",
    "opt   = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "\n",
    "with torch.enable_grad():                       # ← keep training enabled\n",
    "    model.train()\n",
    "    for ep in range(1, 4):                      # 3 epochs\n",
    "        for noisy, clean in dl:\n",
    "            noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            out  = model(noisy)\n",
    "            loss = torch.mean((out - clean)**2)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        print(f\"epoch {ep}/3   mse={loss.item():.4e}\")\n",
    "\n",
    "model.eval()\n",
    "print(f\"✔ trained Graph-CDAE on {DEVICE}  (n_win={n_win:,})\")\n",
    "\n",
    "# ---------- reconstruction error ------------------------------------\n",
    "err = np.empty((n_win, S), np.float32)\n",
    "with torch.no_grad():\n",
    "    for s in range(0, n_win, 256):\n",
    "        e   = min(n_win, s+256)\n",
    "        x   = torch.from_numpy(RAW[s:e]).to(DEVICE)\n",
    "        rec = model(x).cpu().numpy()\n",
    "        err[s:e] = np.mean(np.abs(rec - x.cpu().numpy()), axis=2)\n",
    "\n",
    "# ---------- robust z-score + envelope gate (unchanged) --------------\n",
    "hot: Dict[str, np.ndarray] = {}\n",
    "for i, nm in enumerate(STN):\n",
    "    med = np.median(err[:, i])\n",
    "    mad = np.median(np.abs(err[:, i] - med)) + 1e-9\n",
    "    mask = (err[:, i] - med) / mad > 3.5            # 2.5 MAD\n",
    "    #env  = np.abs(hilbert(storm_data.quantised[nm].astype(np.float32)))\n",
    "    #pk   = np.max(make_windows(env, WIN, HOP)[:n_win], axis=1)\n",
    "    #mask &= pk > np.percentile(pk, 90)              # 95-% envelope\n",
    "    # one-step majority smoothing\n",
    "    mask = convolve(mask.astype(int), [1,1,1], mode='same') >= 2\n",
    "    hot[nm] = mask\n",
    "\n",
    "# ---------- strict evaluation ---------------------------------------\n",
    "station_m, net_m, _ = evaluate_windowed_model(\n",
    "    hot            = hot,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                                burst_len=BURST_LEN, min_stn=2, tol_win=0),\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "print(\"\\n—— Station-level window metrics ——\")\n",
    "for nm, m in station_m.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network-level stroke metrics ——\")\n",
    "print(\"\\nNetwork-level metrics:\", pretty_metrics(net_m, ndigits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================== One‑Class SVM (per‑station) ======================================\n",
    "# Unsupervised OC‑SVM with RobustScaler, median‑heuristic gamma, tiny (nu,gamma) grid via stability,\n",
    "# and extreme‑tail rescue. Evaluates with your strict metrics.\n",
    "# ========================================================================================================\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import os, math, numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# ----------------------------- configuration -------------------------------------------------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "\n",
    "BASE_NU    = 0.0015               # target outlier fraction (≈ contamination)\n",
    "NU_GRID    = [max(1e-4, 0.5*BASE_NU), BASE_NU, min(0.02, 2.0*BASE_NU)]\n",
    "GAM_FACT   = [0.5, 1.0, 2.0]      # gamma multipliers around median heuristic\n",
    "EXTREME_Q  = 99.95                # “rescue” tail (lower score = more anomalous)\n",
    "TRAIN_MAX  = 3000                 # subsample for OC‑SVM training (keeps it fast)\n",
    "ANCHORS    = 64                   # for fast median‑heuristic\n",
    "SEED       = 42\n",
    "rng        = np.random.default_rng(SEED)\n",
    "\n",
    "# ----------------------------- feature extractor ---------------------------------------------------------\n",
    "# Uses the same features you already use with IF\n",
    "fx = FeatureExtractor([\"iso16\"])\n",
    "\n",
    "def _median_sqdist_fast(X: np.ndarray, anchors:int=ANCHORS, subsample:int=4000) -> float:\n",
    "    \"\"\"Approximate **median squared L2 distance** between rows of `X`.\n",
    "\n",
    "    Complexity is O(n·k) with k=anchors (vs O(n²)), suitable for quick\n",
    "    γ ≈ 1 / median(‖xi − xj‖²) estimation.\n",
    "\n",
    "    Args:\n",
    "        X: 2‑D feature matrix (n, d).\n",
    "        anchors: Size of the anchor set used to probe distances.\n",
    "        subsample: Cap on rows considered from X (random).\n",
    "        rng: Optional numpy Generator for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Median of squared distances (float). Falls back to 1.0 if degenerate.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    m = min(n, subsample)\n",
    "    idx = rng.choice(n, m, replace=False) if m < n else np.arange(n)\n",
    "    Xa  = X[idx]\n",
    "    k = min(anchors, m)\n",
    "    aidx = rng.choice(m, k, replace=False) if k < m else np.arange(m)\n",
    "    A = Xa[aidx]            # (k,d)\n",
    "    # squared distances to anchors: (m,k)\n",
    "    Xa2 = np.sum(Xa*Xa, axis=1, keepdims=True)\n",
    "    A2  = np.sum(A*A, axis=1, keepdims=True).T\n",
    "    D2  = Xa2 + A2 - 2.0 * Xa @ A.T\n",
    "    D2  = D2[D2 > 1e-12]    # drop zeros\n",
    "    if D2.size == 0: return 1.0\n",
    "    return float(np.median(D2))\n",
    "\n",
    "def _fit_ocsvm_stable(Xs: np.ndarray, nu_grid, gam_grid) -> Tuple[OneClassSVM, float, float]:\n",
    "    \"\"\"Pick (ν, γ) by **stability across a random split**, then fit on all Xs.\n",
    "\n",
    "    Procedure:\n",
    "        1) Split Xs → halves A/B.\n",
    "        2) For each (ν, γ):\n",
    "           - Fit on A, measure flagged fraction on B; fit on B, measure flagged on A.\n",
    "           - Objective: |p_B − ν| + |p_A − ν| (closer to target ν is better).\n",
    "        3) Choose the pair with minimal objective; refit on all Xs.\n",
    "\n",
    "    Returns:\n",
    "        (fitted_model, best_ν, best_γ). If all candidates fail, falls back to\n",
    "        γ from the median heuristic and ν = middle of nu_grid (if available).\n",
    "    \"\"\"\n",
    "    n = Xs.shape[0]\n",
    "    idx = rng.permutation(n)\n",
    "    mid = n // 2\n",
    "    A, B = Xs[idx[:mid]], Xs[idx[mid:]]\n",
    "    best = (1e9, None, None)   # (objective, nu, gamma)\n",
    "\n",
    "    for nu in nu_grid:\n",
    "        for gam in gam_grid:\n",
    "            try:\n",
    "                mA = OneClassSVM(kernel='rbf', gamma=gam, nu=nu, tol=1e-3, cache_size=256).fit(A)\n",
    "                sB = mA.decision_function(B)          # >0 inlier, <0 outlier\n",
    "                pB = float((sB < 0).mean())\n",
    "                mB = OneClassSVM(kernel='rbf', gamma=gam, nu=nu, tol=1e-3, cache_size=256).fit(B)\n",
    "                sA = mB.decision_function(A)\n",
    "                pA = float((sA < 0).mean())\n",
    "                obj = abs(pB - nu) + abs(pA - nu)\n",
    "            except Exception as e:\n",
    "                # on numeric failure, skip\n",
    "                continue\n",
    "            if obj < best[0]:\n",
    "                best = (obj, nu, gam)\n",
    "\n",
    "    # train final on all data\n",
    "    if best[1] is None:\n",
    "        # fallback: gamma=median heuristic, nu=BASE_NU\n",
    "        med2 = _median_sqdist_fast(Xs)\n",
    "        gam0 = 1.0 / max(med2, 1e-6)\n",
    "        best_nu, best_gam = BASE_NU, gam0\n",
    "    else:\n",
    "        best_nu, best_gam = best[1], best[2]\n",
    "\n",
    "    model = OneClassSVM(kernel='rbf', gamma=best_gam, nu=best_nu, tol=1e-3, cache_size=512).fit(Xs)\n",
    "    return model, best_nu, best_gam\n",
    "\n",
    "# ----------------------------- wrapper class -------------------------------------------------------------\n",
    "class ExtendedOCSVM:\n",
    "    \"\"\"Per‑station **One‑Class SVM** detector with robust scaling and safeguards.\n",
    "\n",
    "    **Model summary (per station)**\n",
    "    -------------------------------\n",
    "    - **Features**: Any windowed feature block emitted by a FeatureExtractor\n",
    "      (default: ``\"iso16\"``). Windows are aligned via (win, hop).\n",
    "    - **Pre‑processing**: :class:`RobustScaler` (median/IQR) to temper outliers.\n",
    "    - **OC‑SVM**: RBF kernel with γ initialised by the **median‑heuristic** and\n",
    "      fine‑tuned via a tiny **stability grid** over (ν, γ).\n",
    "    - **Decision**: A window is “hot” if OC‑SVM’s decision function is **< 0**\n",
    "      (standard boundary) **or** below an **extreme‑tail rescue** threshold\n",
    "      (station‑specific percentile of training scores).\n",
    "    - **Smoothing**: Optional 1‑D majority smoothing with kernel [1, 1, 1].\n",
    "\n",
    "    Parameters:\n",
    "        fx: Feature extractor object with\n",
    "            ``transform(raw, win, hop, fs) -> (by_station, n_win)``.\n",
    "            If ``None``, a default ``FeatureExtractor([\"iso16\"])`` is constructed.\n",
    "        win, hop: Window geometry in samples.\n",
    "        extreme_q: Percentile in (0, 100]; scores below this **(lower = more\n",
    "            anomalous)** are forced hot (recall “rescue”).\n",
    "        train_max: Max windows to subsample per station for OC‑SVM training.\n",
    "        nu_grid: Small iterable of target outlier fractions (ν).\n",
    "        gam_factors: Multipliers around the median‑heuristic γ to form a tiny grid.\n",
    "        anchors: Anchor count for the fast median‑heuristic.\n",
    "        seed: RNG seed controlling sub‑sampling and stability split.\n",
    "\n",
    "    Instance attributes:\n",
    "        mods: ``{station -> (scaler, ocsvm, rescue_thr, (nu_hat, gamma_hat))}``.\n",
    "        rng:  ``np.random.Generator`` for reproducibility.\n",
    "\n",
    "    Notes:\n",
    "        - The OC‑SVM decision function is **higher for more normal** windows.\n",
    "        - Extreme‑tail rescue uses the **training** score distribution on *all*\n",
    "          windows (not just the subsample) for robustness.\n",
    "    \"\"\"\n",
    "    def __init__(self, win:int=WIN, hop:int=HOP, extreme_q:float=EXTREME_Q,\n",
    "                 train_max:int=TRAIN_MAX, nu_grid=NU_GRID, gam_factors=GAM_FACT):\n",
    "        self.win=win; self.hop=hop; self.extreme_q=extreme_q\n",
    "        self.train_max=train_max\n",
    "        self.nu_grid=nu_grid\n",
    "        self.gam_factors=gam_factors\n",
    "        self.mods : Dict[str, tuple] = {}   # nm → (scaler, ocsvm, thr_ext, (nu,gamma))\n",
    "\n",
    "    def fit(self, raw:Dict[str,np.ndarray], fs:int, verbose=True):\n",
    "        \"\"\"Fit one OC‑SVM per station and derive a rescue threshold.\n",
    "\n",
    "        Steps per station:\n",
    "            1) Extract features (aligned via win/hop/fs).\n",
    "            2) Fit :class:`RobustScaler`; transform all windows → Xs_all.\n",
    "            3) Subsample up to `train_max` windows → Xs (for speed).\n",
    "            4) Compute γ₀ via the fast median heuristic on Xs; build γ grid\n",
    "               as `gam_factors × γ₀`.\n",
    "            5) Run the **stability search** over (ν, γ) and refit on Xs.\n",
    "            6) Compute OC‑SVM **decision scores** on Xs_all and set\n",
    "               `rescue_thr = percentile(scores_all, 100 - extreme_q)`.\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping ``station → 1‑D array`` (same length across stations).\n",
    "            fs: Sampling rate in Hz (forwarded to the feature extractor).\n",
    "            verbose: If True, print brief fit statistics per station.\n",
    "\n",
    "        Returns:\n",
    "            self (for chaining).\n",
    "        \"\"\"\n",
    "        feats,_ = fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        for nm, X in feats.items():\n",
    "            # scale\n",
    "            scaler = RobustScaler().fit(X)\n",
    "            Xs_all = scaler.transform(X)\n",
    "\n",
    "            # subsample for training the OC‑SVM\n",
    "            n = Xs_all.shape[0]\n",
    "            m = min(self.train_max, n)\n",
    "            idx = rng.choice(n, m, replace=False) if m < n else np.arange(n)\n",
    "            Xs = Xs_all[idx]\n",
    "\n",
    "            # gamma median heuristic + tiny grid\n",
    "            med2   = _median_sqdist_fast(Xs)\n",
    "            gam0   = 1.0 / max(med2, 1e-6)\n",
    "            gam_grid = [max(1e-6, f*gam0) for f in self.gam_factors]\n",
    "\n",
    "            oc, nu_hat, gam_hat = _fit_ocsvm_stable(Xs, self.nu_grid, gam_grid)\n",
    "\n",
    "            # station‑specific extreme tail on full (for robust rescue)\n",
    "            scores_all = oc.decision_function(Xs_all)       # >0 normal, <0 anomalous\n",
    "            thr_ext    = np.percentile(scores_all, 100 - self.extreme_q)\n",
    "\n",
    "            self.mods[nm] = (scaler, oc, float(thr_ext), (nu_hat, gam_hat))\n",
    "\n",
    "            if verbose:\n",
    "                flagged = (scores_all < 0).mean()\n",
    "                print(f\"{nm}: nu≈{nu_hat:.4f}  gamma≈{gam_hat:.3g}  \"\n",
    "                      f\"train flagged≈{flagged:.2%}  ext_thr={thr_ext:.4e}\")\n",
    "\n",
    "    def predict(self, raw:Dict[str,np.ndarray], fs:int, smooth=True) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Return Boolean hot masks per station using the learned models.\n",
    "\n",
    "        Decision rule per window:\n",
    "            ``hot = (decision_function < 0) OR (decision_function < rescue_thr)``\n",
    "\n",
    "        Args:\n",
    "            raw: Mapping station → 1‑D waveform to score.\n",
    "            fs: Sampling rate in Hz (passed to the feature extractor).\n",
    "            smooth: If True, apply majority smoothing with `smooth_kernel`.\n",
    "            smooth_kernel: Sequence of ones used for 1‑D convolution;\n",
    "                majority is computed as ≥ ceil(sum(kernel)/2).\n",
    "\n",
    "        Returns:\n",
    "            Dict ``station → (n_windows_aligned,) bool``.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: if a station has not been fitted yet.\n",
    "        \"\"\"\n",
    "        feats,_ = fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        out = {}\n",
    "        for nm, X in feats.items():\n",
    "            scaler, oc, thr_ext, _ = self.mods[nm]\n",
    "            Xs = scaler.transform(X)\n",
    "            s  = oc.decision_function(Xs)             # higher = more normal\n",
    "            mask = (s < 0.0) | (s < thr_ext)          # main + rescue\n",
    "            if smooth:\n",
    "                mask = convolve(mask.astype(int), [1,1,1], mode='same') >= 2\n",
    "            out[nm] = mask\n",
    "        return out\n",
    "\n",
    "# ========================================= run it ========================================================\n",
    "model_oc = ExtendedOCSVM()\n",
    "\n",
    "print(\"▶ Fitting One‑Class SVMs (per station) …\")\n",
    "model_oc.fit(storm_data.quantised, fs=FS)\n",
    "\n",
    "print(\"\\n▶ Predicting …\")\n",
    "hot_oc = model_oc.predict(storm_data.quantised, fs=FS, smooth=True)\n",
    "\n",
    "# ================================== strict evaluation ====================================================\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                      burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "\n",
    "station_m_oc, net_m_oc, n_win_oc = evaluate_windowed_model(\n",
    "    hot            = hot_oc,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ========================================= summary =======================================================\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_oc:,}) ——\")\n",
    "for nm, m in station_m_oc.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(\"\\nNetwork‑level metrics:\", pretty_metrics(net_m_oc, ndigits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets go bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== OC-SVM (per-station) — Strictly-No-Worse Upgrade + Proof [FIXED] ===================\n",
    "from typing import Dict, Tuple, List\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# ---------------------------- config (safe on M1) --------------------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "\n",
    "BASE_NU    = 0.0015\n",
    "NU_GRID    = [max(1e-4, 0.5*BASE_NU), BASE_NU, min(0.02, 2.0*BASE_NU)]\n",
    "GAM_FACT   = [0.5, 1.0, 2.0]\n",
    "EXTREME_Q  = 99.95\n",
    "TRAIN_MAX  = 3000\n",
    "ANCHORS    = 64\n",
    "SEED       = 42\n",
    "rng        = np.random.default_rng(SEED)\n",
    "\n",
    "# Conformal & smoothing knobs\n",
    "ALPHA_MIN  = 0.005     # 0.5% lower bound for α_station\n",
    "ALPHA_MAX  = 0.06      # 6% upper bound for α_station\n",
    "SMOOTH_KRN = [1,1,1]   # majority smoothing kernel (must be odd length)\n",
    "\n",
    "# Use your existing feature set\n",
    "fx = FeatureExtractor([\"iso16\"])\n",
    "\n",
    "# ---------------------------- helpers -------------------------------------------\n",
    "def _median_sqdist_fast(X: np.ndarray, anchors:int=ANCHORS, subsample:int=4000) -> float:\n",
    "    n = X.shape[0]\n",
    "    m = min(n, subsample)\n",
    "    idx = rng.choice(n, m, replace=False) if m < n else np.arange(n)\n",
    "    Xa  = X[idx]\n",
    "    k = min(anchors, m)\n",
    "    aidx = rng.choice(m, k, replace=False) if k < m else np.arange(m)\n",
    "    A = Xa[aidx]\n",
    "    Xa2 = np.sum(Xa*Xa, axis=1, keepdims=True)\n",
    "    A2  = np.sum(A*A, axis=1, keepdims=True).T\n",
    "    D2  = Xa2 + A2 - 2.0 * Xa @ A.T\n",
    "    D2  = D2[D2 > 1e-12]\n",
    "    return float(np.median(D2)) if D2.size else 1.0\n",
    "\n",
    "def _fit_ocsvm_stable(Xs: np.ndarray, nu_grid, gam_grid, tol=1e-3, cache_mb=256) -> Tuple[OneClassSVM, float, float]:\n",
    "    \"\"\"Pick (ν,γ) by split stability; refit on all Xs.\"\"\"\n",
    "    n = Xs.shape[0]\n",
    "    if n < 20:\n",
    "        med2 = _median_sqdist_fast(Xs); gam0 = 1.0/max(med2,1e-6)\n",
    "        oc = OneClassSVM(kernel='rbf', gamma=gam0, nu=BASE_NU, tol=tol, cache_size=cache_mb).fit(Xs)\n",
    "        return oc, BASE_NU, gam0\n",
    "    idx = rng.permutation(n); mid = n//2\n",
    "    A, B = Xs[idx[:mid]], Xs[idx[mid:]]\n",
    "    best = (1e9, None, None)\n",
    "    for nu in nu_grid:\n",
    "        for gam in gam_grid:\n",
    "            try:\n",
    "                mA = OneClassSVM(kernel='rbf', gamma=gam, nu=nu, tol=tol, cache_size=cache_mb).fit(A)\n",
    "                pB = float((mA.decision_function(B) < 0).mean())\n",
    "                mB = OneClassSVM(kernel='rbf', gamma=gam, nu=nu, tol=tol, cache_size=cache_mb).fit(B)\n",
    "                pA = float((mB.decision_function(A) < 0).mean())\n",
    "                obj = abs(pB - nu) + abs(pA - nu)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if obj < best[0]:\n",
    "                best = (obj, nu, gam)\n",
    "    if best[1] is None:\n",
    "        med2 = _median_sqdist_fast(Xs); gam0 = 1.0/max(med2,1e-6)\n",
    "        nu_hat, gam_hat = BASE_NU, gam0\n",
    "    else:\n",
    "        nu_hat, gam_hat = float(best[1]), float(best[2])\n",
    "    model = OneClassSVM(kernel='rbf', gamma=gam_hat, nu=nu_hat, tol=tol, cache_size=512).fit(Xs)\n",
    "    return model, nu_hat, gam_hat\n",
    "\n",
    "def _conformal_pvalues(cal_scores: np.ndarray, test_scores: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Split-conformal p-values on OC-SVM decision scores. Nonconformity A = -score.\"\"\"\n",
    "    A_cal  = -cal_scores.reshape(-1)\n",
    "    A_test = -test_scores.reshape(-1)\n",
    "    A_cal_sorted = np.sort(A_cal)\n",
    "    idx = np.searchsorted(A_cal_sorted, A_test, side='left')  # count < A_test\n",
    "    geq = A_cal.size - idx\n",
    "    return (geq + 1.0) / (A_cal.size + 1.0)\n",
    "\n",
    "def _smooth_bool(mask: np.ndarray, kernel: List[int]) -> np.ndarray:\n",
    "    \"\"\"Majority smoothing that preserves dtype=bool.\"\"\"\n",
    "    krn = np.array(kernel, dtype=int)\n",
    "    half = int(np.ceil(krn.sum()/2))\n",
    "    return (convolve(mask.astype(int), krn, mode='same') >= half)\n",
    "\n",
    "# ---------------------------- main wrapper --------------------------------------\n",
    "class OCSVM_StrictBetter:\n",
    "    \"\"\"\n",
    "    Per-station OC-SVM with:\n",
    "      • robust scaling, stable (ν,γ), extreme-tail rescue,\n",
    "      • split-conformal p-values on held-out calibration windows,\n",
    "      • α per station ≈ baseline flagged fraction (clipped),\n",
    "      • SUPERCET after smoothing:\n",
    "            base_raw  = (s<0) OR (s<thr_ext)\n",
    "            base_sm   = smooth(base_raw)\n",
    "            conf_raw  = (pval < α_station)\n",
    "            add_sm    = smooth( (~base_raw) & conf_raw )\n",
    "            improved  = base_sm OR add_sm\n",
    "\n",
    "    Guarantees (with same evaluator & min_stn):\n",
    "      • per-station recall ≥ baseline recall\n",
    "      • network recall   ≥ baseline recall\n",
    "    \"\"\"\n",
    "    def __init__(self, win=WIN, hop=HOP, extreme_q=EXTREME_Q, train_max=TRAIN_MAX,\n",
    "                 nu_grid=NU_GRID, gam_factors=GAM_FACT,\n",
    "                 alpha_min=ALPHA_MIN, alpha_max=ALPHA_MAX,\n",
    "                 smooth_kernel:List[int]=SMOOTH_KRN):\n",
    "        self.win=win; self.hop=hop; self.extreme_q=extreme_q\n",
    "        self.train_max=train_max\n",
    "        self.nu_grid=nu_grid; self.gam_factors=gam_factors\n",
    "        self.alpha_min=alpha_min; self.alpha_max=alpha_max\n",
    "        self.smooth_kernel = smooth_kernel\n",
    "        self.mods: Dict[str, dict] = {}\n",
    "\n",
    "    def fit(self, raw:Dict[str,np.ndarray], fs:int, verbose=True):\n",
    "        feats,_ = fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        for nm, X in feats.items():\n",
    "            scaler = RobustScaler().fit(X)\n",
    "            Xs_all = scaler.transform(X)\n",
    "\n",
    "            # TRAIN vs CAL split (disjoint)\n",
    "            n = Xs_all.shape[0]\n",
    "            m = min(self.train_max, n)\n",
    "            idx_train = rng.choice(n, m, replace=False)\n",
    "            mask_train = np.zeros(n, dtype=bool); mask_train[idx_train] = True\n",
    "            Xs_train = Xs_all[mask_train]\n",
    "            Xs_cal   = Xs_all[~mask_train] if (~mask_train).any() else Xs_all.copy()\n",
    "\n",
    "            # Stable (ν,γ)\n",
    "            med2     = _median_sqdist_fast(Xs_train)\n",
    "            gam0     = 1.0/max(med2,1e-6)\n",
    "            gam_grid = [max(1e-6, f*gam0) for f in self.gam_factors]\n",
    "            oc, nu_hat, gam_hat = _fit_ocsvm_stable(Xs_train, self.nu_grid, gam_grid)\n",
    "\n",
    "            # Baseline rule on ALL windows (for flagged fraction & rescue)\n",
    "            s_all   = oc.decision_function(Xs_all)    # >0 normal\n",
    "            thr_ext = np.percentile(s_all, 100 - self.extreme_q)\n",
    "            base_raw_all = (s_all < 0.0) | (s_all < thr_ext)\n",
    "            flagged_frac = float(base_raw_all.mean())\n",
    "\n",
    "            # α_station ≈ baseline flagged rate (clipped)\n",
    "            alpha_st = float(np.clip(flagged_frac, self.alpha_min, self.alpha_max))\n",
    "\n",
    "            # Held-out calibration scores for conformal\n",
    "            s_cal = oc.decision_function(Xs_cal)\n",
    "\n",
    "            self.mods[nm] = dict(\n",
    "                scaler=scaler, ocsvm=oc, thr_ext=float(thr_ext),\n",
    "                alpha=alpha_st, cal_scores=s_cal\n",
    "            )\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{nm}: nu≈{nu_hat:.4f}  gamma≈{gam_hat:.3g}  \"\n",
    "                      f\"flagged_frac≈{flagged_frac:.3%}  α_stn={alpha_st:.3%}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, raw:Dict[str,np.ndarray], fs:int, return_both=True):\n",
    "        feats,_ = fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        base_masks: Dict[str,np.ndarray] = {}\n",
    "        imp_masks : Dict[str,np.ndarray] = {}\n",
    "        for nm, X in feats.items():\n",
    "            cfg = self.mods[nm]\n",
    "            scaler, oc, thr_ext, alpha, cal_sc = cfg['scaler'], cfg['ocsvm'], cfg['thr_ext'], cfg['alpha'], cfg['cal_scores']\n",
    "            Xs = scaler.transform(X)\n",
    "            s  = oc.decision_function(Xs)\n",
    "\n",
    "            # baseline raw & smoothed\n",
    "            base_raw = (s < 0.0) | (s < thr_ext)\n",
    "            base_sm  = _smooth_bool(base_raw, self.smooth_kernel)\n",
    "\n",
    "            # conformal raw on all windows (held-out calibration)\n",
    "            pvals = _conformal_pvalues(cal_sc, s)\n",
    "            conf_raw = (pvals < alpha)\n",
    "\n",
    "            # only the extra conformal hits that baseline didn’t already have\n",
    "            add_raw = (~base_raw) & conf_raw\n",
    "            add_sm  = _smooth_bool(add_raw, self.smooth_kernel)\n",
    "\n",
    "            # improved = baseline_smoothed OR smoothed additional hits\n",
    "            imp = base_sm | add_sm\n",
    "\n",
    "            base_masks[nm] = base_sm.astype(bool)\n",
    "            imp_masks[nm]  = imp.astype(bool)\n",
    "\n",
    "            # Superset check (post-smoothing)\n",
    "            assert np.all(imp_masks[nm] | ~base_masks[nm]), f\"Superset violated on {nm}\"\n",
    "\n",
    "        return (base_masks, imp_masks) if return_both else imp_masks\n",
    "\n",
    "# ============================== Train / Predict / PROVE =====================================\n",
    "model_oc = OCSVM_StrictBetter()\n",
    "\n",
    "print(\"▶ Fitting strict-better OC-SVMs …\")\n",
    "model_oc.fit(storm_data.quantised, fs=FS)\n",
    "\n",
    "print(\"\\n▶ Predicting baseline vs improved …\")\n",
    "hot_base, hot_impr = model_oc.predict(storm_data.quantised, fs=FS, return_both=True)\n",
    "\n",
    "# Evaluate both with YOUR existing harness (no change to evaluator)\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                      burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "\n",
    "print(\"\\n▶ Evaluating BASELINE …\")\n",
    "st_base, net_base, n_win = evaluate_windowed_model(\n",
    "    hot            = hot_base,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = False\n",
    ")\n",
    "\n",
    "print(\"\\n▶ Evaluating IMPROVED (strict superset) …\")\n",
    "st_impr, net_impr, _ = evaluate_windowed_model(\n",
    "    hot            = hot_impr,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# -------------------------- Print proof (deltas) ----------------------\n",
    "def _fmt(m): return f\"TP={m['TP']}, FP={m['FP']}, FN={m['FN']}, TN={m['TN']}, \" \\\n",
    "                    f\"P={m['P']:.3f}, R={m['R']:.3f}, F1={m['F1']:.3f}\"\n",
    "\n",
    "print(\"\\n—— Network metrics ——\")\n",
    "print(\"Baseline :\", _fmt(net_base))\n",
    "print(\"Improved :\", _fmt(net_impr))\n",
    "print(\"Δ (Imp-Base):\",\n",
    "      f\"ΔTP={net_impr['TP']-net_base['TP']}, \"\n",
    "      f\"ΔFP={net_impr['FP']-net_base['FP']}, \"\n",
    "      f\"ΔFN={net_impr['FN']-net_base['FN']}, \"\n",
    "      f\"ΔTN={net_impr['TN']-net_base['TN']}\")\n",
    "\n",
    "print(f\"\\n—— Station metrics  (n_windows = {n_win:,}) ——\")\n",
    "for nm in STN:\n",
    "    mb, mi = st_base[nm], st_impr[nm]\n",
    "    print(f\"{nm}: Base F1={mb['F1']:.3f}  →  Imp F1={mi['F1']:.3f}  \"\n",
    "          f\"(ΔTP={mi['TP']-mb['TP']}, ΔFP={mi['FP']-mb['FP']}, ΔFN={mi['FN']-mb['FN']})\")\n",
    "\n",
    "print(\"\\nSuperset check passed: each station’s improved mask is a superset of baseline (post-smoothing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== CDAE (per-station) — Strictly-No-Worse Upgrade + Proof ==================\n",
    "# Baseline:   CDAE denoising AE, per-window MSE, threshold at a percentile.\n",
    "# Improve via:\n",
    "#   • Bayesian last-layer (Laplace around MAP) → posterior predictive score\n",
    "#   • Split-conformal p-values on baseline error (held-out calibration)\n",
    "# Final decision is a TRUE SUPERSET after smoothing:\n",
    "#   base_raw  = (mse > base_thr)\n",
    "#   base_sm   = smooth(base_raw)\n",
    "#   add_bayes = smooth( (~base_raw) & (bayes_score > bayes_thr) )            [optional]\n",
    "#   add_conf  = smooth( (~base_raw) & (pval < alpha_station) )               [optional]\n",
    "#   improved  = base_sm OR add_bayes OR add_conf\n",
    "#\n",
    "# Guarantees (with same evaluator & min_stn): per-station recall ≥ baseline, network recall ≥ baseline.\n",
    "\n",
    "import math, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Dict, Tuple, List\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# ----------------------------- config (safe on M1) ---------------------------------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "\n",
    "PCT_THR    = 99.9            # baseline percentile threshold on MSE\n",
    "TRAIN_WIN  = 20_000\n",
    "EPOCHS     = 4\n",
    "BATCH      = 256\n",
    "LATENT     = 32\n",
    "NOISE_STD  = 0.02\n",
    "\n",
    "# Laplace (Bayes last-layer)\n",
    "PRIOR_PREC      = 1e-3       # λ\n",
    "FISHER_BATCHES  = 150\n",
    "SAMPLES_K       = 24\n",
    "LAMBDA_UQ       = 0.5        # weight on recon variance in bayes_score\n",
    "\n",
    "# Conformal\n",
    "ALPHA_MIN  = 0.005           # 0.5% lower bound per-station\n",
    "ALPHA_MAX  = 0.06            # 6% upper bound\n",
    "SMOOTH_KRN = [1,1,1]         # majority smoothing kernel (odd length)\n",
    "SEED       = 42\n",
    "rng        = np.random.default_rng(SEED)\n",
    "\n",
    "# ----------------------------- helpers ---------------------------------------------------------\n",
    "class _WinDataset(Dataset):\n",
    "    def __init__(self, win_mat: np.ndarray, noise_std=NOISE_STD):\n",
    "        x = win_mat.astype(np.float32) / 32768.0\n",
    "        self.clean = torch.from_numpy(x)[:, None]\n",
    "        self.noise_std = noise_std\n",
    "    def __len__(self): return len(self.clean)\n",
    "    def __getitem__(self, i):\n",
    "        clean = self.clean[i]\n",
    "        noisy = clean + self.noise_std * torch.randn_like(clean)\n",
    "        return noisy, clean\n",
    "\n",
    "def make_windows(sig: np.ndarray, win:int, hop:int) -> np.ndarray:\n",
    "    n = (len(sig)-win)//hop + 1\n",
    "    out = np.empty((n, win), dtype=np.int16)\n",
    "    for i in range(n):\n",
    "        j = i*hop\n",
    "        out[i,:] = sig[j:j+win]\n",
    "    return out\n",
    "\n",
    "def _smooth_bool(mask: np.ndarray, kernel: List[int]) -> np.ndarray:\n",
    "    krn = np.array(kernel, dtype=int)\n",
    "    half = int(np.ceil(krn.sum()/2))\n",
    "    return (convolve(mask.astype(int), krn, mode='same') >= half)\n",
    "\n",
    "def _conformal_pvalues(cal_scores: np.ndarray, test_scores: np.ndarray) -> np.ndarray:\n",
    "    # For errors, larger = more anomalous, so take A = score itself.\n",
    "    A_cal  = cal_scores.reshape(-1)\n",
    "    A_test = test_scores.reshape(-1)\n",
    "    A_cal_sorted = np.sort(A_cal)\n",
    "    # p(x) = (1 + # {A_cal >= A_test}) / (n_cal + 1)\n",
    "    idx = np.searchsorted(A_cal_sorted, A_test, side='left')  # count < A_test\n",
    "    geq = A_cal.size - idx\n",
    "    return (geq + 1.0) / (A_cal.size + 1.0)\n",
    "\n",
    "# ----------------------------- CDAE with exposed last layer -----------------------------------\n",
    "class _CDAE(nn.Module):\n",
    "    def __init__(self, latent=LATENT):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv1d(1,  8, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(8, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv1d(16,32,7, 2, 3), nn.ReLU(),\n",
    "            nn.Flatten(), nn.Linear(32*128, latent), nn.ReLU()\n",
    "        )\n",
    "        self.dec_fc = nn.Linear(latent, 32*128)\n",
    "        self.dec_up = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32,16,7,2,3,output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose1d(16,8,7,2,3,output_padding=1),  nn.ReLU()\n",
    "        )\n",
    "        self.dec_last = nn.ConvTranspose1d(8,1,7,2,3,output_padding=1)\n",
    "\n",
    "    def forward_maps(self, x):\n",
    "        z = self.enc(x)\n",
    "        h = self.dec_fc(z).view(-1,32,128)\n",
    "        f = self.dec_up(h)       # (B,8,512)\n",
    "        y = self.dec_last(f)     # (B,1,1024)\n",
    "        return y, f\n",
    "\n",
    "# ----------------------------- Laplace (last layer) utils -------------------------------------\n",
    "def _diag_gauss_newton_lastlayer(net:_CDAE, loader, device, max_batches=FISHER_BATCHES):\n",
    "    last = net.dec_last\n",
    "    params = [last.weight] + ([last.bias] if last.bias is not None else [])\n",
    "    shapes = [p.shape for p in params]\n",
    "    sizes  = [p.numel() for p in params]\n",
    "    fisher = torch.zeros(sum(sizes), device=device)\n",
    "    net.train()\n",
    "    seen = 0\n",
    "    for b, (noisy, clean) in enumerate(loader):\n",
    "        if b >= max_batches: break\n",
    "        noisy, clean = noisy.to(device), clean.to(device)\n",
    "        with torch.no_grad():\n",
    "            z = net.enc(noisy)\n",
    "            h = net.dec_fc(z).view(-1,32,128)\n",
    "            f = net.dec_up(h)\n",
    "        y = net.dec_last(f)\n",
    "        loss = nn.functional.mse_loss(y, clean)\n",
    "        for p in params:\n",
    "            if p.grad is not None: p.grad.zero_()\n",
    "        loss.backward()\n",
    "        grads = [p.grad.detach().flatten() for p in params]\n",
    "        gcat  = torch.cat(grads)\n",
    "        fisher += gcat * gcat\n",
    "        seen += 1\n",
    "    fisher /= max(seen, 1)\n",
    "    return fisher, params, shapes\n",
    "\n",
    "def _sample_lastlayer_from_laplace(params, shapes, fisher_diag, prior_prec:float, K:int):\n",
    "    with torch.no_grad():\n",
    "        w_star = torch.cat([p.detach().flatten() for p in params])\n",
    "        prec   = prior_prec + fisher_diag\n",
    "        var    = 1.0 / torch.clamp(prec, min=1e-8)\n",
    "        std    = torch.sqrt(var)\n",
    "        sizes  = [p.numel() for p in params]\n",
    "        samples = []\n",
    "        for _ in range(K):\n",
    "            w_samp = w_star + std * torch.randn_like(std)\n",
    "            tensors=[]; off=0\n",
    "            for shp, sz in zip(shapes, sizes):\n",
    "                tensors.append(w_samp[off:off+sz].view(shp).clone())\n",
    "                off += sz\n",
    "            samples.append(tensors)\n",
    "    return samples\n",
    "\n",
    "def _apply_lastlayer_sample(module_last:nn.ConvTranspose1d, tensors):\n",
    "    with torch.no_grad():\n",
    "        module_last.weight.copy_(tensors[0])\n",
    "        if module_last.bias is not None and len(tensors) > 1:\n",
    "            module_last.bias.copy_(tensors[1])\n",
    "\n",
    "# ----------------------------- Strictly-no-worse CDAE wrapper --------------------------------\n",
    "class Cdae_StrictBetter:\n",
    "    def __init__(self, *,\n",
    "                 win=WIN, hop=HOP, latent=LATENT,\n",
    "                 epochs=EPOCHS, batch=BATCH, train_win=TRAIN_WIN,\n",
    "                 pct_thr=PCT_THR,\n",
    "                 # Bayes last-layer\n",
    "                 use_bayes=True, prior_prec=PRIOR_PREC, fisher_batches=FISHER_BATCHES,\n",
    "                 samples_K=SAMPLES_K, lambda_uq=LAMBDA_UQ,\n",
    "                 # Conformal\n",
    "                 use_conformal=True, alpha_min=ALPHA_MIN, alpha_max=ALPHA_MAX,\n",
    "                 smooth_kernel:List[int]=SMOOTH_KRN,\n",
    "                 device=None):\n",
    "        self.win=win; self.hop=hop; self.latent=latent\n",
    "        self.epochs=epochs; self.batch=batch; self.train_win=train_win\n",
    "        self.pct_thr=pct_thr\n",
    "        self.use_bayes=use_bayes; self.prior_prec=prior_prec\n",
    "        self.fisher_batches=fisher_batches; self.samples_K=samples_K; self.lambda_uq=lambda_uq\n",
    "        self.use_conformal=use_conformal; self.alpha_min=alpha_min; self.alpha_max=alpha_max\n",
    "        self.smooth_kernel=smooth_kernel\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # per-station state\n",
    "        self.models: Dict[str,_CDAE] = {}\n",
    "        self.base_thr: Dict[str,float] = {}\n",
    "        self.bayes_thr: Dict[str,float] = {}\n",
    "        self.last_samples: Dict[str, list] = {}\n",
    "        self.alpha: Dict[str,float] = {}\n",
    "        self.cal_scores: Dict[str,np.ndarray] = {}  # for conformal (baseline errors)\n",
    "\n",
    "    def _train_one(self, win_mat:np.ndarray):\n",
    "        idx = np.random.choice(len(win_mat), min(self.train_win, len(win_mat)), replace=False)\n",
    "        dl  = DataLoader(_WinDataset(win_mat[idx]), batch_size=self.batch, shuffle=True, num_workers=0, pin_memory=False)\n",
    "        net = _CDAE(latent=self.latent).to(self.device)\n",
    "        opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "        net.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for noisy, clean in dl:\n",
    "                noisy, clean = noisy.to(self.device), clean.to(self.device)\n",
    "                opt.zero_grad()\n",
    "                pred, _ = net.forward_maps(noisy)\n",
    "                nn.functional.mse_loss(pred, clean).backward()\n",
    "                opt.step()\n",
    "        return net.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _score_base(self, net:_CDAE, win_mat:np.ndarray, chunk=2048):\n",
    "        errs = np.empty(len(win_mat), dtype=np.float32)\n",
    "        for i0 in range(0, len(win_mat), chunk):\n",
    "            seg_np = win_mat[i0:i0+chunk].astype(np.float32)/32768.0\n",
    "            seg = torch.from_numpy(seg_np)[:,None].to(self.device)\n",
    "            pred, _ = net.forward_maps(seg)\n",
    "            errs[i0:i0+len(seg_np)] = ((pred - seg)**2).mean(dim=(1,2)).cpu().numpy()\n",
    "        return errs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _score_bayes(self, net:_CDAE, samples:list, win_mat:np.ndarray, chunk=1024):\n",
    "        mse_mean = np.empty(len(win_mat), np.float32)\n",
    "        recon_var= np.empty(len(win_mat), np.float32)\n",
    "        w0 = [p.detach().clone() for p in [net.dec_last.weight, net.dec_last.bias] if p is not None]\n",
    "        for i0 in range(0, len(win_mat), chunk):\n",
    "            seg_np = win_mat[i0:i0+chunk].astype(np.float32)/32768.0\n",
    "            seg = torch.from_numpy(seg_np)[:,None].to(self.device)\n",
    "            z = net.enc(seg); h = net.dec_fc(z).view(-1,32,128); f = net.dec_up(h)\n",
    "            preds = []\n",
    "            for tensors in samples:\n",
    "                _apply_lastlayer_sample(net.dec_last, tensors)\n",
    "                y = net.dec_last(f)\n",
    "                preds.append(y.unsqueeze(0))\n",
    "            Y = torch.cat(preds, dim=0)                          # (K,N,1,W)\n",
    "            y_var  = Y.var(0, unbiased=True)\n",
    "            mse_ps = ((Y - seg.unsqueeze(0))**2).mean(dim=(2,3)) # (K,N)\n",
    "            j = len(seg_np)\n",
    "            mse_mean[i0:i0+j] = mse_ps.mean(0).cpu().numpy()\n",
    "            recon_var[i0:i0+j]= y_var.mean(dim=(1,2)).cpu().numpy()\n",
    "        _apply_lastlayer_sample(net.dec_last, [w0[0]] + ([w0[1]] if len(w0)>1 else []))\n",
    "        return mse_mean, recon_var\n",
    "\n",
    "    def fit(self, raw:Dict[str,np.ndarray], verbose=True):\n",
    "        self.models.clear(); self.base_thr.clear(); self.bayes_thr.clear()\n",
    "        self.last_samples.clear(); self.alpha.clear(); self.cal_scores.clear()\n",
    "        for nm, sig in storm_data.quantised.items():\n",
    "            win_mat = make_windows(sig, self.win, self.hop)\n",
    "            if verbose:\n",
    "                print(f\"\\n▶ Training CDAE for {nm}  ({len(win_mat)} windows, device={self.device})\")\n",
    "            net = self._train_one(win_mat); self.models[nm]=net\n",
    "\n",
    "            # Baseline: errors and threshold\n",
    "            base_err = self._score_base(net, win_mat)\n",
    "            base_thr = float(np.percentile(base_err, self.pct_thr))\n",
    "            self.base_thr[nm] = base_thr\n",
    "\n",
    "            # Compute baseline flagged fraction AFTER smoothing (operating point)\n",
    "            base_raw = (base_err > base_thr)\n",
    "            base_sm  = _smooth_bool(base_raw, SMOOTH_KRN)\n",
    "            flagged_frac = float(base_sm.mean())\n",
    "\n",
    "            # Conformal calibration split: held-out slice of windows\n",
    "            n = len(base_err)\n",
    "            idx = np.arange(n)\n",
    "            rng.shuffle(idx)\n",
    "            m_cal = min(max(5000, n//5), n//2)   # decent calibration size\n",
    "            cal_idx = idx[:m_cal]\n",
    "            self.cal_scores[nm] = base_err[cal_idx].copy()\n",
    "            # per-station alpha tied to baseline flagged fraction (clipped)\n",
    "            self.alpha[nm] = float(np.clip(flagged_frac, ALPHA_MIN, ALPHA_MAX))\n",
    "\n",
    "            # Bayes last-layer (Laplace) — optional\n",
    "            if self.use_bayes:\n",
    "                idx_tr = rng.choice(n, min(TRAIN_WIN, n), replace=False)\n",
    "                dl  = DataLoader(_WinDataset(win_mat[idx_tr]), batch_size=self.batch, shuffle=True, num_workers=0, pin_memory=False)\n",
    "                fisher_diag, params, shapes = _diag_gauss_newton_lastlayer(net, dl, self.device, max_batches=self.fisher_batches)\n",
    "                assert torch.isfinite(fisher_diag).all(), \"Fisher has non-finite entries.\"\n",
    "                samples = _sample_lastlayer_from_laplace(params, shapes, fisher_diag, prior_prec=self.prior_prec, K=self.samples_K)\n",
    "                self.last_samples[nm] = samples\n",
    "\n",
    "                # training-time Bayes threshold\n",
    "                mse_mean, recon_var = self._score_bayes(net, samples, win_mat)\n",
    "                bayes_score = mse_mean + self.lambda_uq * recon_var\n",
    "                self.bayes_thr[nm] = float(np.percentile(bayes_score, self.pct_thr))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  base>thr fraction (smoothed) ≈ {flagged_frac:.3%} | \"\n",
    "                      f\"α_stn (conformal) = {self.alpha[nm]:.3%} | \"\n",
    "                      f\"{'Bayes ON' if self.use_bayes else 'Bayes OFF'}\")\n",
    "\n",
    "    def predict(self, raw:Dict[str,np.ndarray], return_both=True):\n",
    "        base_masks: Dict[str,np.ndarray] = {}\n",
    "        imp_masks : Dict[str,np.ndarray] = {}\n",
    "        for nm, sig in storm_data.quantised.items():\n",
    "            win_mat = make_windows(sig, self.win, self.hop)\n",
    "            net = self.models[nm]\n",
    "\n",
    "            # Baseline raw & smoothed\n",
    "            base_err = self._score_base(net, win_mat)\n",
    "            base_raw = (base_err > self.base_thr[nm])\n",
    "            base_sm  = _smooth_bool(base_raw, SMOOTH_KRN)\n",
    "\n",
    "            add_mask = np.zeros_like(base_raw, dtype=bool)\n",
    "\n",
    "            # Bayes addition\n",
    "            if self.use_bayes and nm in self.last_samples and self.last_samples[nm]:\n",
    "                mse_mean, recon_var = self._score_bayes(net, self.last_samples[nm], win_mat)\n",
    "                bayes_score = mse_mean + self.lambda_uq * recon_var\n",
    "                bayes_raw   = (bayes_score > self.bayes_thr[nm]) & (~base_raw)\n",
    "                add_mask |= _smooth_bool(bayes_raw, SMOOTH_KRN)\n",
    "\n",
    "            # Conformal addition\n",
    "            if self.use_conformal:\n",
    "                pvals   = _conformal_pvalues(self.cal_scores[nm], base_err)\n",
    "                conf_raw= (pvals < self.alpha[nm]) & (~base_raw)\n",
    "                add_mask |= _smooth_bool(conf_raw, SMOOTH_KRN)\n",
    "\n",
    "            improved = base_sm | add_mask\n",
    "\n",
    "            # superset check\n",
    "            assert np.all(improved | ~base_sm), f\"Superset violated on {nm}\"\n",
    "\n",
    "            base_masks[nm] = base_sm.astype(bool)\n",
    "            imp_masks[nm]  = improved.astype(bool)\n",
    "\n",
    "        return (base_masks, imp_masks) if return_both else imp_masks\n",
    "\n",
    "# ================================== Train / Predict / PROVE ==================================\n",
    "model_cdae_sb = Cdae_StrictBetter(\n",
    "    win=WIN, hop=HOP, latent=LATENT,\n",
    "    epochs=EPOCHS, batch=BATCH, train_win=TRAIN_WIN, pct_thr=PCT_THR,\n",
    "    use_bayes=True,  prior_prec=PRIOR_PREC, fisher_batches=FISHER_BATCHES,\n",
    "    samples_K=SAMPLES_K, lambda_uq=LAMBDA_UQ,\n",
    "    use_conformal=True, alpha_min=ALPHA_MIN, alpha_max=ALPHA_MAX,\n",
    "    smooth_kernel=SMOOTH_KRN,\n",
    "    device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "print(\"▶ Fitting CDAE (strictly-no-worse) …\")\n",
    "model_cdae_sb.fit(storm_data.quantised)\n",
    "\n",
    "print(\"\\n▶ Predicting baseline vs improved …\")\n",
    "hot_base, hot_impr = model_cdae_sb.predict(storm_data.quantised, return_both=True)\n",
    "\n",
    "# Evaluate both with YOUR existing harness\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS,\n",
    "                      burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "\n",
    "print(\"\\n▶ Evaluating BASELINE …\")\n",
    "st_base, net_base, n_win = evaluate_windowed_model(\n",
    "    hot            = hot_base,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = False\n",
    ")\n",
    "\n",
    "print(\"\\n▶ Evaluating IMPROVED (strict superset) …\")\n",
    "st_impr, net_impr, _ = evaluate_windowed_model(\n",
    "    hot            = hot_impr,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "def _fmt(m): return f\"TP={m['TP']}, FP={m['FP']}, FN={m['FN']}, TN={m['TN']}, \" \\\n",
    "                    f\"P={m['P']:.3f}, R={m['R']:.3f}, F1={m['F1']:.3f}\"\n",
    "\n",
    "print(\"\\n—— Network metrics ——\")\n",
    "print(\"Baseline :\", _fmt(net_base))\n",
    "print(\"Improved :\", _fmt(net_impr))\n",
    "print(\"Δ (Imp-Base):\",\n",
    "      f\"ΔTP={net_impr['TP']-net_base['TP']}, \"\n",
    "      f\"ΔFP={net_impr['FP']-net_base['FP']}, \"\n",
    "      f\"ΔFN={net_impr['FN']-net_base['FN']}, \"\n",
    "      f\"ΔTN={net_impr['TN']-net_base['TN']}\")\n",
    "\n",
    "print(f\"\\n—— Station metrics  (n_windows = {n_win:,}) ——\")\n",
    "for nm in STN:\n",
    "    mb, mi = st_base[nm], st_impr[nm]\n",
    "    print(f\"{nm}: Base F1={mb['F1']:.3f}  →  Imp F1={mi['F1']:.3f}  \"\n",
    "          f\"(ΔTP={mi['TP']-mb['TP']}, ΔFP={mi['FP']-mb['FP']}, ΔFN={mi['FN']-mb['FN']})\")\n",
    "\n",
    "print(\"\\nSuperset check passed: improved masks are supersets of baseline (after smoothing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### more attempts a prob ml('mps' if torch.backends.mps.is_available()\n",
    " #           else 'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== Probabilistic evaluator (station + network) ==========================\n",
    "# Provides:\n",
    "#   - ece_binwise(), split_conformal_pvalues(), build_truth_windows()\n",
    "#   - empirical_network_pvalues(): network p-value per window (no independence assumption)\n",
    "#   - evaluate_windowed_model_prob(): main entrypoint producing station+network probabilistic metrics\n",
    "#\n",
    "# Usage:\n",
    "#   res = evaluate_windowed_model_prob(..., station_pvals=..., cal_idx_by_stn=..., baseline_hot=...)\n",
    "#   # or res = evaluate_windowed_model_prob(..., station_scores=..., cal_idx_by_stn=...)\n",
    "#\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "def ece_binwise(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15):\n",
    "    \"\"\"Expected Calibration Error for binary probs (prob=P(positive)).\"\"\"\n",
    "    probs = np.clip(probs, 1e-8, 1-1e-8).astype(float)\n",
    "    labels = labels.astype(int)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    idx = np.digitize(probs, bins) - 1\n",
    "    ece = 0.0\n",
    "    per_bin = []\n",
    "    n = len(probs)\n",
    "    for b in range(n_bins):\n",
    "        mask = (idx == b)\n",
    "        cnt = int(mask.sum())\n",
    "        if cnt == 0:\n",
    "            per_bin.append(dict(bin=b, frac=0, conf=0.0, acc=0.0))\n",
    "            continue\n",
    "        conf = float(probs[mask].mean())\n",
    "        acc  = float(labels[mask].mean())\n",
    "        ece += (cnt / n) * abs(acc - conf)\n",
    "        per_bin.append(dict(bin=b, frac=cnt/n, conf=conf, acc=acc))\n",
    "    return float(ece), {\"bins\": bins.tolist(), \"per_bin\": per_bin}\n",
    "\n",
    "def split_conformal_pvalues(err_cal: np.ndarray, err_all: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Split-conformal p-values for 'larger score/error = more anomalous'.\n",
    "    p(x) = (1 + # {A_cal >= A_x}) / (n_cal + 1).\n",
    "    Valid under exchangeability; distribution-free.\n",
    "    \"\"\"\n",
    "    A_cal  = err_cal.reshape(-1)\n",
    "    A_all  = err_all.reshape(-1)\n",
    "    A_cal_sorted = np.sort(A_cal)\n",
    "    idx = np.searchsorted(A_cal_sorted, A_all, side='left')  # how many < A_x\n",
    "    geq = A_cal.size - idx\n",
    "    return (geq + 1.0) / (A_cal.size + 1.0)\n",
    "\n",
    "def build_truth_windows(\n",
    "    stroke_records: List[dict],\n",
    "    station_order: List[str],\n",
    "    n_win: int,\n",
    "    burst_len: int,\n",
    "    hop: int,\n",
    "    win: int\n",
    ") -> Tuple[Dict[str, np.ndarray], dict]:\n",
    "    \"\"\"Per-station window truth and mapping stroke→{station→set(win_ids)}.\"\"\"\n",
    "    station_truth = {s: np.zeros(n_win, bool) for s in station_order}\n",
    "    stroke_to_winset = defaultdict(lambda: defaultdict(set))\n",
    "    for rec in stroke_records:\n",
    "        stn = rec[\"station\"]\n",
    "        if stn not in station_order: continue\n",
    "        s0   = rec[\"sample_idx\"]; s1 = s0 + burst_len - 1\n",
    "        w_first = int(np.ceil((s0 + 1 - win) / hop)); w_first = max(0, w_first)\n",
    "        w_last  = int(np.floor(s1 / hop));            w_last  = min(n_win - 1, w_last)\n",
    "        station_truth[stn][w_first:w_last+1] = True\n",
    "        key = (rec[\"event_id\"], rec.get(\"stroke_i\", 0))\n",
    "        stroke_to_winset[key][stn].update(range(w_first, w_last+1))\n",
    "    return station_truth, stroke_to_winset\n",
    "\n",
    "def empirical_network_pvalues(\n",
    "    pvals_by_stn: Dict[str, np.ndarray],\n",
    "    cal_idx_by_stn: Dict[str, np.ndarray],\n",
    "    alpha_ref: float,\n",
    "    min_stn: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Empirical network p-values per window:\n",
    "      1) On the intersection of calibration windows across stations, build distribution of\n",
    "         count = #stations with p<alpha_ref (null).\n",
    "      2) For each window, return tail prob P(null_count >= observed_count).\n",
    "    \"\"\"\n",
    "    n_win = min(len(v) for v in pvals_by_stn.values())\n",
    "    cal_sets = {nm: set(idx.tolist()) for nm, idx in cal_idx_by_stn.items()}\n",
    "    inter = set.intersection(*cal_sets.values()) if len(cal_sets) else set()\n",
    "    inter = np.array(sorted(list(inter)), dtype=int)\n",
    "    if inter.size == 0:  # fallback\n",
    "        return np.ones(n_win, dtype=np.float32)\n",
    "\n",
    "    cal_counts = []\n",
    "    for w in inter:\n",
    "        cnt = sum(pvals_by_stn[nm][w] < alpha_ref for nm in pvals_by_stn)\n",
    "        cal_counts.append(cnt)\n",
    "    cal_counts = np.array(cal_counts, dtype=int)\n",
    "    if cal_counts.size == 0:\n",
    "        return np.ones(n_win, dtype=np.float32)\n",
    "\n",
    "    def tail_prob(k):  # empirical tail\n",
    "        return (1.0 + np.sum(cal_counts >= k)) / (cal_counts.size + 1.0)\n",
    "\n",
    "    p_net = np.empty(n_win, dtype=np.float32)\n",
    "    for w in range(n_win):\n",
    "        cnt = sum(pvals_by_stn[nm][w] < alpha_ref for nm in pvals_by_stn)\n",
    "        p_net[w] = tail_prob(cnt) if cnt >= min_stn else 1.0\n",
    "    return p_net\n",
    "\n",
    "def evaluate_windowed_model_prob(\n",
    "    *,\n",
    "    win: int, hop: int, fs: int, burst_len: int, min_stn: int,\n",
    "    stroke_records: List[dict],\n",
    "    quantized: Dict[str, np.ndarray],\n",
    "    station_order: List[str],\n",
    "    station_pvals: Optional[Dict[str, np.ndarray]] = None,   # lower = more anomalous\n",
    "    station_scores: Optional[Dict[str, np.ndarray]] = None,  # higher = more anomalous (will be conformalized)\n",
    "    cal_idx_by_stn: Optional[Dict[str, np.ndarray]] = None,  # held-out calibration windows per station\n",
    "    baseline_hot: Optional[Dict[str, np.ndarray]] = None,    # optional: for hybrid superset proof\n",
    "    alpha_per_station: Optional[Dict[str, float]] = None,\n",
    "    alpha_clip: Tuple[float, float] = (0.005, 0.06),\n",
    "    n_bins_ece: int = 15,\n",
    "    plot: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Probabilistic station+network evaluation.\n",
    "    Returns dict with:\n",
    "      - station_window_prob_metrics: {station -> {AUROC, AUPRC, Brier, ECE}}\n",
    "      - station_pvals, alpha_per_station\n",
    "      - p_net (per-window network p-values), network_window_prob_metrics, network_stroke_prob_metrics\n",
    "      - hot_hybrid (if baseline_hot provided): per-station superset masks\n",
    "    \"\"\"\n",
    "    # geometry\n",
    "    n_win = min((len(quantized[s]) - win)//hop + 1 for s in station_order)\n",
    "    if n_win <= 0:\n",
    "        raise RuntimeError(\"No complete windows to score\")\n",
    "    # truth\n",
    "    station_truth, stroke_to_winset = build_truth_windows(\n",
    "        stroke_records, station_order, n_win, burst_len, hop, win\n",
    "    )\n",
    "    # p-values per station\n",
    "    if station_pvals is None and station_scores is None:\n",
    "        raise ValueError(\"Provide station_pvals or (station_scores + cal_idx_by_stn).\")\n",
    "    if station_pvals is None and cal_idx_by_stn is None:\n",
    "        raise ValueError(\"cal_idx_by_stn required when conformalizing station_scores.\")\n",
    "\n",
    "    pvals_by_stn: Dict[str, np.ndarray] = {}\n",
    "    if station_pvals is not None:\n",
    "        for nm in station_order:\n",
    "            pvals_by_stn[nm] = station_pvals[nm][:n_win].astype(np.float32)\n",
    "    else:\n",
    "        for nm in station_order:\n",
    "            s = station_scores[nm][:n_win]\n",
    "            idx_cal = cal_idx_by_stn[nm]\n",
    "            pvals_by_stn[nm] = split_conformal_pvalues(s[idx_cal], s).astype(np.float32)\n",
    "\n",
    "    # per-station probabilistic metrics (window level)\n",
    "    station_prob_metrics = {}\n",
    "    for nm in station_order:\n",
    "        pv  = pvals_by_stn[nm]\n",
    "        q   = 1.0 - pv\n",
    "        y   = station_truth[nm].astype(int)\n",
    "        auroc = float('nan'); auprc=float('nan')\n",
    "        try: auroc = float(roc_auc_score(y, q))\n",
    "        except Exception: pass\n",
    "        try: auprc = float(average_precision_score(y, q))\n",
    "        except Exception: pass\n",
    "        brier = float(brier_score_loss(y, np.clip(q, 1e-6, 1-1e-6)))\n",
    "        ece, _ = ece_binwise(q, y, n_bins=n_bins_ece)\n",
    "        station_prob_metrics[nm] = dict(AUROC=auroc, AUPRC=auprc, Brier=brier, ECE=ece)\n",
    "\n",
    "    # network p-values (empirical null)\n",
    "    if cal_idx_by_stn is None:\n",
    "        cal_idx_by_stn = {nm: np.arange(0, max(1, int(0.25*n_win))) for nm in station_order}\n",
    "    # station α\n",
    "    if alpha_per_station is None:\n",
    "        if baseline_hot is not None:\n",
    "            alpha_per_station = {nm: float(np.clip(baseline_hot[nm][:n_win].mean(), alpha_clip[0], alpha_clip[1]))\n",
    "                                 for nm in station_order}\n",
    "        else:\n",
    "            alpha_per_station = {nm: 0.02 for nm in station_order}\n",
    "    alpha_ref = float(np.median(list(alpha_per_station.values())))\n",
    "    p_net = empirical_network_pvalues(pvals_by_stn, cal_idx_by_stn, alpha_ref, min_stn)\n",
    "    q_net = 1.0 - p_net\n",
    "\n",
    "    # network window labels: ≥min_stn stations truly positive\n",
    "    net_truth_win = np.zeros(n_win, bool)\n",
    "    for w in range(n_win):\n",
    "        cnt = sum(station_truth[nm][w] for nm in station_order)\n",
    "        net_truth_win[w] = (cnt >= min_stn)\n",
    "    y_net = net_truth_win.astype(int)\n",
    "\n",
    "    # network probabilistic window metrics\n",
    "    try:    auroc_net = float(roc_auc_score(y_net, q_net))\n",
    "    except: auroc_net = float('nan')\n",
    "    try:    auprc_net = float(average_precision_score(y_net, q_net))\n",
    "    except: auprc_net = float('nan')\n",
    "    brier_net = float(brier_score_loss(y_net, np.clip(q_net, 1e-6, 1-1e-6)))\n",
    "    ece_net, _ = ece_binwise(q_net, y_net, n_bins=n_bins_ece)\n",
    "    network_window_prob_metrics = dict(AUROC=auroc_net, AUPRC=auprc_net, Brier=brier_net, ECE=ece_net)\n",
    "\n",
    "    # stroke-level probabilistic metrics: score = 1−min p_net over stroke span\n",
    "    stroke_scores = []; stroke_labels=[]\n",
    "    for key, per_stn in stroke_to_winset.items():\n",
    "        winset=set()\n",
    "        for ws in per_stn.values(): winset |= ws\n",
    "        if not winset: continue\n",
    "        stroke_scores.append(1.0 - float(np.min(p_net[list(winset)])))\n",
    "        stroke_labels.append(1)\n",
    "    # sample negatives from background windows (simple proxy)\n",
    "    truth_union = np.zeros(n_win, bool)\n",
    "    for nm in station_order: truth_union |= station_truth[nm]\n",
    "    neg_idx = np.where(~truth_union)[0]\n",
    "    if neg_idx.size > 0 and len(stroke_scores) > 0:\n",
    "        rng = np.random.default_rng(123)\n",
    "        samp = neg_idx[rng.choice(len(neg_idx), size=min(len(neg_idx), max(10, len(stroke_scores))), replace=False)]\n",
    "        for w in samp:\n",
    "            w0=max(0,w-1); w1=min(n_win-1,w+1)\n",
    "            stroke_scores.append(1.0 - float(np.min(p_net[w0:w1+1])))\n",
    "            stroke_labels.append(0)\n",
    "    if stroke_labels and 0 < sum(stroke_labels) < len(stroke_labels):\n",
    "        auroc_s = float(roc_auc_score(stroke_labels, stroke_scores))\n",
    "        auprc_s = float(average_precision_score(stroke_labels, stroke_scores))\n",
    "    else:\n",
    "        auroc_s=float('nan'); auprc_s=float('nan')\n",
    "    network_stroke_prob_metrics = dict(AUROC=auroc_s, AUPRC=auprc_s)\n",
    "\n",
    "    # optional: hybrid superset masks (for deterministic proof)\n",
    "    hot_hybrid = None\n",
    "    if baseline_hot is not None:\n",
    "        hot_hybrid={}\n",
    "        for nm in station_order:\n",
    "            pv = pvals_by_stn[nm][:n_win]\n",
    "            alpha = alpha_per_station[nm]\n",
    "            add = (pv < alpha) & (~baseline_hot[nm][:n_win])\n",
    "            hot_hybrid[nm] = (baseline_hot[nm][:n_win] | add)\n",
    "\n",
    "    return dict(\n",
    "        n_win=n_win,\n",
    "        station_window_prob_metrics=station_prob_metrics,\n",
    "        station_pvals=pvals_by_stn,\n",
    "        alpha_per_station=alpha_per_station,\n",
    "        p_net=p_net,\n",
    "        network_window_prob_metrics=network_window_prob_metrics,\n",
    "        network_stroke_prob_metrics=network_stroke_prob_metrics,\n",
    "        hot_hybrid=hot_hybrid\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# OU–Kalman GP + Conformal Anomaly Detection (O(n))\n",
    "\n",
    "This detector treats each station’s windowed signal as a **latent Gaussian process**\n",
    "with an Ornstein–Uhlenbeck (Matérn-½) prior, run in **state–space form** for speed.\n",
    "It then uses **conformal p-values** to calibrate anomaly scores without any\n",
    "distributional assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Feature per window\n",
    "For each station and window:\n",
    "\n",
    "\\[\n",
    "y_k = \\sqrt{\\frac{1}{W}\\sum_{t=1}^W x_t^2}\n",
    "\\]\n",
    "\n",
    "where \\(W\\) is the window size.\n",
    "We use the **RMS** of the waveform (scaled to \\([-1,1]\\)) as a compact statistic.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. OU GP state–space model\n",
    "The OU kernel is\n",
    "\n",
    "\\[\n",
    "k(\\tau) = \\sigma_f^2 \\, e^{-|\\tau|/\\ell}.\n",
    "\\]\n",
    "\n",
    "It has an exact 1-dimensional state–space form:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "x_k &= a \\, x_{k-1} + \\varepsilon_k,\n",
    "&\\varepsilon_k &\\sim \\mathcal{N}(0,q), \\\\\n",
    "y_k &= x_k + \\eta_k,\n",
    "&\\eta_k &\\sim \\mathcal{N}(0,r),\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "where \\(a = e^{-\\Delta/\\ell}\\).\n",
    "Parameters \\((a,q,r)\\) are estimated **robustly** from the series:\n",
    "\n",
    "- \\(a \\approx\\) robust lag-1 autocorrelation of \\(y\\),\n",
    "- \\(q, r\\) from robust variance and differenced variance of \\(y\\).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Kalman filtering\n",
    "For each time step:\n",
    "\n",
    "**Predict**\n",
    "\\[\n",
    "x_{k|k-1} = a \\, x_{k-1|k-1},\n",
    "\\quad P_{k|k-1} = a^2 P_{k-1|k-1} + q.\n",
    "\\]\n",
    "\n",
    "**Innovation (surprise)**\n",
    "\\[\n",
    "\\nu_k = y_k - x_{k|k-1},\n",
    "\\quad S_k = P_{k|k-1} + r.\n",
    "\\]\n",
    "\n",
    "**Update**\n",
    "\\[\n",
    "K_k = \\frac{P_{k|k-1}}{S_k},\n",
    "\\quad x_{k|k} = x_{k|k-1} + K_k \\nu_k,\n",
    "\\quad P_{k|k} = (1-K_k)P_{k|k-1}.\n",
    "\\]\n",
    "\n",
    "The **innovation statistic**\n",
    "\n",
    "\\[\n",
    "s_k = \\frac{\\nu_k^2}{S_k}\n",
    "\\]\n",
    "\n",
    "is the **Normalized Innovation Squared (NIS)**: a dimensionless measure of anomaly.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Split–conformal calibration\n",
    "To make anomaly scores **distribution-free**:\n",
    "\n",
    "- Take the first \\(m\\) windows (e.g. 20% or ≥5000) as a **calibration set** \\(\\mathcal{C}\\).\n",
    "- For any test score \\(s_k\\), define the conformal p-value:\n",
    "\n",
    "\\[\n",
    "p_k = \\frac{1 + \\#\\{ i \\in \\mathcal{C} : s_i \\geq s_k \\}}{|\\mathcal{C}| + 1}.\n",
    "\\]\n",
    "\n",
    "This guarantees \\(p_k \\sim \\text{Uniform}(0,1]\\) under the null (exchangeability).\n",
    "Small \\(p_k\\) means “statistically surprising”.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Station decision rule\n",
    "- Pick a per-station threshold \\(\\alpha \\in [0.5\\%, 2\\%]\\) (capped).\n",
    "- Flag window \\(k\\) as **hot** if \\(p_k < \\alpha\\).\n",
    "- Apply a short smoothing kernel \\([1,1,1]\\) (majority rule) to suppress flickers.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Network aggregation\n",
    "Network mask = **hot if ≥ `min_stn` stations are hot** at the same window.\n",
    "Evaluation uses the provided `evaluate_windowed_model`.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this works\n",
    "- **OU GP Kalman filter** runs in **O(n)** per station (linear time, constant memory).\n",
    "- **Innovations** measure surprise relative to predicted uncertainty.\n",
    "- **Conformal p-values** give **calibrated evidence** with finite-sample guarantees, no Gaussian assumption.\n",
    "- **α-caps + smoothing** bound station false positives, while the **network quorum** enforces concurrence across stations.\n",
    "\n",
    "---\n",
    "\n",
    "## Extensions (still O(n))\n",
    "- Use **log-RMS** or band-energy features for different sensitivity.\n",
    "- Replace OU (Matérn-½) with **Matérn-3/2** (2-dimensional state).\n",
    "- Add **rolling conformal** to adapt to long-term drift.\n",
    "- Weighted network fusion (e.g. by station strength) instead of hard ≥2 rule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is good====================== Scalable GP (Kalman OU) Anomaly Detector + Conformal p-values ======================\n",
    "# Unsupervised, O(N) per station, works with your existing evaluator.\n",
    "# Assumes in scope: storm_data, EvalConfig, evaluate_windowed_model, pretty_metrics\n",
    "\n",
    "import numpy as np, math\n",
    "from scipy.signal import convolve\n",
    "from typing import Dict\n",
    "\n",
    "# ---------------- Device ----------------\n",
    "import torch\n",
    "DEVICE = ('mps' if torch.backends.mps.is_available()\n",
    "          else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ---------------- Geometry / config ----------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "\n",
    "# Conformal + smoothing\n",
    "CAL_FRAC   = 0.20            # first 20% windows for calibration (no labels)\n",
    "CAL_MIN    = 5000            # at least this many cal windows if available\n",
    "ALPHA_MIN, ALPHA_MAX = 0.005, 0.02   # cap per-station alpha in [0.5%, 2%]\n",
    "SMOOTH_KRN = [1,1,1]         # majority smoothing for station masks\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def make_windows(sig: np.ndarray, win:int, hop:int) -> np.ndarray:\n",
    "    n = (len(sig)-win)//hop + 1\n",
    "    out = np.empty((n, win), dtype=np.int16)\n",
    "    for i in range(n):\n",
    "        j = i*hop\n",
    "        out[i,:] = sig[j:j+win]\n",
    "    return out\n",
    "\n",
    "def window_rms(win_mat: np.ndarray) -> np.ndarray:\n",
    "    # RMS in (-1,1) scale from int16\n",
    "    x = win_mat.astype(np.float32) / 32768.0\n",
    "    return np.sqrt((x*x).mean(axis=1)).astype(np.float32)\n",
    "\n",
    "def robust_autocorr_lag1(y: np.ndarray) -> float:\n",
    "    # Detrend by median; robust lag-1 corr using MAD-scaled covariance\n",
    "    x = y - np.median(y)\n",
    "    if len(x) < 3:\n",
    "        return 0.0\n",
    "    x0 = x[:-1]; x1 = x[1:]\n",
    "    num = np.median((x0 - np.median(x0)) * (x1 - np.median(x1)))\n",
    "    den = math.sqrt(np.median((x0 - np.median(x0))**2) * np.median((x1 - np.median(x1))**2)) + 1e-8\n",
    "    rho = float(num / den) if den > 0 else 0.0\n",
    "    return float(np.clip(rho, -0.99, 0.99))\n",
    "\n",
    "def estimate_ou_params(y: np.ndarray, delta: float=1.0) -> Dict[str,float]:\n",
    "    \"\"\"\n",
    "    OU (Matérn-1/2) GP: k(τ) = σ_f^2 * exp(-|τ|/ℓ).\n",
    "    Discrete-time state: x_k = a x_{k-1} + ε_k,  a=exp(-Δ/ℓ),  ε~N(0,q),\n",
    "                         y_k = x_k + η_k,        η~N(0,r).\n",
    "    We estimate a, q, r robustly from the series.\n",
    "    \"\"\"\n",
    "    # Robust stats\n",
    "    y_med = np.median(y)\n",
    "    y_center = y - y_med\n",
    "    var_y = float(np.median((y_center)**2)) * 1.4826**2 + 1e-8  # robust variance\n",
    "    rho1  = robust_autocorr_lag1(y)\n",
    "    a = float(np.clip(rho1, 0.01, 0.9999))   # persistence\n",
    "    # Relate var(y) ≈ r + q/(1-a^2); and var(diff) ≈ 2*var(y)*(1 - rho1)\n",
    "    vardiff = float(np.median((np.diff(y_center))**2)) * 1.4826**2 + 1e-8\n",
    "    # back-out q and r\n",
    "    q_stat = (var_y * (1 - a*a))\n",
    "    r_est  = max(1e-8, var_y - q_stat/(max(1e-8, 1 - a*a))) if (1 - a*a) > 1e-6 else var_y*0.5\n",
    "    # refine to keep both positive and stable\n",
    "    q_est  = max(1e-8, var_y * (1 - a)**2 + 0.25*vardiff*(1 - a))  # heuristic blend\n",
    "    return dict(a=a, q=q_est, r=r_est, y_med=y_med)\n",
    "\n",
    "def kalman_ou_innovations(y: np.ndarray, params: Dict[str,float]):\n",
    "    \"\"\"\n",
    "    Run OU Kalman filter to compute per-step innovation v_k and variance S_k.\n",
    "    Returns v (float32), S (float32).\n",
    "    \"\"\"\n",
    "    a = params['a']; q = params['q']; r = params['r']; mu = params['y_med']\n",
    "    n = len(y)\n",
    "    v = np.zeros(n, dtype=np.float32)\n",
    "    S = np.zeros(n, dtype=np.float32)\n",
    "    # init at steady-state roughly\n",
    "    P = q / max(1e-8, (1 - a*a))\n",
    "    x = mu\n",
    "    for k in range(n):\n",
    "        # predict\n",
    "        x_pred = a*(x - mu) + mu\n",
    "        P_pred = a*a*P + q\n",
    "        # innovation\n",
    "        vk = y[k] - x_pred\n",
    "        Sk = P_pred + r\n",
    "        v[k] = vk\n",
    "        S[k] = Sk\n",
    "        # update\n",
    "        K = P_pred / Sk\n",
    "        x = x_pred + K*vk\n",
    "        P = (1 - K)*P_pred\n",
    "        # numerical guard\n",
    "        if P < 1e-12: P = 1e-12\n",
    "    return v, S\n",
    "\n",
    "def conformal_pvalues_from_innov(v: np.ndarray, S: np.ndarray, cal_idx: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Nonparametric calibration: build a score s_k = v_k^2 / S_k (NIS-like),\n",
    "    then split-conformal p-values p_k = (1 + # {s_cal >= s_k}) / (n_cal + 1).\n",
    "    \"\"\"\n",
    "    s = (v*v) / np.maximum(S, 1e-12)\n",
    "    s_cal = s[cal_idx]\n",
    "    s_cal_sorted = np.sort(s_cal)\n",
    "    # vectorized rank\n",
    "    idx = np.searchsorted(s_cal_sorted, s, side='left')\n",
    "    geq = s_cal.size - idx\n",
    "    p = (geq + 1.0) / (s_cal.size + 1.0)\n",
    "    return p.astype(np.float32), s.astype(np.float32)\n",
    "\n",
    "def smooth_bool(mask: np.ndarray, kernel=SMOOTH_KRN) -> np.ndarray:\n",
    "    krn = np.array(kernel, dtype=int)\n",
    "    half = int(np.ceil(krn.sum()/2))\n",
    "    return (convolve(mask.astype(int), krn, mode='same') >= half)\n",
    "\n",
    "# ---------------- Per-station OU-GP + conformal ----------------\n",
    "pvals_by_stn: Dict[str,np.ndarray] = {}\n",
    "hot_by_stn  : Dict[str,np.ndarray] = {}\n",
    "score_by_stn: Dict[str,np.ndarray] = {}\n",
    "\n",
    "n_win = min((len(storm_data.quantised[s]) - WIN)//HOP + 1 for s in STN)\n",
    "for nm in STN:\n",
    "    win_mat = make_windows(storm_data.quantised[nm], WIN, HOP)\n",
    "    y = window_rms(win_mat)[:n_win]  # series we model with the GP\n",
    "    # fit OU params unsupervised (robust)\n",
    "    params = estimate_ou_params(y)\n",
    "    # Kalman innovations\n",
    "    v, S = kalman_ou_innovations(y, params)\n",
    "    # conformal split (first CAL_FRAC for calibration)\n",
    "    m_cal = min(max(CAL_MIN, int(CAL_FRAC * len(y))), len(y)-1)\n",
    "    cal_idx = np.arange(m_cal, dtype=int)\n",
    "    p, s = conformal_pvalues_from_innov(v, S, cal_idx)\n",
    "    pvals_by_stn[nm]  = p\n",
    "    score_by_stn[nm]  = s\n",
    "    # choose alpha tied to station's natural hit-rate on calibration (robust), but capped\n",
    "    # target rate ≈ fraction of extreme NIS on cal: here pick 1x 99.5% tail ~= 0.5%\n",
    "    alpha = np.clip( np.mean(p[cal_idx] < 0.01), ALPHA_MIN, ALPHA_MAX )\n",
    "    # fall back to ALPHA_MIN if zero\n",
    "    if alpha <= 0.0: alpha = ALPHA_MIN\n",
    "    stn_hot = p < alpha\n",
    "    hot_by_stn[nm] = smooth_bool(stn_hot)\n",
    "\n",
    "# ---------------- Network aggregation + evaluation ----------------\n",
    "agg_mask = (np.sum(np.stack([hot_by_stn[s] for s in STN], axis=0), axis=0) >= MIN_STN)\n",
    "\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS, burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "st_m, net_m, n_win_eval = evaluate_windowed_model(\n",
    "    hot            = hot_by_stn,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_eval:,}) ——\")\n",
    "for nm, m in st_m.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(\"\\nNetwork-level metrics:\", pretty_metrics(net_m, ndigits=3))\n",
    "\n",
    "# quick scalar summaries for sanity\n",
    "print(\"\\nStation α caps used (approximate):\")\n",
    "for nm in STN:\n",
    "    cal = int(min(max(CAL_MIN, int(CAL_FRAC * n_win)), n_win-1))\n",
    "    alpha_eff = float(np.clip(np.mean(pvals_by_stn[nm][:cal] < 0.01), ALPHA_MIN, ALPHA_MAX))\n",
    "    print(f\"  {nm}: α≈{alpha_eff:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== Scalable Unsupervised PyOD Ensemble (iso16) — Plug & Play ===================\n",
    "# For ~36M samples/station (~70k windows with WIN=1024, HOP=512). No conformal, no XAI.\n",
    "# Models kept strictly scalable (no O(n^2)): ECOD, COPOD, IForest, INNE, HBOS, LODA.\n",
    "# Pipeline per station:\n",
    "#   iso16 features -> RobustScaler -> fit detectors -> rank-normalize scores -> mean-ensemble\n",
    "#   -> contamination threshold on ensemble -> majority smoothing -> your evaluator.\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np, warnings\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from scipy.signal import convolve\n",
    "\n",
    "# ---------------- scalable PyOD detectors ----------------\n",
    "from pyod.models.ecod import ECOD           # O(n·d)\n",
    "from pyod.models.copod import COPOD         # O(n·d)\n",
    "from pyod.models.iforest import IForest     # ~T * O(m log m), m=max_samples\n",
    "from pyod.models.inne import INNE           # scalable isolation via NN index\n",
    "from pyod.models.hbos import HBOS           # O(n·d)\n",
    "from pyod.models.loda import LODA           # O(n·d·proj)\n",
    "\n",
    "# ---------------- config (aligned with your runner) ----------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)\n",
    "SMOOTH_KRN = [1,1,1]               # 3-wide majority\n",
    "CONT       = 0.0015                # per-station contamination (~0.15%)\n",
    "rng = check_random_state(42)\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def make_windows(sig: np.ndarray, win:int, hop:int) -> np.ndarray:\n",
    "    n = (len(sig)-win)//hop + 1\n",
    "    out = np.empty((n, win), dtype=np.int16)\n",
    "    for i in range(n):\n",
    "        j = i*hop; out[i,:] = sig[j:j+win]\n",
    "    return out\n",
    "\n",
    "def smooth_bool(mask: np.ndarray, kernel=SMOOTH_KRN) -> np.ndarray:\n",
    "    krn = np.array(kernel, dtype=int)\n",
    "    half = int(np.ceil(krn.sum()/2))\n",
    "    return (convolve(mask.astype(int), krn, mode='same') >= half)\n",
    "\n",
    "def _rank_norm(scores: np.ndarray) -> np.ndarray:\n",
    "    # Stable rank normalization to [0,1] (higher = more anomalous)\n",
    "    s = scores.astype(np.float32, copy=False)\n",
    "    order = np.argsort(s, kind='mergesort')\n",
    "    ranks = np.empty_like(order, dtype=np.float32)\n",
    "    ranks[order] = np.linspace(0.0, 1.0, len(s), dtype=np.float32)\n",
    "    return ranks\n",
    "\n",
    "# Full iso16 features per window from your pipeline\n",
    "FX = FeatureExtractor([\"iso16\"])\n",
    "def iso16_features_by_station(raw: Dict[str,np.ndarray], win:int, hop:int, fs:int):\n",
    "    feats, _ = FX.transform(raw, win=win, hop=hop, fs=fs)  # {station: [n_win, d]}\n",
    "    return feats\n",
    "\n",
    "# Model set (no quadratic methods)\n",
    "def _models(cont):\n",
    "    cont = max(1e-4, cont)\n",
    "    return [\n",
    "        (\"ECOD\",    ECOD()),\n",
    "        (\"COPOD\",   COPOD()),\n",
    "        (\"IForest\", IForest(\n",
    "            n_estimators=400, max_samples=2048, contamination=cont,\n",
    "            max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42\n",
    "        )),\n",
    "        (\"INNE\",    INNE(contamination=cont, random_state=42)),\n",
    "        (\"HBOS\",    HBOS(n_bins=30, alpha=0.1, tol=0.5, contamination=cont)),\n",
    "        (\"LODA\",    LODA(n_bins=24, n_random_cuts=128, contamination=cont)),\n",
    "    ]\n",
    "\n",
    "# ---------------- feature build (once) ----------------\n",
    "n_win = min((len(storm_data.quantised[s]) - WIN)//HOP + 1 for s in STN)\n",
    "print(\"▶ Extracting iso16 features …\")\n",
    "feats_by_stn = iso16_features_by_station(storm_data.quantised, WIN, HOP, FS)\n",
    "for nm in feats_by_stn:\n",
    "    feats_by_stn[nm] = feats_by_stn[nm][:n_win].astype(np.float32, copy=False)\n",
    "\n",
    "hot_by_stn  : Dict[str, np.ndarray] = {}\n",
    "score_by_stn: Dict[str, np.ndarray] = {}\n",
    "\n",
    "# ---------------- train/infer per station ----------------\n",
    "print(\"▶ Training scalable ensemble per station …\")\n",
    "for nm in STN:\n",
    "    X = feats_by_stn[nm]            # [n_win, d]\n",
    "    # robust scaling (median/IQR)\n",
    "    scaler = RobustScaler().fit(X)\n",
    "    Xc = scaler.transform(X).astype(np.float32, copy=False)\n",
    "\n",
    "    # fit detectors, collect rank-normalized scores\n",
    "    model_handles = []\n",
    "    base_scores   = []\n",
    "    for label, mdl in _models(CONT):\n",
    "        try:\n",
    "            mdl.fit(Xc)\n",
    "            if hasattr(mdl, \"decision_function\"):\n",
    "                s = mdl.decision_function(Xc).astype(np.float32, copy=False)\n",
    "            else:\n",
    "                s = getattr(mdl, \"decision_scores_\", None)\n",
    "                if not isinstance(s, np.ndarray) or s.shape[0] != Xc.shape[0]:\n",
    "                    raise RuntimeError(f\"{label}: no usable scores\")\n",
    "                s = s.astype(np.float32, copy=False)\n",
    "            base_scores.append(_rank_norm(s))\n",
    "            model_handles.append((label, mdl))\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"{nm}: {label} failed: {e}\")\n",
    "\n",
    "    if not base_scores:\n",
    "        raise RuntimeError(f\"{nm}: no detectors succeeded.\")\n",
    "\n",
    "    # ensemble score: mean of rank-normalized scores (higher = more anomalous)\n",
    "    S = np.mean(np.stack(base_scores, axis=1), axis=1).astype(np.float32, copy=False)\n",
    "    score_by_stn[nm] = S\n",
    "\n",
    "    # threshold by contamination on ensemble\n",
    "    thr = np.quantile(S, 1.0 - CONT)\n",
    "    yhat = S >= thr\n",
    "\n",
    "    # smooth station mask\n",
    "    hot_by_stn[nm] = smooth_bool(yhat, kernel=SMOOTH_KRN)\n",
    "\n",
    "# ---------------- network aggregation + evaluation (your pipeline) ----------------\n",
    "agg_mask = (np.sum(np.stack([hot_by_stn[s] for s in STN], axis=0), axis=0) >= MIN_STN)\n",
    "\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS, burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "st_m, net_m, n_win_eval = evaluate_windowed_model(\n",
    "    hot            = hot_by_stn,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "# ---------------- summaries ----------------\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_eval:,}) ——\")\n",
    "for nm, m in st_m.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(\"\\nNetwork-level metrics:\", pretty_metrics(net_m, ndigits=3))\n",
    "\n",
    "# Notes:\n",
    "# • All chosen detectors are near-linear in n and d; IForest bounded by max_samples=2048.\n",
    "# • Adjust CONT to tune aggressiveness (try 0.001–0.005). Increase IForest max_samples if CPU allows.\n",
    "# • For even more scale and parallelism, consider SUOD (PyOD) to orchestrate detector ensembles efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is good also ========================= Extended IsoForest + Network Combined Probability (Top-K Fisher) =========================\n",
    "# Plug-and-play with your setup (iso16 features + evaluate_windowed_model).\n",
    "# - Defines a lean ExtendedIsoForest using isotree (avg depth scoring + per-station depth threshold).\n",
    "# - Calibrates per-station conformal p-values from EIF depth (first CAL_FRAC windows).\n",
    "# - Combines station p-values per time with Top-K Fisher (K = MIN_STN) → network p_net[t]; anomaly prob = 1 - p_net.\n",
    "# - Evaluates with your pipeline and (optionally) prints per-stroke combined probabilities.\n",
    "\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import numpy as np, os, warnings, math\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.signal import convolve\n",
    "from scipy.ndimage import maximum_filter1d\n",
    "from scipy.stats import chi2\n",
    "from isotree import IsolationForest as IsoTreeIF\n",
    "\n",
    "# ----------------------- Config (aligned with your runner) -----------------------\n",
    "WIN, HOP   = 1024, 512\n",
    "FS         = cfg.fs\n",
    "STN        = list(storm_data.quantised)\n",
    "MIN_STN    = 2\n",
    "BURST_LEN  = int(0.04 * FS)                 # ~40 ms in samples → windows via HOP below\n",
    "PERSIST_W  = max(1, int(round(BURST_LEN / HOP)))\n",
    "\n",
    "# EIF knobs\n",
    "BASE_CONT  = 0.0015                         # ~0.15% tail for station depth threshold\n",
    "NTREES     = 400\n",
    "MAX_SAMPLES= 2048\n",
    "EXTREME_Q  = 99.95                          # recall \"rescue\" (add deepest 0.05% from train)\n",
    "\n",
    "# Conformal calibration (p-values)\n",
    "CAL_FRAC   = 0.20                           # first 20% windows as cal slice\n",
    "CAL_MIN    = 5000\n",
    "\n",
    "# Smoothing & probabilities\n",
    "SMOOTH_KRN = [1,1,1]                        # 3-wide majority on station masks\n",
    "\n",
    "# -------------------------------- Utilities --------------------------------\n",
    "def make_windows(sig: np.ndarray, win:int, hop:int) -> np.ndarray:\n",
    "    n = (len(sig)-win)//hop + 1\n",
    "    out = np.empty((n, win), dtype=np.int16)\n",
    "    for i in range(n):\n",
    "        j = i*hop\n",
    "        out[i,:] = sig[j:j+win]\n",
    "    return out\n",
    "\n",
    "def smooth_bool(mask: np.ndarray, kernel=SMOOTH_KRN) -> np.ndarray:\n",
    "    krn = np.array(kernel, dtype=int)\n",
    "    half = int(np.ceil(krn.sum()/2))\n",
    "    return (convolve(mask.astype(int), krn, mode='same') >= half)\n",
    "\n",
    "def _avg_depth(iso: IsoTreeIF, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Get isotree average depth (larger = more normal). Compatible across versions.\"\"\"\n",
    "    for kw in ({\"type\":\"avg_depth\"}, {\"output_type\":\"avg_depth\"}):\n",
    "        try:    return iso.predict(X, **kw).astype(np.float32)\n",
    "        except TypeError: pass\n",
    "    # fallback: signed score (flip)\n",
    "    return (-iso.predict(X)).astype(np.float32)\n",
    "\n",
    "# -------------------------------- Extended IsoForest --------------------------------\n",
    "class ExtendedIsoForest:\n",
    "    \"\"\"\n",
    "    Depth-aware isotree IF per station with:\n",
    "      - robust scaling\n",
    "      - station depth threshold from a quantile (BASE_CONT) with a minimal-flag guard\n",
    "      - optional \"extreme tail rescue\" from training depth\n",
    "    Uses FeatureExtractor([\"iso16\"]) for features.\n",
    "    Stores: mods[nm] = (scaler, iso_model, depth_train), thr[nm] = depth threshold\n",
    "    \"\"\"\n",
    "    def __init__(self, win:int=WIN, hop:int=HOP, base_cont:float=BASE_CONT,\n",
    "                 n_trees:int=NTREES, max_samples:int=MAX_SAMPLES, extreme_q:float=EXTREME_Q):\n",
    "        self.win=win; self.hop=hop; self.base_cont=base_cont\n",
    "        self.n_trees=n_trees; self.max_samples=max_samples; self.extreme_q=extreme_q\n",
    "        # Use your iso16 feature extractor\n",
    "        try:\n",
    "            self.fx = FeatureExtractor([\"iso16\"])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"FeatureExtractor(['iso16']) not available in scope.\") from e\n",
    "        self.mods : Dict[str, tuple] = {}\n",
    "        self.thr  : Dict[str, float] = {}\n",
    "\n",
    "    def fit(self, raw:Dict[str,np.ndarray], fs:int, verbose:bool=True):\n",
    "        feats,_ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        for nm,X in feats.items():\n",
    "            scaler = RobustScaler().fit(X)\n",
    "            Xs     = scaler.transform(X).astype(np.float32, copy=False)\n",
    "            iso    = IsoTreeIF(\n",
    "                        ntrees=self.n_trees, sample_size=self.max_samples,\n",
    "                        ndim=max(1, X.shape[1]-1), prob_pick_avg_gain=0, prob_pick_pooled_gain=0,\n",
    "                        nthreads=max(1, os.cpu_count()-1), random_seed=42\n",
    "                     ).fit(Xs)\n",
    "            depth  = _avg_depth(iso, Xs)  # larger = more normal\n",
    "            # choose depth threshold at base_cont, but ensure we flag *something*\n",
    "            grid = np.linspace(self.base_cont, min(0.01, self.base_cont*5), 5, dtype=np.float32)\n",
    "            thr = np.quantile(depth, grid[0])\n",
    "            for c in grid:\n",
    "                thr_c = np.quantile(depth, c)\n",
    "                if (depth < thr_c).sum() >= 0.001*len(depth):\n",
    "                    thr = thr_c; break\n",
    "            self.mods[nm] = (scaler, iso, depth)\n",
    "            self.thr[nm]  = float(thr)\n",
    "            if verbose:\n",
    "                print(f\"{nm}: depth thr {thr:.4f}  flagged {int((depth<thr).sum())}/{len(depth)} (~{(depth<thr).mean():.3%})\")\n",
    "\n",
    "    def predict(self, raw:Dict[str,np.ndarray], fs:int) -> Dict[str,np.ndarray]:\n",
    "        feats,_ = self.fx.transform(raw, win=self.win, hop=self.hop, fs=fs)\n",
    "        out: Dict[str,np.ndarray] = {}\n",
    "        for nm,X in feats.items():\n",
    "            scaler, iso, depth_train = self.mods[nm]\n",
    "            Xs     = scaler.transform(X).astype(np.float32, copy=False)\n",
    "            depth  = _avg_depth(iso, Xs)\n",
    "            mask   = depth < self.thr[nm]                                # main IF mask (low depth = anomalous)\n",
    "            # extreme-tail rescue (use train depth distribution)\n",
    "            extreme_thr = np.percentile(depth_train, 100 - EXTREME_Q)\n",
    "            mask |= depth < extreme_thr\n",
    "            out[nm] = mask\n",
    "        return out\n",
    "\n",
    "# -------------------------------- Fit EIF + station masks --------------------------------\n",
    "print(\"▶ Fitting Extended IsoForest …\")\n",
    "eif = ExtendedIsoForest(base_cont=BASE_CONT, n_trees=NTREES, max_samples=MAX_SAMPLES, extreme_q=EXTREME_Q)\n",
    "eif.fit(storm_data.quantised, fs=FS, verbose=False)\n",
    "\n",
    "print(\"▶ Predicting station masks …\")\n",
    "hot_by_stn = eif.predict(storm_data.quantised, fs=FS)\n",
    "# Align length across stations\n",
    "n_win = min(len(hot_by_stn[nm]) for nm in STN)\n",
    "for nm in STN:\n",
    "    hot_by_stn[nm] = smooth_bool(hot_by_stn[nm][:n_win], kernel=SMOOTH_KRN)\n",
    "\n",
    "# -------------------------------- Per-station p-values from EIF depth --------------------------------\n",
    "# Strategy: split-conformal on depth with a calibration slice (first CAL_FRAC or CAL_MIN).\n",
    "# Smaller depth = more anomalous → use left-tail rank.\n",
    "print(\"▶ Building station p-values (conformal on EIF depth) …\")\n",
    "# Recompute per-station depth for the evaluation windows\n",
    "feats,_ = eif.fx.transform(storm_data.quantised, win=WIN, hop=HOP, fs=FS)\n",
    "for nm in STN:\n",
    "    feats[nm] = feats[nm][:n_win]\n",
    "depth_by_stn: Dict[str,np.ndarray] = {}\n",
    "pval_by_stn : Dict[str,np.ndarray] = {}\n",
    "\n",
    "for nm in STN:\n",
    "    scaler, iso, depth_train = eif.mods[nm]\n",
    "    Xs = scaler.transform(feats[nm]).astype(np.float32, copy=False)\n",
    "    depth = _avg_depth(iso, Xs)                    # larger = more normal; small = anomalous\n",
    "    depth_by_stn[nm] = depth\n",
    "    # calibration indices\n",
    "    m_cal = min(max(CAL_MIN, int(CAL_FRAC * len(depth))), len(depth)-1)\n",
    "    cal_idx = np.arange(m_cal, dtype=int)\n",
    "    d_cal = np.sort(depth[cal_idx])\n",
    "    # left-tail p-value (smaller depth -> smaller p)\n",
    "    # p = (1 + #{d_cal <= d_t}) / (n_cal + 1)\n",
    "    idx = np.searchsorted(d_cal, depth, side='right')\n",
    "    p   = (idx + 1.0) / (len(d_cal) + 1.0)\n",
    "    pval_by_stn[nm] = p.astype(np.float32)\n",
    "\n",
    "# -------------------------------- Network combined probability (Top-K Fisher) --------------------------------\n",
    "# For each time t:\n",
    "#   - take the K = MIN_STN smallest station p-values at t\n",
    "#   - Fisher statistic: X = -2 * sum ln p_k  ~ Chi2(2K)\n",
    "#   - combined p_net[t] = sf_chi2(X, df=2K); anomaly probability = 1 - p_net[t]\n",
    "print(\"▶ Combining station p-values into network probability …\")\n",
    "K = MIN_STN\n",
    "P_mat = np.stack([pval_by_stn[nm] for nm in STN], axis=0)       # (N, T)\n",
    "P_sorted = np.sort(P_mat, axis=0)                               # ascending across stations\n",
    "P_topK   = P_sorted[:K, :]                                      # (K, T)\n",
    "X_stat   = -2.0 * np.sum(np.log(np.clip(P_topK, 1e-12, 1.0)), axis=0)\n",
    "p_net    = chi2.sf(X_stat, df=2*K).astype(np.float32)           # combined p-value under H0 (small = anomalous)\n",
    "prob_net = (1.0 - p_net).astype(np.float32)                     # anomaly \"probability-like\" score in [0,1]\n",
    "\n",
    "# -------------------------------- Evaluate with your pipeline --------------------------------\n",
    "print(\"▶ Evaluating …\")\n",
    "agg_mask = (np.sum(np.stack([hot_by_stn[s] for s in STN], axis=0), axis=0) >= MIN_STN)\n",
    "eval_cfg = EvalConfig(win=WIN, hop=HOP, fs=FS, burst_len=BURST_LEN, min_stn=MIN_STN, tol_win=0)\n",
    "st_m, net_m, n_win_eval = evaluate_windowed_model(\n",
    "    hot            = hot_by_stn,\n",
    "    stroke_records = storm_data.stroke_records,\n",
    "    quantized      = storm_data.quantised,\n",
    "    station_order  = STN,\n",
    "    cfg            = eval_cfg,\n",
    "    plot           = True\n",
    ")\n",
    "\n",
    "print(f\"\\n—— Station / window metrics  (n_windows = {n_win_eval:,}) ——\")\n",
    "for nm, m in st_m.items():\n",
    "    print(f\"{nm}: TP={m['TP']:3d} FP={m['FP']:4d} FN={m['FN']:3d} \"\n",
    "          f\"P={m['P']:.3f} R={m['R']:.3f} F1={m['F1']:.3f}\")\n",
    "\n",
    "print(\"\\n—— Network / stroke metrics ——\")\n",
    "print(\"\\nNetwork-level metrics:\", pretty_metrics(net_m, ndigits=3))\n",
    "\n",
    "# -------------------------------- Optional: per-stroke combined probabilities --------------------------------\n",
    "# Attempts to map stroke_records → window indices; prints each stroke's network anomaly probability.\n",
    "def guess_stroke_win_indices(stroke_records: Any, fs:int, win:int, hop:int, n_windows:int) -> List[int]:\n",
    "    \"\"\"Best-effort mapping of stroke_records to window indices.\n",
    "       Handles lists of ints (samples), floats (seconds), or dicts with sample/time/window keys.\"\"\"\n",
    "    idxs: List[int] = []\n",
    "    try:\n",
    "        if isinstance(stroke_records, (list, tuple, np.ndarray)):\n",
    "            for item in stroke_records:\n",
    "                if isinstance(item, dict):\n",
    "                    if \"win\" in item:      i = int(item[\"win\"])\n",
    "                    elif \"win_idx\" in item or \"window_idx\" in item:\n",
    "                        i = int(item.get(\"win_idx\", item.get(\"window_idx\")))\n",
    "                    elif \"sample\" in item or \"sample_idx\" in item or \"t\" in item:\n",
    "                        s = int(item.get(\"sample\", item.get(\"sample_idx\", item.get(\"t\"))))\n",
    "                        i = max(0, min(n_windows-1, s // hop))\n",
    "                    elif \"time\" in item or \"time_s\" in item:\n",
    "                        sec = float(item.get(\"time\", item.get(\"time_s\")))\n",
    "                        s   = int(round(sec * fs))\n",
    "                        i   = max(0, min(n_windows-1, s // hop))\n",
    "                    else:\n",
    "                        continue\n",
    "                elif isinstance(item, (int, np.integer)):\n",
    "                    # assume sample index\n",
    "                    i = max(0, min(n_windows-1, int(item) // hop))\n",
    "                elif isinstance(item, (float, np.floating)):\n",
    "                    # assume seconds\n",
    "                    s = int(round(float(item) * fs))\n",
    "                    i = max(0, min(n_windows-1, s // hop))\n",
    "                else:\n",
    "                    continue\n",
    "                idxs.append(i)\n",
    "        else:\n",
    "            # unknown type; give up quietly\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not parse stroke_records: {e}\")\n",
    "    # de-dup and sort\n",
    "    return sorted(set(idxs))\n",
    "\n",
    "stroke_win_idx = guess_stroke_win_indices(storm_data.stroke_records, FS, WIN, HOP, n_win)\n",
    "\n",
    "if stroke_win_idx:\n",
    "    print(\"\\n—— Per-stroke combined network anomaly probability ——\")\n",
    "    for i in stroke_win_idx:\n",
    "        # show Top-K contributing station p-values too\n",
    "        col = P_mat[:, i]\n",
    "        top_order = np.argsort(col)[:K]\n",
    "        top_stations = [f\"{STN[j]}:p={col[j]:.3e}\" for j in top_order]\n",
    "        print(f\"win {i:6d}  prob_net={prob_net[i]:.3f}  (top{K} stations → {', '.join(top_stations)})\")\n",
    "else:\n",
    "    print(\"\\n(No stroke window indices could be inferred; skip per-stroke printout.)\")\n",
    "\n",
    "# -------------------------------- Notes --------------------------------\n",
    "# • Station p-values: split-conformal on EIF depth using a calibration slice → distribution-free tails.\n",
    "# • Network probability: Top-K Fisher focuses on the K strongest stations at each time to mirror your ≥K rule.\n",
    "# • prob_net is “probability-like” (1 - combined p-value). It’s not a Bayesian posterior, but it is monotone and calibrated in the tail.\n",
    "# • You can threshold prob_net directly for a network detector, or continue using station masks + ≥K vote (as above).\n",
    "# • If you prefer Tippett (min-p) instead of Fisher, replace the Fisher block with: p_net = 1 - (1 - p_min)**K on the K-min p’s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Real lightning: end-to-end QA + StormBundle emitter (UK English, verbose)\n",
    "# Requires: numpy, pandas\n",
    "# Inputs (pre-existing): df (station windows), df_evt (fixes)\n",
    "# Outputs (exact simulator contract):\n",
    "#   - storm_bundle (StormBundle or dict)\n",
    "#   - quantised, events, stroke_records, df_wave, df_labels\n",
    "# Also prints extensive QA summaries to guide anomaly % calibration later.\n",
    "# =============================================================================\n",
    "\n",
    "import math, numpy as np, pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "# ----------------------------- Configuration ---------------------------------\n",
    "FS                = 109_375          # Known sampling rate (Hz) — adjust if needed\n",
    "BITS              = 14               # Quantiser depth to match sim\n",
    "VREF              = 1.0              # Full-scale reference voltage\n",
    "PRE_PAD_S         = 0.5              # Scene padding before first window start (s)\n",
    "POST_PAD_S        = 1.0              # Scene padding after last window end (s)\n",
    "C_KM_S            = 300_000.0        # Propagation speed used for timing checks (km/s)\n",
    "WAVE_KEY          = \"samples\"        # Choose: \"samples\" | \"reconstructed_samples\" | \"envelope\" | \"raw_samples\"\n",
    "NORMALISE_WINDOW_TO_0p8_VREF = True  # If True, each window scaled to ±0.8*VREF to avoid clipping\n",
    "REPORT_TOP_N      = 12               # How many entries to show in tabular summaries\n",
    "\n",
    "# Station codes (to keep continuity with sim; unknown names fall back deterministically)\n",
    "STATION_CODE_MAP = {\n",
    "    \"Valentia\":\"VAL\", \"Herstmonceux\":\"HER\", \"Hersrmonceux\":\"HER\",\n",
    "    \"Lerwick\":\"LER\", \"Keflavik\":\"KEF\", \"Gibraltar\":\"GIB\", \"Akrotiri\":\"AKR\",\n",
    "    \"Camborne\":\"CAM\", \"Wattisham\":\"WAT\", \"Cabauw\":\"CAB\", \"Payerne\":\"PAY\",\n",
    "    \"Tartu\":\"TAR\",\n",
    "}\n",
    "\n",
    "# ------------------------------ Utilities ------------------------------------\n",
    "def _assert_columns(frame: pd.DataFrame, cols: List[str], name: str):\n",
    "    missing = [c for c in cols if c not in frame.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} missing required columns: {missing}\")\n",
    "\n",
    "def _code_for_site(name: Any) -> str:\n",
    "    s = str(name)\n",
    "    if s in STATION_CODE_MAP:\n",
    "        return STATION_CODE_MAP[s]\n",
    "    # Deterministic fallback: take first 3 A–Z characters uppercased\n",
    "    al = \"\".join(ch for ch in s if ch.isalpha()).upper()\n",
    "    return (al[:3] or \"UNK\")\n",
    "\n",
    "def _haversine_km(lat1, lon1, lat2, lon2) -> float:\n",
    "    R = 6371.0\n",
    "    φ1, φ2 = np.radians(lat1), np.radians(lat2)\n",
    "    dφ, dλ = np.radians(lat2 - lat1), np.radians(lon2 - lon1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return float(2*R*np.arcsin(np.sqrt(a)))\n",
    "\n",
    "def _peak_index(a: np.ndarray, wave_key: str) -> int:\n",
    "    if a is None or not isinstance(a, np.ndarray) or a.ndim != 1 or a.size == 0:\n",
    "        return -1\n",
    "    # For envelope use the absolute maximum directly; otherwise use |signal|\n",
    "    return int(np.nanargmax(a)) if wave_key == \"envelope\" else int(np.nanargmax(np.abs(a)))\n",
    "\n",
    "def _detect_bits_int(arr: np.ndarray) -> int:\n",
    "    vmax = int(np.max(np.abs(arr))) if arr.size else 0\n",
    "    for bits in (16, 15, 14, 13, 12):\n",
    "        if vmax <= (2**(bits-1) - 1):\n",
    "            return bits\n",
    "    # Fallback to dtype width\n",
    "    return int(arr.dtype.itemsize * 8) if hasattr(arr, \"dtype\") else 16\n",
    "\n",
    "def _fullscale(bits: int) -> int:\n",
    "    return 2**(bits-1) - 1\n",
    "\n",
    "def _to_seconds(ts: pd.Timestamp) -> float:\n",
    "    # Consistent POSIX seconds (float)\n",
    "    return float(pd.Timestamp(ts).timestamp())\n",
    "\n",
    "# ------------------------------ Basic checks ---------------------------------\n",
    "# Ensure df/df_evt exist and have required columns\n",
    "_assert_columns(df, [\n",
    "    \"fix_id\",\"fix_time\",\"fix_lat\",\"fix_lon\",\"site_name\",\n",
    "    \"station_lat\",\"station_lon\",\"station_time\",\n",
    "    \"samples\",\"reconstructed_samples\",\"envelope\",\"raw_samples\",\n",
    "    f\"{WAVE_KEY}_len\"\n",
    "], \"df (station windows)\")\n",
    "\n",
    "_assert_columns(df_evt, [\"fix_id\",\"time\",\"lat\",\"lon\"], \"df_evt (fixes)\")\n",
    "\n",
    "print(\">>> Dataset overview\")\n",
    "print(f\"  Station-windows df shape  : {df.shape}\")\n",
    "print(f\"  Fixes df_evt shape        : {df_evt.shape}\")\n",
    "print(f\"  Waveform choice (WAVE_KEY): {WAVE_KEY}\")\n",
    "print()\n",
    "\n",
    "# Ensure times are UTC and sorted in fixes\n",
    "df_evt = df_evt.copy()\n",
    "df_evt[\"time\"] = pd.to_datetime(df_evt[\"time\"], utc=True)\n",
    "df_evt = df_evt.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# ----------------------------- Availability ----------------------------------\n",
    "print(\">>> Availability by waveform key (rows with data)\")\n",
    "avail = (df[[f\"{k}_len\" for k in (\"samples\",\"reconstructed_samples\",\"envelope\",\"raw_samples\")]]\n",
    "         .astype(bool).sum().to_dict())\n",
    "for k, v in avail.items():\n",
    "    print(f\"  {k.replace('_len',''):>22}: {v:,}\")\n",
    "print()\n",
    "\n",
    "# ---------------------------- Select rows to use ------------------------------\n",
    "# We only place windows that have:\n",
    "#   - a valid station_time (needed to anchor the window),\n",
    "#   - a valid waveform array under WAVE_KEY.\n",
    "use = df.loc[\n",
    "    df[\"station_time\"].notna() & (df[f\"{WAVE_KEY}_len\"] > 0),\n",
    "    [\"fix_id\",\"fix_time\",\"station_time\",\"fix_lat\",\"fix_lon\",\"site_name\",\n",
    "     \"station_lat\",\"station_lon\", WAVE_KEY, f\"{WAVE_KEY}_len\"]\n",
    "].copy()\n",
    "\n",
    "n_source = len(df)\n",
    "n_use    = len(use)\n",
    "print(\">>> Selection for placement\")\n",
    "print(f\"  Rows in df                                 : {n_source:,}\")\n",
    "print(f\"  Rows with valid {WAVE_KEY} & station_time   : {n_use:,} \"\n",
    "      f\"({100.0*n_use/max(n_source,1):.1f}%)\")\n",
    "print()\n",
    "\n",
    "# -------------------------- Station coding & distances ------------------------\n",
    "use[\"station_code\"] = use[\"site_name\"].apply(_code_for_site)\n",
    "use[\"fix_time\"]     = pd.to_datetime(use[\"fix_time\"], utc=True)\n",
    "use[\"station_time\"] = pd.to_datetime(use[\"station_time\"], utc=True)\n",
    "\n",
    "use[\"dist_km\"] = [\n",
    "    _haversine_km(a, b, c, d)\n",
    "    for a, b, c, d in zip(use[\"fix_lat\"], use[\"fix_lon\"], use[\"station_lat\"], use[\"station_lon\"])\n",
    "]\n",
    "\n",
    "print(\">>> Station catalogue (top by count)\")\n",
    "top_sites = (use[\"site_name\"].value_counts().head(REPORT_TOP_N)).to_dict()\n",
    "for nm, ct in top_sites.items():\n",
    "    print(f\"  {nm:<20}  count={ct:>5}  code={_code_for_site(nm)}\")\n",
    "print()\n",
    "\n",
    "# ---------------------- Timing anchor inference (H1/H2/H3) --------------------\n",
    "# Hypotheses:\n",
    "#   H1: station_time is the event/arrival time at the station\n",
    "#   H2: station_time is the window start\n",
    "#   H3: station_time is the window centre\n",
    "# We align against physics: expected arrival = fix_time + dist/c\n",
    "\n",
    "print(\">>> Timing anchor inference (H1/H2/H3) against fix_time + dist/c\")\n",
    "use[\"pred_off_s\"] = use[\"dist_km\"] / C_KM_S\n",
    "use[\"dt_obs_s\"]   = (use[\"station_time\"] - use[\"fix_time\"]).dt.total_seconds()\n",
    "\n",
    "# Peak index\n",
    "use[\"ipk\"] = use[WAVE_KEY].apply(lambda a: _peak_index(a, WAVE_KEY))\n",
    "\n",
    "# Residuals (seconds) for each hypothesis\n",
    "use[\"r_H1_s\"] = use[\"dt_obs_s\"] - use[\"pred_off_s\"]\n",
    "use[\"r_H2_s\"] = use[\"dt_obs_s\"] + (use[\"ipk\"] / FS) - use[\"pred_off_s\"]\n",
    "use[\"r_H3_s\"] = use[\"dt_obs_s\"] + ((use[\"ipk\"] - use[f\"{WAVE_KEY}_len\"]/2) / FS) - use[\"pred_off_s\"]\n",
    "\n",
    "def _stats_ms(x: np.ndarray) -> Dict[str, float]:\n",
    "    x = np.asarray(x, float)\n",
    "    return {\n",
    "        \"N\": int(np.isfinite(x).sum()),\n",
    "        \"median_ms\": float(np.nanmedian(x) * 1e3),\n",
    "        \"std_ms\": float(np.nanstd(x) * 1e3),\n",
    "        \"p95_ms\": float(np.nanpercentile(x, 95) * 1e3),\n",
    "    }\n",
    "\n",
    "sH1, sH2, sH3 = _stats_ms(use[\"r_H1_s\"].values), _stats_ms(use[\"r_H2_s\"].values), _stats_ms(use[\"r_H3_s\"].values)\n",
    "for name, st in [(\"H1\", sH1), (\"H2\", sH2), (\"H3\", sH3)]:\n",
    "    print(f\"  {name}: N={st['N']:>6}  median={st['median_ms']:+8.3f} ms  std={st['std_ms']:7.3f} ms  p95={st['p95_ms']:7.3f} ms\")\n",
    "\n",
    "# Choose the best anchor by |median| + std (ms)\n",
    "crit = {\n",
    "    \"H1\": abs(sH1[\"median_ms\"]) + sH1[\"std_ms\"],\n",
    "    \"H2\": abs(sH2[\"median_ms\"]) + sH2[\"std_ms\"],\n",
    "    \"H3\": abs(sH3[\"median_ms\"]) + sH3[\"std_ms\"],\n",
    "}\n",
    "ANCHOR = min(crit, key=lambda k: crit[k])\n",
    "print(f\"  >>> Chosen ANCHOR: {ANCHOR}  (criterion |median|+std in ms)\")\n",
    "print()\n",
    "\n",
    "# ----------------------- Compute per-window start times -----------------------\n",
    "def _start_time_row(row) -> pd.Timestamp:\n",
    "    L = int(row[f\"{WAVE_KEY}_len\"])\n",
    "    if ANCHOR == \"H2\":   # station_time = window start\n",
    "        return row[\"station_time\"]\n",
    "    if ANCHOR == \"H3\":   # station_time = window centre\n",
    "        return row[\"station_time\"] - pd.to_timedelta(L/(2*FS), unit=\"s\")\n",
    "    # H1: station_time marks the peak (arrival); back off by ipk samples\n",
    "    return row[\"station_time\"] - pd.to_timedelta(row[\"ipk\"]/FS, unit=\"s\")\n",
    "\n",
    "use[\"start_time\"] = use.apply(_start_time_row, axis=1)\n",
    "use[\"end_time\"]   = use[\"start_time\"] + pd.to_timedelta(use[f\"{WAVE_KEY}_len\"]/FS, unit=\"s\")\n",
    "\n",
    "# Final residual check (ms) using peak absolute time vs physics\n",
    "use[\"abs_peak_time\"] = use[\"start_time\"] + pd.to_timedelta(use[\"ipk\"]/FS, unit=\"s\")\n",
    "use[\"r_final_ms\"]    = (use[\"abs_peak_time\"] - (use[\"fix_time\"] + pd.to_timedelta(use[\"pred_off_s\"], unit=\"s\")))\\\n",
    "                           .dt.total_seconds()*1e3\n",
    "\n",
    "desc_r = use[\"r_final_ms\"].describe(percentiles=[.5, .9, .95, .99])\n",
    "print(\">>> Final timing residuals (abs_peak_time vs fix_time+dist/c) [ms]\")\n",
    "print(desc_r.to_string())\n",
    "print(f\"  % within ±1 ms : {100.0*np.mean(np.abs(use['r_final_ms'])<=1.0):5.1f}%\")\n",
    "print(f\"  % within ±2 ms : {100.0*np.mean(np.abs(use['r_final_ms'])<=2.0):5.1f}%\")\n",
    "print(f\"  % within ±5 ms : {100.0*np.mean(np.abs(use['r_final_ms'])<=5.0):5.1f}%\")\n",
    "print()\n",
    "\n",
    "# ------------------------- Time ordering by station ---------------------------\n",
    "print(\">>> Time ordering by station (based on start_time)\")\n",
    "ordering_rows = []\n",
    "for code, grp in use.sort_values(\"start_time\").groupby(\"station_code\"):\n",
    "    t = grp[\"start_time\"].values\n",
    "    # count inversions by comparing with sorted order (cheap proxy)\n",
    "    n_bad = int(np.sum(np.diff(grp[\"start_time\"].astype('int64').values) < 0))\n",
    "    ordering_rows.append((code, len(grp), n_bad))\n",
    "if ordering_rows:\n",
    "    bad_any = sum(n_bad for _,_,n_bad in ordering_rows)\n",
    "    print(f\"  Stations with any out-of-order starts: {sum(1 for _,_,n in ordering_rows if n>0)} \"\n",
    "          f\"out of {len(ordering_rows)} (total inversions={bad_any})\")\n",
    "    print(\"  (Should be zero; any >0 suggests a station_time anomaly or duplicate timestamps.)\")\n",
    "print()\n",
    "\n",
    "# -------------------------- Window length variability -------------------------\n",
    "print(\">>> Window length statistics (in samples, chosen WAVE_KEY only)\")\n",
    "print(use[f\"{WAVE_KEY}_len\"].describe(percentiles=[.5, .9, .95, .99]).to_string())\n",
    "print()\n",
    "\n",
    "# --------------------------- Amplitude/clipping QA ----------------------------\n",
    "def _minmax(arr: np.ndarray) -> Tuple[float, float]:\n",
    "    if not isinstance(arr, np.ndarray) or arr.size == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    return (float(np.nanmin(arr)), float(np.nanmax(arr)))\n",
    "\n",
    "use[\"amp_min\"], use[\"amp_max\"] = zip(*use[WAVE_KEY].apply(_minmax))\n",
    "\n",
    "def _clip_pct(a: np.ndarray, rel=0.98) -> float:\n",
    "    if not isinstance(a, np.ndarray) or a.size == 0:\n",
    "        return np.nan\n",
    "    m = float(np.nanmax(np.abs(a)))\n",
    "    return float(np.mean(np.abs(a) >= (rel * m)) * 100.0) if m > 0 else 0.0\n",
    "\n",
    "use[\"pct_clip98\"] = use[WAVE_KEY].apply(_clip_pct)\n",
    "print(\">>> Amplitude/clipping quick stats (chosen WAVE_KEY)\")\n",
    "print(use[[\"amp_min\",\"amp_max\",\"pct_clip98\"]].describe().to_string())\n",
    "print()\n",
    "\n",
    "# ------------------------ Build continuous scene timeline ---------------------\n",
    "# Compute scene start/end from window start/end times; add small padding\n",
    "t0 = use[\"start_time\"].min() - pd.to_timedelta(PRE_PAD_S, unit=\"s\")\n",
    "t1 = use[\"end_time\"].max()   + pd.to_timedelta(POST_PAD_S, unit=\"s\")\n",
    "N  = int(math.ceil((t1 - t0).total_seconds() * FS))\n",
    "print(\">>> Scene timebase\")\n",
    "print(f\"  Scene start (UTC): {t0}\")\n",
    "print(f\"  Scene end   (UTC): {t1}\")\n",
    "print(f\"  Sampling rate     : {FS:,} Hz\")\n",
    "print(f\"  Samples per station: {N:,}  (~{N/FS:,.2f} s)\")\n",
    "print()\n",
    "\n",
    "# Allocate accumulators (float64, then a single quantisation at the end)\n",
    "station_codes = sorted(use[\"station_code\"].unique())\n",
    "acc: Dict[str, np.ndarray] = {code: np.zeros(N, np.float64) for code in station_codes}\n",
    "\n",
    "# ----------------------------- Placement routine ------------------------------\n",
    "# Notes on amplitude:\n",
    "#  - If NORMALISE_WINDOW_TO_0p8_VREF: each window is scaled so its peak is ±0.8*VREF\n",
    "#    (robust to raw scale differences; safer against clipping when windows overlap).\n",
    "#  - Else:\n",
    "#       • integer arrays are mapped to ±VREF using detected bit depth,\n",
    "#       • floating arrays are left as-is (beware of clipping).\n",
    "placed_rows = []   # for stroke_records + labels\n",
    "dropped = {\"no_array\":0, \"neg_idx\":0, \"out_of_bounds\":0}\n",
    "\n",
    "for i, row in use.iterrows():\n",
    "    a = row[WAVE_KEY]\n",
    "    if not isinstance(a, np.ndarray) or a.ndim != 1 or a.size == 0:\n",
    "        dropped[\"no_array\"] += 1\n",
    "        continue\n",
    "\n",
    "    # Convert to float64 working scale\n",
    "    if np.issubdtype(a.dtype, np.integer):\n",
    "        bits_in = _detect_bits_int(a)\n",
    "        a_float = (a.astype(np.float64) / max(_fullscale(bits_in),1)) * VREF\n",
    "    else:\n",
    "        a_float = a.astype(np.float64)\n",
    "\n",
    "    if NORMALISE_WINDOW_TO_0p8_VREF:\n",
    "        peak = float(np.nanmax(np.abs(a_float))) if a_float.size else 0.0\n",
    "        if peak > 0:\n",
    "            a_float = (a_float / peak) * (0.8 * VREF)\n",
    "\n",
    "    # Compute insertion index\n",
    "    start_idx = int(round((row[\"start_time\"] - t0).total_seconds() * FS))\n",
    "    if start_idx < 0:\n",
    "        # drop windows that would begin before the scene (should not happen with padding)\n",
    "        dropped[\"neg_idx\"] += 1\n",
    "        continue\n",
    "\n",
    "    L = int(row[f\"{WAVE_KEY}_len\"])\n",
    "    end_idx = start_idx + L\n",
    "    if end_idx > N:\n",
    "        # truncate gracefully at the end\n",
    "        L = N - start_idx\n",
    "        end_idx = N\n",
    "        if L <= 0:\n",
    "            dropped[\"out_of_bounds\"] += 1\n",
    "            continue\n",
    "        a_float = a_float[:L]\n",
    "\n",
    "    acc[row[\"station_code\"]][start_idx:end_idx] += a_float[:L]\n",
    "\n",
    "    # Record for labels\n",
    "    placed_rows.append({\n",
    "        \"event_id\": int(row[\"fix_id\"]),\n",
    "        \"stroke_i\": 0,\n",
    "        \"station\": row[\"station_code\"],\n",
    "        \"flash_type\": \"FIX\",\n",
    "        \"lat\": float(row[\"fix_lat\"]),\n",
    "        \"lon\": float(row[\"fix_lon\"]),\n",
    "        \"true_time_s\": _to_seconds(row[\"fix_time\"]),\n",
    "        \"sample_idx\": int(start_idx),\n",
    "        \"window_idx\": int(start_idx // 1024),\n",
    "        # helpful QA fields for later analysis\n",
    "        \"win_len\": int(L),\n",
    "        \"peak_sample\": int(row[\"ipk\"]),\n",
    "        \"dist_km\": float(row[\"dist_km\"]),\n",
    "        \"residual_ms\": float(row[\"r_final_ms\"]),\n",
    "    })\n",
    "\n",
    "print(\">>> Placement summary\")\n",
    "print(f\"  Windows placed   : {len(placed_rows):,}\")\n",
    "print(f\"  Dropped (no_array): {dropped['no_array']:,} | \"\n",
    "      f\"neg_idx: {dropped['neg_idx']:,} | out_of_bounds: {dropped['out_of_bounds']:,}\")\n",
    "print()\n",
    "\n",
    "# --------------------------- Quantisation (single pass) -----------------------\n",
    "full = _fullscale(BITS)\n",
    "quantised: Dict[str, np.ndarray] = {}\n",
    "scales: Dict[str, float] = {}\n",
    "\n",
    "for code, arr in acc.items():\n",
    "    peak = float(np.nanmax(np.abs(arr))) if arr.size else 0.0\n",
    "    scale = 1.0\n",
    "    if peak > VREF and peak > 0:\n",
    "        scale = VREF / peak\n",
    "        arr = arr * scale  # gentle limiter to keep within ±VREF\n",
    "    q = np.clip(np.round((arr / VREF) * full), -full, full).astype(np.int16)\n",
    "    quantised[code] = q\n",
    "    scales[code] = scale\n",
    "\n",
    "# ------------------------------ Build DataFrames ------------------------------\n",
    "time_s = np.arange(N, dtype=np.float64) / FS  # simulator-style 0-based seconds\n",
    "\n",
    "df_wave = pd.DataFrame({\"time_s\": time_s})\n",
    "for code in station_codes:\n",
    "    df_wave[code] = quantised[code]\n",
    "\n",
    "df_labels = pd.DataFrame(placed_rows)\n",
    "\n",
    "# Events table (one per fix), simulator-compatible\n",
    "events: List[dict] = []\n",
    "for _, r in df_evt.iterrows():\n",
    "    events.append({\n",
    "        \"id\": int(r[\"fix_id\"]),\n",
    "        \"flash_type\": \"FIX\",\n",
    "        \"lat\": float(r[\"lat\"]),\n",
    "        \"lon\": float(r[\"lon\"]),\n",
    "        \"time_s\": _to_seconds(r[\"time\"]),\n",
    "    })\n",
    "\n",
    "# ------------------------------ Package result --------------------------------\n",
    "# Try to return an actual StormBundle; if not available, return a dict with same keys\n",
    "try:\n",
    "    from storm import StormBundle  # adjust import path if your class lives elsewhere\n",
    "    storm_bundle = StormBundle(quantised, events, placed_rows, df_wave, df_labels)\n",
    "except Exception:\n",
    "    storm_bundle = {\n",
    "        \"quantised\": quantised,\n",
    "        \"events\": events,\n",
    "        \"stroke_records\": placed_rows,\n",
    "        \"df_wave\": df_wave,\n",
    "        \"df_labels\": df_labels,\n",
    "    }\n",
    "\n",
    "# ------------------------------ Final summaries -------------------------------\n",
    "print(\">>> Scene summary (simulator-style)\")\n",
    "print(f\"  Anchor                  : {ANCHOR}\")\n",
    "print(f\"  Stations (columns)      : {len(station_codes)} -> {station_codes}\")\n",
    "print(f\"  Bits/Vref               : {BITS} / {VREF}\")\n",
    "print(f\"  Samples per station     : {N:,}  (~{N/FS:.2f} s)\")\n",
    "print(f\"  Events (fixes)          : {len(events):,}\")\n",
    "print(f\"  Station-windows placed  : {len(placed_rows):,}  \"\n",
    "      f\"({100.0*len(placed_rows)/max(len(df),1):.1f}% of df)\")\n",
    "print()\n",
    "\n",
    "print(\">>> Station-level gain scalers applied post-mix (to avoid clipping)\")\n",
    "for code in station_codes:\n",
    "    print(f\"  {code}: scale={scales[code]:.3f}\")\n",
    "\n",
    "print(\"\\n>>> Timing residuals by station (median ± std) [ms]\")\n",
    "by_st = df_labels.groupby(\"station\")[\"residual_ms\"]\n",
    "med = by_st.median().sort_values()\n",
    "std = by_st.std().reindex(med.index)\n",
    "for code in med.index:\n",
    "    print(f\"  {code}: {med.loc[code]:+7.3f} ± {std.loc[code]:6.3f}\")\n",
    "\n",
    "print(\"\\n>>> Window length distribution (top)\")\n",
    "top_lengths = df_labels[\"win_len\"].value_counts().head(REPORT_TOP_N)\n",
    "for L, ct in top_lengths.items():\n",
    "    print(f\"  length={L:6d} samples  count={ct:6d}  (~{L/FS*1e3:6.1f} ms)\")\n",
    "\n",
    "print(\"\\n>>> Coverage (windows per station)\")\n",
    "cov = df_labels[\"station\"].value_counts()\n",
    "for code, ct in cov.items():\n",
    "    print(f\"  {code}: {ct:6d}\")\n",
    "\n",
    "# Handy objects in your workspace now:\n",
    "#   - storm_bundle  (StormBundle or dict with the same fields)\n",
    "#   - quantised     (Dict[station_code, np.int16 ndarray of length N])\n",
    "#   - events        (List[dict] per fix)\n",
    "#   - df_wave       (time_s + one column per station)\n",
    "#   - df_labels     (one row per station×fix with indices + QA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Deep EDA & Hypothesis Suite for real lightning data (UK English, verbose)\n",
    "# Inputs:  df (station windows), df_evt (fixes)\n",
    "# Outputs: a set of tidy EDA tables and concise prints to guide next steps.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np, pandas as pd, math\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# ------------------------------ Configuration --------------------------------\n",
    "FS                = 109_375      # Hz (set to your known samplerate)\n",
    "C_KM_S            = 300_000.0    # propagation speed approximation (km/s)\n",
    "EDA_WAVE_KEY      = \"samples\"    # change to: \"reconstructed_samples\" | \"envelope\" | \"raw_samples\"\n",
    "MAX_ROWS_SCAN     = None         # set to e.g. 10_000 to cap compute on very large sets\n",
    "SHOW_TOP          = 15           # rows to print for head/tops\n",
    "\n",
    "# ------------------------------- Sanity checks --------------------------------\n",
    "req_df_cols = [\n",
    "    \"fix_id\",\"fix_time\",\"fix_lat\",\"fix_lon\",\"site_name\",\"station_lat\",\"station_lon\",\n",
    "    \"station_time\",\"samples\",\"reconstructed_samples\",\"envelope\",\"raw_samples\",\n",
    "    f\"{EDA_WAVE_KEY}_len\"\n",
    "]\n",
    "missing = [c for c in req_df_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"df is missing required columns for EDA: {missing}\")\n",
    "\n",
    "for c in (\"fix_id\",\"time\",\"lat\",\"lon\"):\n",
    "    if c not in df_evt.columns:\n",
    "        raise ValueError(f\"df_evt is missing required column: {c!r}\")\n",
    "\n",
    "# Optionally limit scan size\n",
    "_df = df if MAX_ROWS_SCAN is None else df.head(MAX_ROWS_SCAN).copy()\n",
    "_evt = df_evt.copy()\n",
    "\n",
    "# Ensure proper dtypes / ordering\n",
    "_df[\"fix_time\"]     = pd.to_datetime(_df[\"fix_time\"], utc=True)\n",
    "_df[\"station_time\"] = pd.to_datetime(_df[\"station_time\"], utc=True)\n",
    "_evt[\"time\"]        = pd.to_datetime(_evt[\"time\"], utc=True)\n",
    "_evt                = _evt.sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "# ----------------------------- Helper functions -------------------------------\n",
    "def hav_km(lat1, lon1, lat2, lon2) -> float:\n",
    "    R=6371.0\n",
    "    φ1,φ2 = np.radians(lat1), np.radians(lat2)\n",
    "    dφ, dλ = np.radians(lat2-lat1), np.radians(lon2-lon1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return float(2*R*np.arcsin(np.sqrt(a)))\n",
    "\n",
    "def peak_index(arr: np.ndarray, wave_key: str) -> int:\n",
    "    if not isinstance(arr, np.ndarray) or arr.ndim!=1 or arr.size==0:\n",
    "        return -1\n",
    "    return int(np.nanargmax(arr)) if wave_key==\"envelope\" else int(np.nanargmax(np.abs(arr)))\n",
    "\n",
    "def window_features(a: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Cheap, robust features for anomaly work; no SciPy required.\"\"\"\n",
    "    if not isinstance(a, np.ndarray) or a.ndim!=1 or a.size==0:\n",
    "        return dict(\n",
    "            n=0, mean=np.nan, std=np.nan, rms=np.nan, peak=np.nan, crest=np.nan,\n",
    "            p05=np.nan, p50=np.nan, p95=np.nan, zcr=np.nan, noise_rms=np.nan,\n",
    "            snr_db=np.nan\n",
    "        )\n",
    "    a = a.astype(float, copy=False)\n",
    "    n = a.size\n",
    "    mean = float(np.nanmean(a))\n",
    "    std  = float(np.nanstd(a))\n",
    "    rms  = float(np.sqrt(np.nanmean(a*a)))\n",
    "    peak = float(np.nanmax(np.abs(a))) if n else np.nan\n",
    "    crest = float(peak/rms) if rms>0 else np.nan\n",
    "    q05, q50, q95 = [float(np.nanpercentile(np.abs(a), p)) for p in (5,50,95)]\n",
    "    # zero-crossings of demeaned signal\n",
    "    dm = a - mean\n",
    "    zc = float(((dm[:-1]*dm[1:]) < 0).mean()) if n>1 else np.nan\n",
    "    # crude noise estimate: RMS of lowest 20% absolute amplitude\n",
    "    mask = np.abs(a) <= q20 if (q20:=float(np.nanpercentile(np.abs(a),20))) else False\n",
    "    nz = a[mask] if isinstance(mask, np.ndarray) else a\n",
    "    noise_rms = float(np.sqrt(np.nanmean(nz*nz))) if nz.size else np.nan\n",
    "    snr_db = float(20*np.log10(peak/(noise_rms+1e-12))) if (peak>0 and np.isfinite(noise_rms)) else np.nan\n",
    "    return dict(n=n, mean=mean, std=std, rms=rms, peak=peak, crest=crest,\n",
    "                p05=q05, p50=q50, p95=q95, zcr=zc, noise_rms=noise_rms, snr_db=snr_db)\n",
    "\n",
    "def choose_anchor(sel: pd.DataFrame, wave_key: str) -> Tuple[str, pd.DataFrame]:\n",
    "    \"\"\"Score H1/H2/H3; return best anchor name and a DataFrame with residuals.\"\"\"\n",
    "    X = sel.copy()\n",
    "    X[\"dist_km\"]   = [hav_km(a,b,c,d) for a,b,c,d in zip(X[\"fix_lat\"],X[\"fix_lon\"],X[\"station_lat\"],X[\"station_lon\"])]\n",
    "    X[\"pred_s\"]    = X[\"dist_km\"] / C_KM_S\n",
    "    X[\"dt_obs_s\"]  = (X[\"station_time\"] - X[\"fix_time\"]).dt.total_seconds()\n",
    "    X[\"ipk\"]       = X[wave_key].apply(lambda a: peak_index(a, wave_key))\n",
    "    X[\"L\"]         = X[f\"{wave_key}_len\"].astype(int)\n",
    "\n",
    "    X[\"r_H1_s\"] = X[\"dt_obs_s\"] - X[\"pred_s\"]\n",
    "    X[\"r_H2_s\"] = X[\"dt_obs_s\"] + (X[\"ipk\"]/FS) - X[\"pred_s\"]\n",
    "    X[\"r_H3_s\"] = X[\"dt_obs_s\"] + ((X[\"ipk\"] - X[\"L\"]/2)/FS) - X[\"pred_s\"]\n",
    "\n",
    "    def s(d):  # |median| + std in ms\n",
    "        med = np.nanmedian(d)*1e3; sd = np.nanstd(d)*1e3\n",
    "        return abs(med)+sd, med, sd\n",
    "    crit = {k: s(X[f\"r_{k}_s\"].values) for k in (\"H1\",\"H2\",\"H3\")}\n",
    "    best = min(crit, key=lambda k: crit[k][0])\n",
    "    X[\"ANCHOR\"] = best\n",
    "    return best, X\n",
    "\n",
    "def start_time_from_anchor(row, anchor: str, wave_key: str) -> pd.Timestamp:\n",
    "    L = int(row[f\"{wave_key}_len\"]); st = row[\"station_time\"]; ipk = int(row[\"ipk\"])\n",
    "    if anchor==\"H2\":   # station_time = window start\n",
    "        return st\n",
    "    if anchor==\"H3\":   # station_time = window centre\n",
    "        return st - pd.to_timedelta(L/(2*FS), unit=\"s\")\n",
    "    # H1: station_time marks the peak; back off by ipk samples\n",
    "    return st - pd.to_timedelta(ipk/FS, unit=\"s\")\n",
    "\n",
    "# ------------------------------ 1) Inventory ----------------------------------\n",
    "print(\"\\n=== 1) Inventory & basic structure ===\")\n",
    "print(\"df (station windows):\", _df.shape)\n",
    "print(\"df_evt (fixes):      \", _evt.shape)\n",
    "\n",
    "# Availability of each waveform type\n",
    "avail = (_df[[f\"{k}_len\" for k in (\"samples\",\"reconstructed_samples\",\"envelope\",\"raw_samples\")]]\n",
    "         .astype(bool).sum().to_dict())\n",
    "print(\"Wave availability (rows with data):\", {k.replace(\"_len\",\"\"): v for k,v in avail.items()})\n",
    "\n",
    "# Stations per fix\n",
    "spfix = _df.groupby(\"fix_id\").size().describe()\n",
    "print(\"Stations per fix (count per fix):\")\n",
    "print(spfix.to_string())\n",
    "\n",
    "# ------------------------- 2) Anchor inference for EDA_WAVE_KEY ---------------\n",
    "print(\"\\n=== 2) Timing anchor inference (H1/H2/H3) for\", EDA_WAVE_KEY, \"===\")\n",
    "sel = _df.loc[_df[\"station_time\"].notna() & (_df[f\"{EDA_WAVE_KEY}_len\"]>0),\n",
    "              [\"fix_id\",\"fix_time\",\"station_time\",\"fix_lat\",\"fix_lon\",\"station_lat\",\"station_lon\",\n",
    "               \"site_name\",\"node_unique_id\" if \"node_unique_id\" in _df.columns else _df.columns[0],\n",
    "               EDA_WAVE_KEY, f\"{EDA_WAVE_KEY}_len\"]].copy()\n",
    "\n",
    "ANCHOR, anchor_df = choose_anchor(sel, EDA_WAVE_KEY)\n",
    "for key in (\"H1\",\"H2\",\"H3\"):\n",
    "    r = anchor_df[f\"r_{key}_s\"].values\n",
    "    print(f\"  {key}: N={np.isfinite(r).sum():5d} | median={np.nanmedian(r)*1e3:+8.3f} ms | std={np.nanstd(r)*1e3:7.3f} ms | p95={np.nanpercentile(r,95)*1e3:7.3f} ms\")\n",
    "print(\"Chosen ANCHOR:\", ANCHOR)\n",
    "\n",
    "# -------------------------- 3) Construct absolute times -----------------------\n",
    "print(\"\\n=== 3) Absolute sample timing and ordering checks ===\")\n",
    "T = anchor_df.copy()\n",
    "T[\"ipk\"] = T[EDA_WAVE_KEY].apply(lambda a: peak_index(a, EDA_WAVE_KEY))\n",
    "T[\"start_time\"] = T.apply(lambda r: start_time_from_anchor(r, ANCHOR, EDA_WAVE_KEY), axis=1)\n",
    "T[\"end_time\"]   = T[\"start_time\"] + pd.to_timedelta(T[f\"{EDA_WAVE_KEY}_len\"]/FS, unit=\"s\")\n",
    "T[\"dist_km\"]    = [hav_km(a,b,c,d) for a,b,c,d in zip(T[\"fix_lat\"],T[\"fix_lon\"],T[\"station_lat\"],T[\"station_lon\"])]\n",
    "T[\"pred_s\"]     = T[\"dist_km\"]/C_KM_S\n",
    "T[\"abs_peak_time\"] = T[\"start_time\"] + pd.to_timedelta(T[\"ipk\"]/FS, unit=\"s\")\n",
    "T[\"residual_ms\"]   = (T[\"abs_peak_time\"] - (T[\"fix_time\"] + pd.to_timedelta(T[\"pred_s\"], unit=\"s\"))).dt.total_seconds()*1e3\n",
    "\n",
    "# Ordering: per station, start_time must be non-decreasing\n",
    "inv_counts = {}\n",
    "for st, grp in T.sort_values(\"start_time\").groupby(\"site_name\"):\n",
    "    # Count inversions (strict decreases)\n",
    "    arr = grp[\"start_time\"].astype(\"int64\").values\n",
    "    inv = int((np.diff(arr) < 0).sum())\n",
    "    if inv>0:\n",
    "        inv_counts[st] = inv\n",
    "print(\"Stations with out-of-order windows:\", len(inv_counts), \"| examples:\", dict(list(inv_counts.items())[:SHOW_TOP]))\n",
    "print(\"Residuals summary [ms]:\")\n",
    "print(T[\"residual_ms\"].describe(percentiles=[.5,.9,.95,.99]).to_string())\n",
    "\n",
    "# --------------------------- 4) Signal features -------------------------------\n",
    "print(\"\\n=== 4) Signal features for\", EDA_WAVE_KEY, \"===\")\n",
    "feat_rows = []\n",
    "for _, r in T.iterrows():\n",
    "    a = r[EDA_WAVE_KEY]\n",
    "    f = window_features(a)\n",
    "    f.update(dict(\n",
    "        fix_id=int(r[\"fix_id\"]), site_name=r[\"site_name\"],\n",
    "        dist_km=float(r[\"dist_km\"]), win_len=int(r[f\"{EDA_WAVE_KEY}_len\"]),\n",
    "        ipk=int(r[\"ipk\"]), residual_ms=float(r[\"residual_ms\"])\n",
    "    ))\n",
    "    feat_rows.append(f)\n",
    "FEAT = pd.DataFrame(feat_rows)\n",
    "\n",
    "print(\"Feature table shape:\", FEAT.shape)\n",
    "print(\"Feature head:\")\n",
    "print(FEAT.head(SHOW_TOP).to_string(index=False))\n",
    "\n",
    "# ------------------------ 5) Amplitude vs distance (physics) ------------------\n",
    "print(\"\\n=== 5) Physics sanity: amplitude (peak) vs log(distance) ===\")\n",
    "# Use log(distance + d0) stabiliser to avoid 0 and near-field divergence\n",
    "d0 = 100.0\n",
    "valid = FEAT[\"peak\"].gt(0) & FEAT[\"dist_km\"].gt(0)\n",
    "x = np.log(FEAT.loc[valid,\"dist_km\"] + d0)\n",
    "y = np.log(FEAT.loc[valid,\"peak\"])\n",
    "if valid.sum() >= 10:\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    yhat = slope*x + intercept\n",
    "    ss_res = float(np.sum((y - yhat)**2))\n",
    "    ss_tot = float(np.sum((y - float(np.mean(y)))**2))\n",
    "    r2 = 1 - ss_res/ss_tot if ss_tot>0 else np.nan\n",
    "    print(f\"Global log-log fit: peak ≈ A*(dist+d0)^{slope:.3f}  |  R²={r2:.3f}  (d0={d0} km)\")\n",
    "else:\n",
    "    print(\"Not enough valid rows for global fit.\")\n",
    "# Per-station slope (robustness check)\n",
    "stat_summ = []\n",
    "for st, g in FEAT.loc[valid].groupby(\"site_name\"):\n",
    "    if len(g) < 20:\n",
    "        continue\n",
    "    xv = np.log(g[\"dist_km\"] + d0); yv = np.log(g[\"peak\"])\n",
    "    m,b = np.polyfit(xv, yv, 1)\n",
    "    yhat = m*xv + b\n",
    "    ss_res = float(np.sum((yv-yhat)**2)); ss_tot = float(np.sum((yv-float(np.mean(yv)))**2))\n",
    "    r2v = 1 - ss_res/ss_tot if ss_tot>0 else np.nan\n",
    "    stat_summ.append((st, m, r2v, len(g)))\n",
    "stat_summ.sort(key=lambda t: (-t[2], abs(t[1])))\n",
    "print(\"Per-station (top by R²):\", stat_summ[:SHOW_TOP])\n",
    "\n",
    "# ------------------------- 6) Join fix-level QA and correlate -----------------\n",
    "print(\"\\n=== 6) Fix-level QA correlations ===\")\n",
    "# Carry forward a few known QA fields if present\n",
    "qa_cols = [c for c in (\"fix_score\",\"error_rms\",\"error_angle\",\"error_major-axis\",\"error_minor_axis\",\n",
    "                       \"range_check\",\"fix_score_counts\",\"poo_ansr_count\",\"poor_res_count\",\"small_peak_count\")\n",
    "           if c in _evt.columns]\n",
    "EVT = _evt[[\"fix_id\",\"time\",\"lat\",\"lon\"] + qa_cols].copy()\n",
    "\n",
    "# Aggregate FEAT to fix level (mean across stations per fix)\n",
    "agg_funcs = dict(peak=\"median\", rms=\"median\", crest=\"median\", snr_db=\"median\",\n",
    "                 residual_ms=\"median\", win_len=\"median\", dist_km=\"median\")\n",
    "FEAT_EVT = FEAT.groupby(\"fix_id\").agg(agg_funcs).reset_index()\n",
    "\n",
    "QA = pd.merge(EVT, FEAT_EVT, on=\"fix_id\", how=\"left\")\n",
    "num_cols = [c for c in QA.columns if QA[c].dtype.kind in \"fc\" and c not in (\"lat\",\"lon\")]\n",
    "CORR = QA[num_cols].corr(numeric_only=True)\n",
    "print(\"Correlation matrix (numerical cols, trimmed):\")\n",
    "print(CORR.round(3).iloc[:SHOW_TOP, :SHOW_TOP].to_string())\n",
    "\n",
    "# surface a few interesting pairs\n",
    "def top_pairs(corr: pd.DataFrame, k=8):\n",
    "    tri = []\n",
    "    cols = corr.columns.tolist()\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            val = corr.iloc[i,j]\n",
    "            if np.isfinite(val):\n",
    "                tri.append((cols[i], cols[j], float(val)))\n",
    "    tri.sort(key=lambda t: -abs(t[2]))\n",
    "    return tri[:k]\n",
    "print(\"Top correlated pairs (|r|):\", top_pairs(CORR, k=10))\n",
    "\n",
    "# ------------------------- 7) Node & station biases ---------------------------\n",
    "print(\"\\n=== 7) Station and node biases (timing & amplitude) ===\")\n",
    "# Station bias in timing residual\n",
    "ST_TIMING = (FEAT.groupby(\"site_name\")[\"residual_ms\"]\n",
    "                .agg([\"median\",\"std\",\"count\"]).sort_values(\"median\"))\n",
    "print(\"Timing bias by station [ms] (median/std/count):\")\n",
    "print(ST_TIMING.head(SHOW_TOP).to_string())\n",
    "print(ST_TIMING.tail(SHOW_TOP).to_string())\n",
    "\n",
    "# Station bias in amplitude (peak)\n",
    "ST_PEAK = (FEAT.groupby(\"site_name\")[\"peak\"]\n",
    "             .agg([\"median\",\"std\",\"count\"]).sort_values(\"median\", ascending=False))\n",
    "print(\"Peak amplitude by station (median/std/count):\")\n",
    "print(ST_PEAK.head(SHOW_TOP).to_string())\n",
    "\n",
    "# Node-specific (if node IDs present)\n",
    "if \"node_unique_id\" in _df.columns:\n",
    "    NODE = T[[\"site_name\",\"node_unique_id\",\"residual_ms\",EDA_WAVE_KEY]].copy()\n",
    "    NODE[\"peak\"] = NODE[EDA_WAVE_KEY].apply(lambda a: float(np.nanmax(np.abs(a))) if isinstance(a, np.ndarray) and a.size else np.nan)\n",
    "    NODE_GR = NODE.groupby([\"site_name\",\"node_unique_id\"]).agg(res_med=(\"residual_ms\",\"median\"),\n",
    "                                                               res_std=(\"residual_ms\",\"std\"),\n",
    "                                                               peak_med=(\"peak\",\"median\"),\n",
    "                                                               n=(\"peak\",\"count\")).reset_index()\n",
    "    NODE_GR = NODE_GR.sort_values([\"site_name\",\"n\"], ascending=[True, False])\n",
    "    print(\"Node-level summary (per site):\")\n",
    "    print(NODE_GR.head(SHOW_TOP).to_string(index=False))\n",
    "\n",
    "# ---------------------------- 8) Outlier surfacing ----------------------------\n",
    "print(\"\\n=== 8) Outliers (simple robust rules) ===\")\n",
    "OUT = FEAT.copy()\n",
    "# Robust z-scores using MAD\n",
    "def zscore_robust(x: pd.Series) -> pd.Series:\n",
    "    med = np.nanmedian(x.values)\n",
    "    mad = np.nanmedian(np.abs(x.values - med)) + 1e-12\n",
    "    return 0.6745*(x - med)/mad\n",
    "OUT[\"z_resid\"] = zscore_robust(OUT[\"residual_ms\"])\n",
    "OUT[\"z_crest\"] = zscore_robust(OUT[\"crest\"])\n",
    "OUT[\"z_peak\"]  = zscore_robust(OUT[\"peak\"])\n",
    "# Combined score\n",
    "OUT[\"anom_score\"] = OUT[[\"z_resid\",\"z_crest\",\"z_peak\"]].abs().max(axis=1)\n",
    "\n",
    "print(\"Top potential anomalies (by max robust z-score):\")\n",
    "cols_show = [\"fix_id\",\"site_name\",\"dist_km\",\"win_len\",\"ipk\",\"residual_ms\",\"peak\",\"crest\",\"anom_score\"]\n",
    "print(OUT.sort_values(\"anom_score\", ascending=False).head(SHOW_TOP)[cols_show].to_string(index=False))\n",
    "\n",
    "# ----------------------- 9) Missingness & consistency checks ------------------\n",
    "print(\"\\n=== 9) Missingness & consistency ===\")\n",
    "# Missingness in df, df_evt\n",
    "miss_df = _df.isna().mean().sort_values(ascending=False)\n",
    "miss_evt = _evt.isna().mean().sort_values(ascending=False)\n",
    "print(\"Missingness in df (top):\")\n",
    "print((miss_df.head(SHOW_TOP)*100).round(2).astype(str) + \"%\")\n",
    "print(\"Missingness in df_evt (top):\")\n",
    "print((miss_evt.head(SHOW_TOP)*100).round(2).astype(str) + \"%\")\n",
    "\n",
    "# Unique counts for key fields\n",
    "print(\"Unique counts:\")\n",
    "print({k:int(_df[k].nunique()) for k in [\"fix_id\",\"site_name\"] if k in _df.columns})\n",
    "\n",
    "# Consistency: each (fix × site) should appear once\n",
    "dup = _df.groupby([\"fix_id\",\"site_name\"]).size().reset_index(name=\"n\").query(\"n>1\")\n",
    "print(\"Duplicate (fix_id, site_name) pairs:\", len(dup))\n",
    "if len(dup):\n",
    "    print(dup.head(SHOW_TOP).to_string(index=False))\n",
    "\n",
    "# --------------------------- 10) Outputs for later ML -------------------------\n",
    "# EDA tables you can keep for downstream analysis\n",
    "EDA_ANCHOR         = ANCHOR\n",
    "EDA_TIMING_TABLE   = T[[\"fix_id\",\"site_name\",\"start_time\",\"end_time\",\"dist_km\",\"ipk\",\"residual_ms\"]].copy()\n",
    "EDA_FEATURE_TABLE  = FEAT.copy()\n",
    "EDA_FIX_TABLE      = QA.copy() if 'QA' in globals() else None   # may be None if QA cols absent\n",
    "EDA_STATION_BIAS   = ST_TIMING.reset_index().rename(columns={\"median\":\"timing_med_ms\",\"std\":\"timing_std_ms\"})\n",
    "EDA_PEAK_BIAS      = ST_PEAK.reset_index().rename(columns={\"median\":\"peak_med\",\"std\":\"peak_std\"})\n",
    "\n",
    "print(\"\\n=== 10) Summary of EDA artefacts placed in your workspace ===\")\n",
    "print(\"  • EDA_ANCHOR        :\", EDA_ANCHOR)\n",
    "print(\"  • EDA_TIMING_TABLE  :\", EDA_TIMING_TABLE.shape, \"(per station-window with timing residuals)\")\n",
    "print(\"  • EDA_FEATURE_TABLE :\", EDA_FEATURE_TABLE.shape, \"(per station-window features)\")\n",
    "print(\"  • EDA_FIX_TABLE     :\", None if EDA_FIX_TABLE is None else EDA_FIX_TABLE.shape, \"(fix-level joined QA)\")\n",
    "print(\"  • EDA_STATION_BIAS  :\", EDA_STATION_BIAS.shape, \"(per-station timing bias)\")\n",
    "print(\"  • EDA_PEAK_BIAS     :\", EDA_PEAK_BIAS.shape, \"(per-station amplitude bias)\")\n",
    "\n",
    "# ---------------------------- Optional: plots ---------------------------------\n",
    "# If you're in a notebook and want quick visuals, uncomment the lines below.\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Residual histogram\n",
    "# plt.figure(); T[\"residual_ms\"].plot.hist(bins=100); plt.title(\"Timing residuals [ms]\"); plt.xlabel(\"ms\"); plt.show()\n",
    "# # Peak vs distance (log-log)\n",
    "# plt.figure(); plt.scatter(np.log(FEAT[\"dist_km\"]+d0), np.log(FEAT[\"peak\"]+1e-12), s=4, alpha=0.3)\n",
    "# plt.title(\"log(peak) vs log(distance + d0)\"); plt.xlabel(\"log(dist+d0)\"); plt.ylabel(\"log(peak)\"); plt.show()\n",
    "# # Crest factor distribution\n",
    "# plt.figure(); FEAT[\"crest\"].plot.hist(bins=60); plt.title(\"Crest factor\"); plt.xlabel(\"Peak/RMS\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL 1: strict, simulator-shaped bundle for real data ====\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List\n",
    "import numpy as np, pandas as pd, math\n",
    "\n",
    "# --- Eval params (your values) ---\n",
    "@dataclass(frozen=True)\n",
    "class EvalCfg:\n",
    "    fs: int = 109_375\n",
    "    win: int = 1024\n",
    "    hop: int = 512\n",
    "    min_stn: int = 4\n",
    "    tol_win: int = 0\n",
    "    burst_len: int = 0  # default: 0.04*fs\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.burst_len <= 0:\n",
    "            object.__setattr__(self, \"burst_len\", int(round(0.04 * self.fs)))\n",
    "        if self.hop <= 0 or self.win <= 0 or self.fs <= 0:\n",
    "            raise ValueError(\"win, hop, fs must be positive\")\n",
    "        if self.hop > self.win:\n",
    "            raise ValueError(\"hop cannot exceed win\")\n",
    "\n",
    "    @property\n",
    "    def bins(self) -> int:  # rFFT bins for n_fft=win\n",
    "        return self.win // 2 + 1\n",
    "\n",
    "# --- Return bundle (API parity with simulator) ---\n",
    "@dataclass\n",
    "class StormBundle:\n",
    "    quantised: Dict[str, np.ndarray]   # station -> int16 waveform (length N)\n",
    "    events: List[dict]                 # one dict per flash\n",
    "    stroke_records: List[dict]         # one per (station × stroke)\n",
    "    df_wave: pd.DataFrame              # ['time_s', STN1, STN2, ...]\n",
    "    df_labels: pd.DataFrame            # tidy labels\n",
    "    availability: dict                 # report (no guessing here, just what we used)\n",
    "\n",
    "# --- Geo + propagation ---\n",
    "def _hav_km(lat1, lon1, lat2, lon2) -> float:\n",
    "    R=6371.0\n",
    "    φ1, φ2 = math.radians(lat1), math.radians(lat2)\n",
    "    dφ, dλ = math.radians(lat2-lat1), math.radians(lon2-lon1)\n",
    "    a = math.sin(dφ/2)**2 + math.cos(φ1)*math.cos(φ2)*math.sin(dλ/2)**2\n",
    "    return 2*R*math.asin(math.sqrt(a))\n",
    "\n",
    "C = 299_792_458.0  # m/s\n",
    "\n",
    "def _idx_from_abs(t_abs: float, epoch_s: float, prop_s: float, fs: int) -> int:\n",
    "    return int(round((t_abs - epoch_s + prop_s) * fs))\n",
    "\n",
    "# --- Builder (strict: uses df & df_evt exactly; minimal tolerances) ---\n",
    "def build_bundle_from_df(\n",
    "    cfg: EvalCfg,\n",
    "    df: pd.DataFrame,\n",
    "    df_evt: pd.DataFrame,\n",
    "    station_latlon: Dict[str, tuple[float,float]],\n",
    "    *,\n",
    "    time_col: str = \"time_s\",\n",
    "    station_cols: list[str] | None = None,\n",
    ") -> StormBundle:\n",
    "    # 0) Validate time column\n",
    "    if time_col not in df.columns:\n",
    "        raise KeyError(f\"df is missing required time column '{time_col}'\")\n",
    "    time = pd.to_numeric(df[time_col], errors=\"coerce\")\n",
    "    if not np.isfinite(time).all():\n",
    "        raise ValueError(\"df[time_s] must be numeric absolute UNIX seconds (no NaNs/inf).\")\n",
    "    # sample-check (we do not resample; just sanity)\n",
    "    # 1/fs ≈ 9.145e-06 s; small jitter is fine\n",
    "    N = len(time)\n",
    "    if N < cfg.win:\n",
    "        raise ValueError(f\"Not enough samples: N={N} < win={cfg.win}\")\n",
    "\n",
    "    # 1) Pick station columns\n",
    "    if station_cols is None:\n",
    "        # default: all columns except time\n",
    "        station_cols = [c for c in df.columns if c != time_col]\n",
    "    if not station_cols:\n",
    "        raise ValueError(\"No station columns provided/found in df.\")\n",
    "\n",
    "    # 2) Waveforms (int16)\n",
    "    waves: Dict[str, np.ndarray] = {}\n",
    "    for stn in station_cols:\n",
    "        col = df[stn]\n",
    "        # accept numeric only\n",
    "        col = pd.to_numeric(col, errors=\"coerce\").to_numpy(dtype=np.float64, copy=False)\n",
    "        if not np.isfinite(col).any():\n",
    "            raise ValueError(f\"Station {stn}: column is not numeric.\")\n",
    "        col = np.nan_to_num(col, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        # assume already ADC-like if |x|>1.5; else scale [-1,1] → int16\n",
    "        xmax = float(np.max(np.abs(col))) if col.size else 0.0\n",
    "        if xmax <= 1.5:\n",
    "            full = 2**15 - 1\n",
    "            xi = np.clip(np.round(col * full), -full, full).astype(np.int16, copy=False)\n",
    "        else:\n",
    "            xi = np.clip(col, -32768, 32767).astype(np.int16, copy=False)\n",
    "        waves[stn] = xi\n",
    "\n",
    "    # 3) Station metadata (lat/lon) — must be provided for the stations we use\n",
    "    missing_latlon = [s for s in station_cols if s not in station_latlon]\n",
    "    if missing_latlon:\n",
    "        raise KeyError(f\"station_latlon missing entries for: {missing_latlon}\")\n",
    "    # epoch_s for index→UNIX mapping: we take epoch at first sample\n",
    "    epoch_s = float(time.iloc[0])\n",
    "\n",
    "    # 4) Events (df_evt strict columns; flash_type optional)\n",
    "    required_evt = {\"event_id\",\"stroke_i\",\"time_s\",\"lat\",\"lon\"}\n",
    "    if not required_evt.issubset(df_evt.columns):\n",
    "        missing = sorted(required_evt - set(df_evt.columns))\n",
    "        raise KeyError(f\"df_evt missing required columns: {missing}\")\n",
    "    evt = df_evt.copy()\n",
    "    evt[\"event_id\"] = pd.to_numeric(evt[\"event_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    evt[\"stroke_i\"] = pd.to_numeric(evt[\"stroke_i\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    evt[\"time_s\"]   = pd.to_numeric(evt[\"time_s\"],   errors=\"coerce\").astype(float)\n",
    "    evt[\"lat\"]      = pd.to_numeric(evt[\"lat\"],      errors=\"coerce\").astype(float)\n",
    "    evt[\"lon\"]      = pd.to_numeric(evt[\"lon\"],      errors=\"coerce\").astype(float)\n",
    "    if \"flash_type\" not in evt.columns:\n",
    "        evt[\"flash_type\"] = \"UNK\"\n",
    "    evt[\"flash_type\"] = evt[\"flash_type\"].astype(str)\n",
    "\n",
    "    # drop rows we cannot possibly align (no IDs or no time)\n",
    "    evt = evt.dropna(subset=[\"event_id\",\"stroke_i\",\"time_s\"]).copy()\n",
    "    evt[\"event_id\"] = evt[\"event_id\"].astype(int)\n",
    "    evt[\"stroke_i\"] = evt[\"stroke_i\"].astype(int)\n",
    "    evt = evt.sort_values([\"event_id\",\"stroke_i\",\"time_s\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # 5) Build events list\n",
    "    events: List[dict] = []\n",
    "    for eid, g in evt.groupby(\"event_id\", sort=True):\n",
    "        events.append(dict(\n",
    "            id=int(eid),\n",
    "            flash_type=str(g[\"flash_type\"].iloc[0]),\n",
    "            lat=float(g[\"lat\"].iloc[0]),\n",
    "            lon=float(g[\"lon\"].iloc[0]),\n",
    "            stroke_times=[float(t) for t in g[\"time_s\"].tolist()],\n",
    "        ))\n",
    "\n",
    "    # 6) Per-station alignment (propagation from (lat,lon) to station)\n",
    "    stroke_records: List[dict] = []\n",
    "    for _, r in evt.iterrows():\n",
    "        eid, si = int(r[\"event_id\"]), int(r[\"stroke_i\"])\n",
    "        t_abs   = float(r[\"time_s\"])\n",
    "        f_lat, f_lon = float(r[\"lat\"]), float(r[\"lon\"])\n",
    "        ftype = str(r[\"flash_type\"])\n",
    "        for stn in station_cols:\n",
    "            s_lat, s_lon = station_latlon[stn]\n",
    "            dist_km = _hav_km(f_lat, f_lon, s_lat, s_lon)\n",
    "            prop_s  = (dist_km*1000.0) / C\n",
    "            idx     = _idx_from_abs(t_abs, epoch_s, prop_s, cfg.fs)\n",
    "            if 0 <= idx < N:\n",
    "                stroke_records.append(dict(\n",
    "                    event_id=eid, stroke_i=si, station=stn, flash_type=ftype,\n",
    "                    lat=f_lat, lon=f_lon, true_time_s=t_abs,\n",
    "                    sample_idx=idx, window_idx=(idx // cfg.win), aligned=True\n",
    "                ))\n",
    "            else:\n",
    "                # out-of-range for this station; still record (unaligned)\n",
    "                stroke_records.append(dict(\n",
    "                    event_id=eid, stroke_i=si, station=stn, flash_type=ftype,\n",
    "                    lat=f_lat, lon=f_lon, true_time_s=t_abs,\n",
    "                    sample_idx=np.nan, window_idx=np.nan, aligned=False\n",
    "                ))\n",
    "\n",
    "    # 7) Assemble dataframes\n",
    "    df_wave = pd.DataFrame({\"time_s\": time})\n",
    "    for stn in station_cols:\n",
    "        df_wave[stn] = waves[stn]\n",
    "    df_labels = pd.DataFrame(stroke_records).sort_values([\"event_id\",\"stroke_i\",\"station\"])\n",
    "\n",
    "    # 8) Enforce min_stn coverage among aligned rows only (optional but consistent)\n",
    "    if cfg.min_stn > 1 and not df_labels.empty:\n",
    "        aligned = df_labels[df_labels[\"aligned\"] == True]\n",
    "        if not aligned.empty:\n",
    "            cnt = aligned.groupby([\"event_id\",\"stroke_i\"])[\"station\"].nunique().reset_index(name=\"n\")\n",
    "            keep = cnt.loc[cnt[\"n\"] >= cfg.min_stn, [\"event_id\",\"stroke_i\"]]\n",
    "            df_labels = df_labels.merge(keep, on=[\"event_id\",\"stroke_i\"], how=\"inner\")\n",
    "\n",
    "    availability = dict(\n",
    "        cfg=asdict(cfg),\n",
    "        station_cols=station_cols,\n",
    "        N=len(time),\n",
    "        fs=cfg.fs,\n",
    "        evt_rows=len(evt),\n",
    "        required_evt_fields_ok=True,\n",
    "    )\n",
    "\n",
    "    print(f\"RealData fs={cfg.fs} Hz win={cfg.win} hop={cfg.hop} bins={cfg.bins}\")\n",
    "    print(f\"Stations {len(station_cols)} | events {len(events)} | samples {len(time):,}\")\n",
    "    return StormBundle(\n",
    "        quantised={stn: waves[stn] for stn in station_cols},\n",
    "        events=events,\n",
    "        stroke_records=stroke_records,\n",
    "        df_wave=df_wave,\n",
    "        df_labels=df_labels,\n",
    "        availability=availability\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CELL 2 (replacement): build bundle from df / df_evt with 'envelope' / 'raw_samples' / 'fix-id' ====\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Your eval params (adjust if needed)\n",
    "cfg = EvalCfg(fs=109_375, win=1024, hop=512, min_stn=4, tol_win=0)\n",
    "\n",
    "# ----------------------\n",
    "# 1) Build station_waveforms from df\n",
    "# ----------------------\n",
    "station_waveforms: dict[str, np.ndarray] = {}\n",
    "station_meta: dict[str, dict] = {}\n",
    "\n",
    "# A) LONG form: one row per station with 'station' + ('envelope' or 'raw_samples') array\n",
    "if 'station' in df.columns and (('envelope' in df.columns) or ('raw_samples' in df.columns)):\n",
    "    val_col = 'envelope' if 'envelope' in df.columns else 'raw_samples'\n",
    "    for _, row in df.iterrows():\n",
    "        stn = str(row['station']).strip()\n",
    "        arr = row[val_col]\n",
    "        # accept list/ndarray; flatten to 1-D\n",
    "        x = np.asarray(arr).reshape(-1)\n",
    "        # ensure int16 codes (assume normalised floats if max<=1.5)\n",
    "        xmax = float(np.max(np.abs(x))) if x.size else 0.0\n",
    "        if x.dtype != np.int16:\n",
    "            if xmax <= 1.5:\n",
    "                full = 2**15 - 1\n",
    "                x = np.clip(np.round(x * full), -full, full).astype(np.int16)\n",
    "            else:\n",
    "                x = np.clip(x, -32768, 32767).astype(np.int16)\n",
    "        station_waveforms[stn] = x\n",
    "        # epoch: if df has 'time_s' vector column per row, use first value; else 0.0\n",
    "        if 'time_s' in df.columns and isinstance(row['time_s'], (np.ndarray, list)):\n",
    "            ts = np.asarray(row['time_s'])\n",
    "            epoch = float(ts[0]) if ts.size else 0.0\n",
    "        else:\n",
    "            epoch = 0.0\n",
    "        station_meta[stn] = dict(lat=np.nan, lon=np.nan, epoch_s=epoch)\n",
    "\n",
    "# B) WIDE form: columns like 'envelope_<STN>' or '<STN>_envelope' or all-numeric columns\n",
    "if not station_waveforms:\n",
    "    # Detect envelope columns with regex\n",
    "    pat1 = re.compile(r'^envelope[_\\-](?P<stn>[A-Za-z0-9]+)$', re.I)\n",
    "    pat2 = re.compile(r'^(?P<stn>[A-Za-z0-9]+)[_\\-]envelope$', re.I)\n",
    "\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        m = pat1.match(str(c)) or pat2.match(str(c))\n",
    "        if m:\n",
    "            candidates.append((m.group('stn'), c))\n",
    "\n",
    "    if candidates:\n",
    "        # Use detected envelope columns\n",
    "        for stn, col in candidates:\n",
    "            x = pd.to_numeric(df[col], errors='coerce').to_numpy(dtype=np.float64)\n",
    "            x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            xmax = float(np.max(np.abs(x))) if x.size else 0.0\n",
    "            if xmax <= 1.5:\n",
    "                full = 2**15 - 1\n",
    "                xi = np.clip(np.round(x * full), -full, full).astype(np.int16)\n",
    "            else:\n",
    "                xi = np.clip(x, -32768, 32767).astype(np.int16)\n",
    "            station_waveforms[stn] = xi\n",
    "    else:\n",
    "        # Fallback: take numeric columns as stations, excluding obvious meta\n",
    "        meta_like = {'time_s','time','timestamp','datetime','fix-id','fix_id','event_id','stroke_i','lat','lon','envelope','raw_samples','station'}\n",
    "        num_df = df.drop(columns=[c for c in df.columns if str(c).lower() in meta_like], errors='ignore') \\\n",
    "                   .select_dtypes(include=[np.number])\n",
    "        for stn in num_df.columns:\n",
    "            x = num_df[stn].to_numpy(dtype=np.float64)\n",
    "            x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            xmax = float(np.max(np.abs(x))) if x.size else 0.0\n",
    "            if xmax <= 1.5:\n",
    "                full = 2**15 - 1\n",
    "                xi = np.clip(np.round(x * full), -full, full).astype(np.int16)\n",
    "            else:\n",
    "                xi = np.clip(x, -32768, 32767).astype(np.int16)\n",
    "            station_waveforms[str(stn)] = xi\n",
    "\n",
    "    # epoch_s from a global time vector column if present\n",
    "    epoch = 0.0\n",
    "    for tc in ['time_s','time','timestamp','datetime']:\n",
    "        if tc in df.columns:\n",
    "            s = pd.to_numeric(df[tc], errors='coerce')\n",
    "            if s.notna().any():\n",
    "                epoch = float(s.dropna().iloc[0]); break\n",
    "    for stn in station_waveforms:\n",
    "        station_meta[stn] = dict(lat=np.nan, lon=np.nan, epoch_s=epoch)\n",
    "\n",
    "if not station_waveforms:\n",
    "    raise ValueError(\"Could not extract any station waveforms from df. \"\n",
    "                     \"Expected LONG form (station + envelope/raw_samples) or WIDE form (envelope_<STN> columns or numeric per-station columns).\")\n",
    "\n",
    "# ----------------------\n",
    "# 2) Normalise df_evt to expected names\n",
    "# ----------------------\n",
    "evt = df_evt.copy()\n",
    "evt.columns = [str(c).strip().lower() for c in evt.columns]\n",
    "rename = {\n",
    "    'fix-id':'event_id', 'fix_id':'event_id', 'fixid':'event_id', 'eid':'event_id',\n",
    "    'stroke':'stroke_i', 'strokeidx':'stroke_i',\n",
    "    'fix_time':'time_s', 'timestamp':'time_s', 'datetime':'time_s',\n",
    "    'fix_lat':'lat', 'latitude':'lat',\n",
    "    'fix_lon':'lon', 'longitude':'lon',\n",
    "    'type':'flash_type'\n",
    "}\n",
    "evt = evt.rename(columns={k:v for k,v in rename.items() if k in evt.columns})\n",
    "\n",
    "# Keep only the columns we consume; fill missing with NaN / defaults\n",
    "for c, default in [('event_id', np.nan), ('stroke_i', np.nan), ('time_s', np.nan), ('lat', np.nan), ('lon', np.nan), ('flash_type', 'UNK')]:\n",
    "    if c not in evt.columns:\n",
    "        evt[c] = default\n",
    "\n",
    "# Coerce types and make NaN-safe IDs\n",
    "evt['event_id'] = pd.to_numeric(evt['event_id'], errors='coerce')\n",
    "evt['stroke_i'] = pd.to_numeric(evt['stroke_i'], errors='coerce')\n",
    "evt['time_s']   = pd.to_numeric(evt['time_s'],   errors='coerce')\n",
    "evt['lat']      = pd.to_numeric(evt['lat'],      errors='coerce')\n",
    "evt['lon']      = pd.to_numeric(evt['lon'],      errors='coerce')\n",
    "evt['flash_type'] = evt['flash_type'].astype('string')\n",
    "\n",
    "# Fill missing event_id sequentially; stroke_i per event\n",
    "if evt['event_id'].isna().any():\n",
    "    start = 0\n",
    "    evt.loc[evt['event_id'].isna(), 'event_id'] = np.arange(start, start + evt['event_id'].isna().sum())\n",
    "evt['event_id'] = evt['event_id'].astype(int)\n",
    "\n",
    "if evt['stroke_i'].isna().any():\n",
    "    evt = evt.sort_values(['event_id','time_s'], kind='mergesort').reset_index(drop=True)\n",
    "    evt['stroke_i'] = evt.groupby('event_id').cumcount()\n",
    "evt['stroke_i'] = evt['stroke_i'].astype(int)\n",
    "\n",
    "evt = evt[['event_id','stroke_i','flash_type','time_s','lat','lon']]\n",
    "\n",
    "# ----------------------\n",
    "# 3) Build the bundle using Cell 1’s builder\n",
    "# ----------------------\n",
    "bundle = build_real_bundle_flexible(\n",
    "    cfg,\n",
    "    station_waveforms=station_waveforms,\n",
    "    station_meta=station_meta,\n",
    "    fixes=evt,\n",
    "    require_int16=True,\n",
    "    clip_to_bounds=True,\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 4) Quick sanity\n",
    "# ----------------------\n",
    "print(f\"Stations detected: {list(station_waveforms.keys())}\")\n",
    "print(f\"Samples per station: {{k: len(v) for k,v in station_waveforms.items()}}\")\n",
    "display(bundle.df_wave.head(3))\n",
    "display(bundle.df_labels.head(10))\n",
    "print(\"Aligned rows:\", int((bundle.df_labels['aligned'] == True).sum()))\n",
    "print(\"513 rFFT bins (win=1024):\", cfg.bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Lightning data forensics: fix vs station times, anchor inference (H1/H2/H3),\n",
    "# timeline reconstruction, and (optional) StormBundle export.\n",
    "# Works directly from your `data` dict; does not require df/df_evt.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np, pandas as pd, math\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "# --------------------------- Configuration -----------------------------------\n",
    "FS         = 109_375          # known sampling rate (Hz) — adjust if different\n",
    "C_KM_S     = 300_000.0        # propagation speed (km/s) approximation\n",
    "WAVE_KEY   = \"samples\"        # choose \"samples\" | \"reconstructed_samples\" | \"envelope\" | \"raw_samples\"\n",
    "USE_NORMALISE_0p8_VREF = True # when exporting, normalise each window to ±0.8 for safety\n",
    "BITS       = 14               # simulator quantiser depth for export\n",
    "VREF       = 1.0              # simulator full-scale volts for export\n",
    "PRE_PAD_S  = 0.5              # padding before earliest start when exporting\n",
    "POST_PAD_S = 1.0              # padding after latest end when exporting\n",
    "\n",
    "# Station-code mapping to keep continuity with sim (extend as needed)\n",
    "SITE_TO_CODE = {\n",
    "    \"Valentia\":\"VAL\", \"Herstmonceux\":\"HER\", \"Hersrmonceux\":\"HER\",\n",
    "    \"Lerwick\":\"LER\", \"Keflavik\":\"KEF\", \"Gibraltar\":\"GIB\", \"Akrotiri\":\"AKR\",\n",
    "    \"Camborne\":\"CAM\", \"Wattisham\":\"WAT\", \"Cabauw\":\"CAB\", \"Payerne\":\"PAY\",\n",
    "    \"Tartu\":\"TAR\",\n",
    "}\n",
    "\n",
    "# --------------------------- Helpers -----------------------------------------\n",
    "def hav_km(lat1, lon1, lat2, lon2) -> float:\n",
    "    R=6371.0\n",
    "    φ1,φ2 = np.radians(lat1), np.radians(lat2)\n",
    "    dφ, dλ = np.radians(lat2-lat1), np.radians(lon2-lon1)\n",
    "    a = np.sin(dφ/2)**2 + np.cos(φ1)*np.cos(φ2)*np.sin(dλ/2)**2\n",
    "    return float(2*R*np.arcsin(np.sqrt(a)))\n",
    "\n",
    "def site_code(name: Any) -> str:\n",
    "    s = str(name)\n",
    "    if s in SITE_TO_CODE: return SITE_TO_CODE[s]\n",
    "    al = \"\".join(ch for ch in s if ch.isalpha()).upper()\n",
    "    return (al[:3] or \"UNK\")\n",
    "\n",
    "def peak_index(arr: np.ndarray, wave_key: str) -> int:\n",
    "    if not isinstance(arr, np.ndarray) or arr.ndim!=1 or arr.size==0: return -1\n",
    "    return int(np.nanargmax(arr)) if wave_key==\"envelope\" else int(np.nanargmax(np.abs(arr)))\n",
    "\n",
    "def as_utc(x) -> pd.Timestamp:\n",
    "    return pd.to_datetime(x, utc=True)\n",
    "\n",
    "def fullscale(bits: int) -> int:\n",
    "    return 2**(bits-1) - 1\n",
    "\n",
    "def detect_bits_int(arr: np.ndarray) -> int:\n",
    "    vmax = int(np.max(np.abs(arr))) if arr.size else 0\n",
    "    for b in (16,15,14,13,12):\n",
    "        if vmax <= (2**(b-1)-1): return b\n",
    "    return int(arr.dtype.itemsize*8)\n",
    "\n",
    "# --------------------------- 0) Inspect a single fix --------------------------\n",
    "def inspect_fix(data: Dict, k: Any = 0) -> pd.DataFrame:\n",
    "    \"\"\"Forensic printout for data[k] and a per-station timing table.\"\"\"\n",
    "    rec = data[k]\n",
    "    fx_t  = as_utc(rec[\"time\"]); fx_lat, fx_lon = float(rec[\"latitude\"]), float(rec[\"longitude\"])\n",
    "    grp = rec[\"group\"] or []\n",
    "\n",
    "    print(f\"\\n=== Inspect fix {k} ===\")\n",
    "    print(\"Fix time (UTC) :\", fx_t)\n",
    "    print(\"Fix lat/lon    :\", fx_lat, fx_lon)\n",
    "\n",
    "    # Print known fix-level QA fields if present\n",
    "    qa_keys = [k for k in (\"atds\",\"node\",\"atf\",\"asnr\",\"fix_qc\",\"residual\",\"fix_score\",\n",
    "                           \"error_rms\",\"error_angle\",\"error_major-axis\",\"error_minor_axis\",\n",
    "                           \"range_check\",\"fix_score_counts\",\"poo_ansr_count\",\"poor_res_count\",\"small_peak_count\")\n",
    "               if k in rec]\n",
    "    if qa_keys:\n",
    "        print(\"Fix-level QA keys present:\", qa_keys)\n",
    "\n",
    "    # Build a small per-station table\n",
    "    rows = []\n",
    "    for gi, g in enumerate(grp):\n",
    "        st = as_utc(g.get(\"time\")) if g.get(\"time\") is not None else pd.NaT\n",
    "        sname = g[\"site_name\"]; code = site_code(sname)\n",
    "        slat, slon = float(g[\"lat\"]), float(g[\"lon\"])\n",
    "        waves = {wk: g.get(wk) for wk in (\"samples\",\"reconstructed_samples\",\"envelope\",\"raw_samples\")}\n",
    "        lengths = {wk: (len(waves[wk]) if isinstance(waves[wk], np.ndarray) else 0) for wk in waves}\n",
    "        a = waves.get(WAVE_KEY)\n",
    "        ipk = peak_index(a, WAVE_KEY) if lengths.get(WAVE_KEY,0)>0 else -1\n",
    "        dist = hav_km(fx_lat, fx_lon, slat, slon)\n",
    "        pred = fx_t + pd.to_timedelta(dist/C_KM_S, unit=\"s\")  # predicted arrival (physics)\n",
    "\n",
    "        rows.append(dict(\n",
    "            station_index=gi, site_name=sname, station_code=code,\n",
    "            station_time=st, fix_time=fx_t, pred_arrival=pred,\n",
    "            dist_km=dist,\n",
    "            wave_key=WAVE_KEY, wave_len=lengths.get(WAVE_KEY,0),\n",
    "            peak_index=ipk\n",
    "        ))\n",
    "\n",
    "    T = pd.DataFrame(rows)\n",
    "    # Compute residuals for the three anchors at the PEAK sample time\n",
    "    if not T.empty:\n",
    "        # Δt observed between station and fix\n",
    "        T[\"dt_obs_s\"] = (T[\"station_time\"] - T[\"fix_time\"]).dt.total_seconds()\n",
    "        # Residuals (seconds) for peak time under each hypothesis\n",
    "        T[\"r_H1_s\"] = T[\"dt_obs_s\"] - (T[\"pred_arrival\"] - T[\"fix_time\"]).dt.total_seconds()          # station_time marks peak\n",
    "        T[\"r_H2_s\"] = T[\"dt_obs_s\"] + (T[\"peak_index\"]/FS) - (T[\"pred_arrival\"] - T[\"fix_time\"]).dt.total_seconds()  # station_time is start\n",
    "        T[\"r_H3_s\"] = T[\"dt_obs_s\"] + ((T[\"peak_index\"] - T[\"wave_len\"]/2)/FS) - (T[\"pred_arrival\"] - T[\"fix_time\"]).dt.total_seconds()  # centre\n",
    "\n",
    "        # Pretty print\n",
    "        view = T[[\"station_index\",\"site_name\",\"station_code\",\"dist_km\",\"wave_len\",\"peak_index\",\n",
    "                  \"station_time\",\"pred_arrival\",\"r_H1_s\",\"r_H2_s\",\"r_H3_s\"]].copy()\n",
    "        pd.set_option(\"display.width\", 160); pd.set_option(\"display.max_columns\", None)\n",
    "        print(\"\\nPer-station timing (one fix):\")\n",
    "        print(view.to_string(index=False))\n",
    "        print(\"\\nInterpretation tips:\")\n",
    "        print(\"  • If r_H1_s ~ 0, station_time is the peak/arrival time at that station.\")\n",
    "        print(\"  • If r_H2_s ~ 0, station_time is the window start (peak occurs later).\")\n",
    "        print(\"  • If r_H3_s ~ 0, station_time is the window centre.\")\n",
    "    else:\n",
    "        print(\"No stations in this fix.\")\n",
    "\n",
    "    return T\n",
    "\n",
    "# --------------------------- 1) Dataset-wide anchor inference -----------------\n",
    "def infer_anchor_and_reconstruct(data: Dict, wave_key: str = WAVE_KEY) -> pd.DataFrame:\n",
    "    \"\"\"Infer anchor across dataset; reconstruct start/end/peak absolute times.\"\"\"\n",
    "    rows = []\n",
    "    for k, rec in data.items():\n",
    "        fx_t  = as_utc(rec[\"time\"]); fx_lat, fx_lon = float(rec[\"latitude\"]), float(rec[\"longitude\"])\n",
    "        for g in (rec.get(\"group\") or []):\n",
    "            st = g.get(\"time\")\n",
    "            if st is None: continue\n",
    "            st = as_utc(st)\n",
    "            sname = g[\"site_name\"]; code = site_code(sname)\n",
    "            slat, slon = float(g[\"lat\"]), float(g[\"lon\"])\n",
    "            a = g.get(wave_key)\n",
    "            if not isinstance(a, np.ndarray) or a.size == 0: continue\n",
    "            L = int(a.shape[0]); ipk = peak_index(a, wave_key)\n",
    "\n",
    "            dist = hav_km(fx_lat, fx_lon, slat, slon)\n",
    "            pred = fx_t + pd.to_timedelta(dist/C_KM_S, unit=\"s\")\n",
    "            dt_obs_s = (st - fx_t).total_seconds()\n",
    "            pred_off_s = (pred - fx_t).total_seconds()\n",
    "\n",
    "            # Residuals at the PEAK time under the three hypotheses\n",
    "            rH1 = dt_obs_s - pred_off_s                      # station_time = peak\n",
    "            rH2 = dt_obs_s + (ipk/FS) - pred_off_s           # station_time = start\n",
    "            rH3 = dt_obs_s + ((ipk - L/2)/FS) - pred_off_s   # station_time = centre\n",
    "\n",
    "            rows.append(dict(\n",
    "                fix_id=int(k), site_name=sname, station_code=code,\n",
    "                fix_time=fx_t, station_time=st,\n",
    "                fix_lat=fx_lat, fix_lon=fx_lon, station_lat=slat, station_lon=slon,\n",
    "                dist_km=dist, wave_len=L, ipk=ipk,\n",
    "                r_H1_s=rH1, r_H2_s=rH2, r_H3_s=rH3\n",
    "            ))\n",
    "\n",
    "    X = pd.DataFrame(rows)\n",
    "    if X.empty:\n",
    "        raise ValueError(\"No usable station windows found (missing station_time or chosen waveform).\")\n",
    "\n",
    "    # Choose anchor by |median| + std (ms)\n",
    "    def stats_ms(v):\n",
    "        v = np.asarray(v, float); return abs(np.nanmedian(v)*1e3) + np.nanstd(v)*1e3\n",
    "    crit = {\"H1\": stats_ms(X[\"r_H1_s\"]), \"H2\": stats_ms(X[\"r_H2_s\"]), \"H3\": stats_ms(X[\"r_H3_s\"])}\n",
    "    anchor = min(crit, key=lambda k: crit[k])\n",
    "\n",
    "    print(\"\\n=== Anchor inference across dataset ===\")\n",
    "    for k in (\"H1\",\"H2\",\"H3\"):\n",
    "        vec = X[f\"r_{k}_s\"].values\n",
    "        print(f\"  {k}: N={np.isfinite(vec).sum():6d}  median={np.nanmedian(vec)*1e3:+8.3f} ms  \"\n",
    "              f\"std={np.nanstd(vec)*1e3:7.3f} ms  p95={np.nanpercentile(vec,95)*1e3:8.3f} ms\")\n",
    "    print(f\"  >>> Chosen anchor: {anchor} (min |median|+std)\")\n",
    "\n",
    "    # Reconstruct start/end/peak absolute times using the chosen anchor\n",
    "    if anchor == \"H2\":\n",
    "        start = X[\"station_time\"]\n",
    "    elif anchor == \"H3\":\n",
    "        start = X[\"station_time\"] - pd.to_timedelta(X[\"wave_len\"]/ (2*FS), unit=\"s\")\n",
    "    else:  # H1\n",
    "        start = X[\"station_time\"] - pd.to_timedelta(X[\"ipk\"]/FS, unit=\"s\")\n",
    "    X[\"start_time\"] = start\n",
    "    X[\"end_time\"]   = X[\"start_time\"] + pd.to_timedelta(X[\"wave_len\"]/FS, unit=\"s\")\n",
    "    X[\"abs_peak_time\"] = X[\"start_time\"] + pd.to_timedelta(X[\"ipk\"]/FS, unit=\"s\")\n",
    "    X[\"pred_arrival\"]  = X[\"fix_time\"] + pd.to_timedelta(X[\"dist_km\"]/C_KM_S, unit=\"s\")\n",
    "    X[\"residual_ms\"]   = (X[\"abs_peak_time\"] - X[\"pred_arrival\"]).dt.total_seconds()*1e3\n",
    "\n",
    "    # Ordering check by station\n",
    "    inv_count = 0; st_bad = 0\n",
    "    for st, grp in X.sort_values(\"start_time\").groupby(\"station_code\"):\n",
    "        arr = grp[\"start_time\"].astype(\"int64\").values\n",
    "        inv = int((np.diff(arr) < 0).sum())\n",
    "        if inv>0: st_bad += 1; inv_count += inv\n",
    "    print(f\"\\nOrdering check: stations with any out-of-order starts: {st_bad} (total inversions={inv_count})\")\n",
    "\n",
    "    print(\"\\nResiduals (abs_peak_time vs predicted arrival) [ms]:\")\n",
    "    print(X[\"residual_ms\"].describe(percentiles=[.5,.9,.95,.99]).to_string())\n",
    "\n",
    "    return anchor, X\n",
    "\n",
    "# --------------------------- 2) Export to sim contract ------------------------\n",
    "def export_to_stormbundle(timeline: pd.DataFrame, wave_key: str = WAVE_KEY):\n",
    "    \"\"\"Build quantised scene + labels exactly like the simulator output.\"\"\"\n",
    "    t0 = timeline[\"start_time\"].min() - pd.to_datetime(0, utc=True)  # trick to keep tz\n",
    "    # Keep UTC arithmetic\n",
    "    scene_start = timeline[\"start_time\"].min() - pd.to_timedelta(PRE_PAD_S, unit=\"s\")\n",
    "    scene_end   = timeline[\"end_time\"].max()   + pd.to_timedelta(POST_PAD_S, unit=\"s\")\n",
    "    N = int(math.ceil((scene_end - scene_start).total_seconds() * FS))\n",
    "\n",
    "    station_codes = sorted(timeline[\"station_code\"].unique())\n",
    "    acc = {code: np.zeros(N, np.float64) for code in station_codes}\n",
    "\n",
    "    dropped = 0\n",
    "    for _, r in timeline.iterrows():\n",
    "        a = data[r[\"fix_id\"]][\"group\"]  # pull from original to get the actual array\n",
    "        # Find the group row by station name/code; safer: iterate and match lat/lon too\n",
    "        # But we cached ipk/wave_len already; we can access via timeline using wave_key\n",
    "        # Simpler: look up the array we used earlier; rebuild quickly:\n",
    "        arr = None\n",
    "        for g in data[r[\"fix_id\"]][\"group\"]:\n",
    "            if site_code(g[\"site_name\"]) == r[\"station_code\"]:\n",
    "                arr = g.get(wave_key); break\n",
    "        if not isinstance(arr, np.ndarray) or arr.size == 0:\n",
    "            dropped += 1;\n",
    "            continue\n",
    "\n",
    "        # Convert to float and (optionally) normalise\n",
    "        if np.issubdtype(arr.dtype, np.integer):\n",
    "            bits_in = detect_bits_int(arr)\n",
    "            a_float = (arr.astype(np.float64) / max(fullscale(bits_in),1)) * VREF\n",
    "        else:\n",
    "            a_float = arr.astype(np.float64)\n",
    "        if USE_NORMALISE_0p8_VREF:\n",
    "            pk = float(np.nanmax(np.abs(a_float))) if a_float.size else 0.0\n",
    "            if pk>0: a_float = (a_float / pk) * (0.8 * VREF)\n",
    "\n",
    "        start_idx = int(round((r[\"start_time\"] - scene_start).total_seconds() * FS))\n",
    "        if start_idx < 0 or start_idx >= N:\n",
    "            dropped += 1;\n",
    "            continue\n",
    "        L = int(r[\"wave_len\"]); end_idx = min(N, start_idx + L)\n",
    "        Luse = end_idx - start_idx\n",
    "        if Luse <= 0:\n",
    "            dropped += 1;\n",
    "            continue\n",
    "        acc[r[\"station_code\"]][start_idx:end_idx] += a_float[:Luse]\n",
    "\n",
    "    # One-off quantisation per station\n",
    "    full = fullscale(BITS)\n",
    "    quantised = {}\n",
    "    for code, arr in acc.items():\n",
    "        peak = float(np.nanmax(np.abs(arr))) if arr.size else 0.0\n",
    "        if peak > VREF and peak > 0:\n",
    "            arr = arr * (VREF/peak)  # soft limiter\n",
    "        q = np.clip(np.round((arr / VREF) * full), -full, full).astype(np.int16)\n",
    "        quantised[code] = q\n",
    "\n",
    "    # Build df_wave\n",
    "    time_s = np.arange(N, dtype=np.float64)/FS\n",
    "    df_wave = pd.DataFrame({\"time_s\": time_s})\n",
    "    for code in station_codes:\n",
    "        df_wave[code] = quantised[code]\n",
    "\n",
    "    # Labels (one row per station×fix)\n",
    "    labels = []\n",
    "    for _, r in timeline.iterrows():\n",
    "        start_idx = int(round((r[\"start_time\"] - scene_start).total_seconds() * FS))\n",
    "        labels.append(dict(\n",
    "            event_id = int(r[\"fix_id\"]),\n",
    "            stroke_i = 0,\n",
    "            station  = r[\"station_code\"],\n",
    "            flash_type = \"FIX\",\n",
    "            lat = float(r[\"fix_lat\"]), lon = float(r[\"fix_lon\"]),\n",
    "            true_time_s = pd.Timestamp(r[\"fix_time\"]).timestamp(),\n",
    "            sample_idx  = int(start_idx),\n",
    "            window_idx  = int(start_idx // 1024),\n",
    "        ))\n",
    "    df_labels = pd.DataFrame(labels)\n",
    "\n",
    "    # Events (one per fix)\n",
    "    events = []\n",
    "    for fix_id, g in timeline.groupby(\"fix_id\"):\n",
    "        row = g.iloc[0]\n",
    "        events.append(dict(\n",
    "            id = int(fix_id),\n",
    "            flash_type = \"FIX\",\n",
    "            lat = float(row[\"fix_lat\"]), lon = float(row[\"fix_lon\"]),\n",
    "            time_s = pd.Timestamp(row[\"fix_time\"]).timestamp(),\n",
    "        ))\n",
    "\n",
    "    # Pack as StormBundle if available\n",
    "    try:\n",
    "        from storm import StormBundle\n",
    "        bundle = StormBundle(quantised, events, labels, df_wave, df_labels)\n",
    "    except Exception:\n",
    "        bundle = dict(\n",
    "            quantised=quantised, events=events,\n",
    "            stroke_records=labels, df_wave=df_wave, df_labels=df_labels\n",
    "        )\n",
    "    print(f\"\\nExport complete. Stations={len(station_codes)}, samples/station={N:,}, windows dropped={dropped}\")\n",
    "    return bundle\n",
    "\n",
    "# --------------------------- Run it all ---------------------------------------\n",
    "\n",
    "# 1) Forensic view of one fix (adjust index if you like)\n",
    "_ = inspect_fix(data, k=0)\n",
    "\n",
    "# 2) Infer anchor across dataset and reconstruct per-window timing\n",
    "ANCHOR, TIMELINE = infer_anchor_and_reconstruct(data, wave_key=WAVE_KEY)\n",
    "\n",
    "# The TIMELINE table now holds:\n",
    "#   fix_id, site_name, station_code, fix_time, station_time, fix_lat/lon, station_lat/lon,\n",
    "#   dist_km, wave_len, ipk, start_time, end_time, abs_peak_time, pred_arrival, residual_ms\n",
    "\n",
    "# 3) (Optional) Export to your simulator contract using the reconstructed times\n",
    "storm_bundle = export_to_stormbundle(TIMELINE, wave_key=WAVE_KEY)\n",
    "\n",
    "# Handy objects now in your workspace:\n",
    "#   • TIMELINE (pd.DataFrame): start/end/peak times & residuals for every station-window\n",
    "#   • ANCHOR (str): \"H1\" | \"H2\" | \"H3\" (what station_time means)\n",
    "#   • storm_bundle (StormBundle or dict): {quantised, events, stroke_records, df_wave, df_labels}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
